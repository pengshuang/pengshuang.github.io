<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="在路上，慢慢走！">
<meta property="og:type" content="website">
<meta property="og:title" content="小沙文的博客">
<meta property="og:url" content="http://pengshuang.space/page/2/index.html">
<meta property="og:site_name" content="小沙文的博客">
<meta property="og:description" content="在路上，慢慢走！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小沙文的博客">
<meta name="twitter:description" content="在路上，慢慢走！">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://pengshuang.space/page/2/"/>

  <title> 小沙文的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小沙文的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/03/线性模型和非线性模型的区别/" itemprop="url">
                  线性模型和非线性模型的区别
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-03T21:59:49+08:00" content="2017-03-03">
              2017-03-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/03/线性模型和非线性模型的区别/" class="leancloud_visitors" data-flag-title="线性模型和非线性模型的区别">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>在机器学习的回归问题中，线性模型和非线性模型都可以去对曲线进行建模，那么线性模型和非线性模型有什么区别呢？</p>
<p>其实，线性模型和非线性模型的区别并不在于能不能去拟合曲线。下面我们来详细介绍一下它们两个的区别。</p>
<h3 id="线性回归的等式"><a href="#线性回归的等式" class="headerlink" title="线性回归的等式"></a>线性回归的等式</h3><p>线性回归需要一个线性的模型。这到底意味着什么呢？</p>
<p>一个模型如果是线性的，就意味着它的参数项要么是常数，要么是原参数和要预测的特征之间的乘积加和就是我们要预测的值。</p>
<pre><code>Response = constant + parameter * predictior1 + ... + parameter * predictior2
</code></pre><p>下是个典型的线性模型：</p>
<p>$$Y = b + w_1x_1 + w_2x_2 + … + w_kx_k$$</p>
<p>在统计意义上，如果一个回归等式是线性的，那么它的相对于参数就必须也是线性的。如果相对于参数是线性，那么即使性对于样本变量的特征是二次方或者多次方，这个回归模型也是线性的 (例如下面的公式)。</p>
<p>$$Y = b + w_1x_1 + w_2x_2^2$$</p>
<p>你甚至可以使用 log 或者指数去形式化特征</p>
<p>$$Y = b + w_1e^{-x_1} + w_2e^{-x_2}$$</p>
<h3 id="非线性回归的等式"><a href="#非线性回归的等式" class="headerlink" title="非线性回归的等式"></a>非线性回归的等式</h3><p>最简单的判断一个模型是不是非线性，就是关注非线性本身，判断它的参数是不是非线性的。非线性有很多种形象，这也是为什么非线性模型能够那么好的拟合那些曲折的函数曲线的原因。比如下面这个：</p>
<p>$$Y = \theta_1 \ast x^{\theta_2}$$</p>
<p>$$Y = \theta_1 + (\theta_3 - \theta_2) \ast e^{- \theta_4 X}$$</p>
<p>与线性模型不一样的是，这些非线性模型的特征因子对应的参数不止一个。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/02/天池ijcai比赛总结/" itemprop="url">
                  IJCAI SocInf'16 Contest-Brick-and-Mortar Store Recommendation 天池大数据比赛总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-02T16:40:05+08:00" content="2017-03-02">
              2017-03-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/02/天池ijcai比赛总结/" class="leancloud_visitors" data-flag-title="IJCAI SocInf'16 Contest-Brick-and-Mortar Store Recommendation 天池大数据比赛总结">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>一直想总结一下这次的比赛，拖啊拖。。。一直等到现在，趁着现在要找实习，好好总结一下。</p>
<h3 id="比赛题目"><a href="#比赛题目" class="headerlink" title="比赛题目"></a>比赛题目</h3><p>比赛的官方网站在这，<a href="https://tianchi.shuju.aliyun.com/competition/introduction.htm?spm=5176.100066.333.2.4HLjZH&amp;raceId=231532" target="_blank" rel="external">IJCAI SocInf’16</a>。</p>
<p>这次比赛的题目是给定 2015 年 7 ~ 11 月份的用户在不同地点口碑购买记录，以及 2015 年 7 ~ 11 月淘宝上用户的购物行为数据，来预测 12 月这一整月用户来到一个地点之后会光顾哪些口碑商铺。这个比赛有一个很有意思的地方，就是它关注的是一个用户来到一个他之前没有去过的新地点之后，他会去哪些店铺消费，有一点像推荐系统中经典的冷启动问题。比赛提供的数据也有这个特点：</p>
<ol>
<li>在测试集中，只有 10 % 的用户用之前使用口碑的记录；</li>
<li>有 5 % 的用户虽然有之前使用口碑的记录，但是在测试集中，这些用户来到了新的地点；</li>
<li>所有用户都有他们淘宝购物的行为数据。</li>
</ol>
<p>如果题目只是这样的，其实还不算奇怪，奇怪的是题目的评价标准上加上了 <strong>budget</strong> 这个神奇的东西，先来看看评价指标：</p>
<p>$$P = \frac{\sum_i min(|S_i \cap S_i^*, b_i|)}{\sum_i| S_i^* |}$$</p>
<p>$$R = \frac{\sum_i min(|S_i \cap S_i^*, b_i|)}{\sum_i min(|S_i, b_i|)}$$</p>
<p>$$F_1 = \frac{2 \ast P \ast R}{P + R}$$</p>
<p>最后考核的目标是 F1 值。简单来说一下这个 <strong>budget</strong>：</p>
<p>我们的目标是在用户来到一个地点之后，给他推荐他可能会去的店铺。这里，把问题转换一下，我们要给很多店铺来推荐可能会来这里购物的人。这里推荐的人数要受到  <strong>budget</strong> 的限制，即不能超过店铺的最大承载量以及口碑提供给这家店铺的优惠券的个数。</p>
<p>先详细介绍一下比赛提供的数据格式：</p>
<ul>
<li>Online user behavior before Dec. 2015. (ijcai2016 taobao.csv)</li>
</ul>
<p>2015 年 7 ~ 11 月淘宝上用户的购物行为数据，包括用户的点击、购买，购买物品种类等属性。</p>
<ul>
<li>Users shopping records at brick-and-mortar stores before Dec. 2015. (ijcai2016 Koubei train.csv)</li>
</ul>
<p>2015 年 7 ~ 11 月份的用户在不同地点口碑购买记录，数据格式：(user_id, loc_id, merchant_id, time_stamp)</p>
<ul>
<li>Merchant information (ijcai2016 merchant info.csv)</li>
</ul>
<p>不同口碑商铺的位置分布情况，有的商铺有连锁店，数据格式：（merchan_id, loc_id1:loc_id2:…)</p>
<ul>
<li>Prediction result. (ijcai2016 Koubei test.csv)</li>
</ul>
<p>最后要提交的数据格式</p>
<p>user_id, loc_id, merchant_id1:merchant_id2…</p>
<p>对于这个问题，我们的想法是，要充分利用每一个用户和每一个口碑商铺的历史数据。更准确的说，我们要从训练集中提取足够的可训练的特征，然后利用一些经典的模型，比如 <strong>Xgboost</strong> 来构建分类模型。同时最需要注意的是，我们要时刻考虑 <strong>budget</strong> 的影响。</p>
<h3 id="如何来利用历史数据"><a href="#如何来利用历史数据" class="headerlink" title="如何来利用历史数据 ?"></a>如何来利用历史数据 ?</h3><p>我们分析了题目所给的训练集，然后将数据切分为两部分，第一部分是 2015 年 11 月 23 号之前的数据，我们把这当做本地训练集；另一部分是 2015 年 11 月 23 号之后的数据，我们把这当做本地测试集。通过对这些数据的整理和分析，我发现几条条重大的规律：</p>
<ol>
<li>如果地点确定的话，对于一个用户，他最有可能去的是他曾经光顾过的店。</li>
<li>从口碑店铺的角度来讲，如果这家店铺曾经的销量很好，那么它也将在未来吸引更多的顾客去消费。</li>
</ol>
<p>从这两条观察出来的规律来看，我们将探索数据的方向划分为两条道路：一条是从用户的角度出发；另外则是从口碑商店的角度出发。</p>
<h4 id="从用户的历史信息来看"><a href="#从用户的历史信息来看" class="headerlink" title="从用户的历史信息来看"></a>从用户的历史信息来看</h4><p>基于我们的观察，当一个人来到一个他之前去过的地点，他们更倾向于去之前购物过的商店消费。因此，一个在过去的一段时间内在一家商铺的消费次数能够在很大程度上影响我们的推荐质量。频率越高，越能代表这个人在未来会再次来到这家店消费。</p>
<p>我们做了一个统计，如果一个用户在过去曾经关顾过一家口碑店铺超过 6 次的话，当我们在之后再向这位用户推荐这家店的话，我们会得到超过 90 % 的准确率。</p>
<p>不过从另一方面说，每一家口碑店铺都有 <strong>budget</strong> 限制，这意味着，如果按照用户之前光顾过哪家店铺来推荐的话，肯定会有部分店铺的 <strong>budget</strong> 超标。</p>
<p>我们在用户的数据中找寻这样一个键值对 (user, location, merchant)，通过计算这样一个键值对出现的频率，我们可以统计出 (user, location, merchant, frequency) 的键值对，将这个键值对按照 <strong>frequency</strong> 来从高到低排序，然后按照这个顺序，从高到低来给 (user, query) 这样一个查询来推荐店铺，直到这家店铺的 <strong>budget</strong> 耗尽为止。</p>
<p>经过统计发现，平均每一个用户在一个地点只会关顾 1.3 个商家，所以在这里我们限制最大的推荐个数为 4。</p>
<h4 id="从商家的历史信息来看"><a href="#从商家的历史信息来看" class="headerlink" title="从商家的历史信息来看"></a>从商家的历史信息来看</h4><p>经过之前的分析，我们发现不同的店铺有着不同的“受欢迎度”。举个例子，“820”这个商家在整个训练集中几乎出现 1/4。问题是我们怎么定义这个“受欢迎度”呢？</p>
<p>为了解决这个问题，我们首先定义，在不同地点的同一家口碑商铺是不一样。因为在题目给定的数据中，存在大量的连锁商铺，但是这些连锁店铺在不同地点的“受欢迎度”是完全不同的。</p>
<p>接下来我们设想这样一种情况，90 % 来到地点 A 的人都会去 商铺 B 消费，在这种情况下，如果我们给所有的来地点 A 的人都推荐 B 商铺的话，我们就能得到 90 % 的准确率。所以，在一个地点某家店铺消费的总人数占该地点所有人数的比例，我们称之为该店铺的 “受欢迎度” (Popularity)。</p>
<p>因此我们将所有店铺按照 Popularity 从大到小来进行排序，依次推荐来到该地点的所有用户，直到超过 <strong>budget</strong>。经过一些线下的实验，我们取这个 Popularity 的值的阈值为 0.25。</p>
<h4 id="引入淘宝数据来提升推荐质量"><a href="#引入淘宝数据来提升推荐质量" class="headerlink" title="引入淘宝数据来提升推荐质量"></a>引入淘宝数据来提升推荐质量</h4><p>我们之前的推荐完全没有用到每个人的特征，相当于无法做到“千人千面”。于是接下来我们就想办法，如何利用淘宝的数据来提升推荐质量。</p>
<p>从直观上而且，淘宝的数据应该很有帮助，比如，在淘宝上经常浏览或者购买电子产品的人往往不太会去关顾口碑商铺里面那些卖女式服装的。受到这个的启发，我们就建立了一个这样的表：如果存在这样一条记录，一个用户在淘宝上浏览或者购买的商铺 A；同时也在线下口碑上的商铺 B 消费，我们就把 (A, B) 这个关系链表放入表中。基于这样的表，我们对那些之前没有口碑消费记录的新用户，如果他们曾在淘宝上购买或浏览了商铺 A， 那么我们就只给他推荐在关系链表中与 A 相连的口碑商铺。线上的结果证明，我们的预测质量提升了。</p>
<h3 id="引入机器学习模型"><a href="#引入机器学习模型" class="headerlink" title="引入机器学习模型"></a>引入机器学习模型</h3><p>目前为止我们都没有怎么用机器学习模型，用普通的规则就可以在天池上面排一个不错的名次。但是为了取得更好的成绩，我们尝试着去探寻每一个用户、地点和商铺的各种各样可能的特征。下面我将详细介绍这些我们的做法。</p>
<p>我们将这个问题看成是一个二分类问题。我们的方法是对每一个店铺建模，比如说，在数据集中，用户 u 在地点 l 的店铺 m 消费了。我们可以产生一个三元组 (u, l, m)。对应于这个三元组，我们可以产生一些训练数据，首先，对于我们而言，正样本即是那些消费过的用户，即 (u, l, m) 是 True；第二，我们的负例是那些同样是这个地点的其他商铺，比如说 m‘，我们将 (u, l, m’) 定义为 False。按照这个方法，我们可以产生供二分类的训练集。根据这个道理，对于赛题要我们预测的用户来到一个地点之后会去哪些店铺的情况，我们也可以根据这个三元组，产生一个每一个店铺的预测概率。</p>
<p>这么做其实负样本是很多的。。。为了避免正负样本不平衡的问题，我们采取采样的方法去提取负样本。</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>特征工程对应机器学习来说十分重要，俗话说，特征是模型的上限。我们观察到，有些用户喜欢关顾那些他们之前去过的店，有些喜欢光顾该地点上最热门的店铺，有些则喜欢去那些刚开张的店铺。</p>
<p>所以，针对 (u, l, m) 这样一个三元组，我们试图找寻关于他们其中任意一个的特征。</p>
<p>我们找寻的特征如下：</p>
<ol>
<li>键值对 (u, l, m) 出现的次数，即 frequence，用户关顾这家店的频率。</li>
<li>键值对 (u, l, m) 是否出现过，True or False。</li>
<li>user_id 的 onehot 编码。</li>
<li>merchant_id 编码。</li>
<li>(l, m) 出现次数，即这个地点这个商店的总销量。</li>
<li>open_interval：店铺的开业时间。</li>
<li>用户 u 在淘宝上购物的次数</li>
<li>用户 u 在淘宝上浏览的次数</li>
<li>用户 u 在淘宝上购买过的商品的种类的 onehot 编码</li>
<li>用户 u 在淘宝上购买过的商铺的 id 的 onehot 编码</li>
<li>用户 u 在淘宝上购买的比例 （购买数 / (购买 + 点击）</li>
<li>用户 u 在淘宝上购买的次数</li>
<li>用户 u 在淘宝上点击的次数</li>
<li>通过 SVD 算出来的用户 u 的潜在矩阵</li>
<li>通过 SVD 算出来的商店 m 的潜在矩阵</li>
</ol>
<h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><p>之前的规则可以得出一个结果，之后的模型也可以得出一个结果，在比赛的最后阶段，我们对模型进行融合，尝试各种不同的参数，达到了这个名次。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/28/问答社区的回答排序算法/" itemprop="url">
                  问答社区的回答排序算法(一）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-28T15:50:26+08:00" content="2017-02-28">
              2017-02-28
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/28/问答社区的回答排序算法/" class="leancloud_visitors" data-flag-title="问答社区的回答排序算法(一）">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>正好研究生的研究方向是问答社区的专家发现，所以，看过很多关于专家发现的 paper 以及一些工业界实践的经验。一直没有机会好好总结一下，借此机会梳理一下。问答社区的专家用户的发现有助于问题的顺利解决，并且也有助于提高问答社区的用户体验，有利于问答社区的长期发展。</p>
<p>现有的专家发现方法主要分为两种，一种是基于用户投票，另一种是基于用户之间的社交网络，本文将先介绍前一种。</p>
<h2 id="基于用户投票的排序算法"><a href="#基于用户投票的排序算法" class="headerlink" title="基于用户投票的排序算法"></a>基于用户投票的排序算法</h2><p>顾名思义，就是利用用户的反馈，即投票数（又细分为赞成和反对）来对答案进行排名。下面介绍三种方法，分别被 <strong>Urban Dictionary</strong>、<strong>Amazon</strong> 和 <strong>知乎</strong> 采用。</p>
<h3 id="得分-赞成票-反对票"><a href="#得分-赞成票-反对票" class="headerlink" title="得分 = 赞成票 - 反对票"></a>得分 = 赞成票 - 反对票</h3><p>假定有两个项目，项目 A 是 30 张赞成票，10 张反对票，项目 B 是 200 张赞成票，150 张反对票。如果按照上面的公式，B会排在前面，因为它的得分（200 - 150 = 50）高于 A（30 - 10 = 20）。但是实际上，B 的好评率只有 57%（200 / 350），而 A 为 75%（30 / 40），所以正确的结果应该是A排在前面。</p>
<p><strong>Urban Dictionary</strong> 就是这种错误算法的实例。</p>
<p><img src="/img/p1.png" alt=""></p>
<h3 id="得分-赞成票-总票数"><a href="#得分-赞成票-总票数" class="headerlink" title="得分 = 赞成票 / 总票数"></a>得分 = 赞成票 / 总票数</h3><p>如果总票数很大，这种算法其实是对的。问题出在如果总票数很少，这时就会出错。假定 A 有 2 张赞成票、0 张反对票，B 有 100 张赞成票、1 张反对票。这种算法会使得 A 排在 B 前面。这显然错误。</p>
<p><strong>Amazon</strong> 便采用的是这种做法。</p>
<p><img src="/img/p2.png" alt=""></p>
<h3 id="威尔逊得分"><a href="#威尔逊得分" class="headerlink" title="威尔逊得分"></a>威尔逊得分</h3><p>威尔逊得分的思想是，如果把一个回答展示给很多人看并让他们投票，内容质量不同的回答会得到不同比例的赞同和反对票数，最终得到一个反映内容质量的得分。当投票的人比较少时，可以根据已经获得的票数估计这个回答的质量得分，投票的人越多则估计结果越接近真实得分。</p>
<p>如果新一个回答获得了 1 票赞同 0 票反对，也就是说参与投票的用户 100% 都选了赞同，但是因为数量太少，所以得分也不会太高。如果一小段时间后这个回答获得了 20 次赞同 1 次反对，那么基于新算法，我们就有较强的信心把它排在另一个有 50 次赞同 20 次反对的回答前面。原因是我们预测当这个回答同样获得 50 次赞同时，它获得的反对数应该会小于 20。威尔逊得分算法最好的特性就是，即使前一步我们错了，现在这个新回答排到了前面，获得了更多展示，在它得到更多投票后，算法便会自我修正，基于更多的投票数据更准确地计算得分，从而让排序最终能够真实地反映内容的质量。</p>
<p>它有着很多优良特性：</p>
<ol>
<li>投票总数趋向于正无穷时，得分也趋向正反馈占总反馈的比例，对于内容质量的解释下很强。</li>
<li>在数据比较少的情况，总票数较少和极端参数的情况下，结果也能保持很好的鲁棒性。</li>
<li>置信区间大学可以通过参数保证。</li>
<li>虽然二项分布是离散类型，但是由于得分表达式关于正负反馈次数的函数是连续的，因此可以引入非整数的投票（加权投票），同时不改变算法性质。</li>
<li>得分的取值范围是（0，1），与投票总数无关，间接的做了归一化。</li>
</ol>
<p>威尔逊得分的计算公式如下：</p>
<p>$$score = \frac{\phi + \frac{z^2}{2n} - z * \sqrt{(\frac{\phi\ast (1 - \phi)}{n} + \frac{z^2}{4n^2}}}{(1 + \frac{z^2}{n})}$$</p>
<p>\(\phi\) 表示样本的赞成票比例，\(n\) 表示样本的大小，\(z_{1- \alpha/2}\) 表示对应某个置信水平的 \(z\) 统计量，这是一个常数，可以通过查表或统计软件包得到。一般情况下，在 95% 的置信水平下，z 统计量的值为 1.96。</p>
<p>威尔逊得分计算过程 (JavaScript 版本)：</p>
<pre><code>n = Up + Down
if (n==0) {
score = 0;
}
else {
z = 1.96
phat = Up / n
score = (phat + z*z/(2*n) - z * Math.sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n);
}
</code></pre><p>可以看到，当 \(n\) 的值足够大时，这个下限值会趋向。如果 \(n\) 非常小（投票人很少），这个下限值会大大小于。实际上，起到了降低 “赞成票比例” 的作用，使得该项目的得分变小、排名下降。</p>
<p>知乎采用的就是这种方法，下面摘一段<strong><a href="https://zhuanlan.zhihu.com/p/19902495?columnSlug=zhihu-product" target="_blank" rel="external">知乎产品总监</a></strong>对这个算法评价的一段话：</p>
<pre><code>  因此未来我们会看到更多新创作的优质内容，快速获得靠前的排序，低质内容则会长期
保持在底部。细心的你可能也想到了，并不是所有的回答最终都会获得很多投票，大体上
获得投票总数较多的回答仍然会排在投票较少的回答前面。
</code></pre><p>需要提一下，这个威尔逊算法在 x = 0 时函数取值收敛为 0，无法对赞同为 0，但反对票数不一样的回答进行排序。为了方便，知乎默认所有回答者对自己的投票投了一票赞同。这样不仅解决了这个问题，而且让回答者也将自身权重参与到排序的计算中。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://www.ruanyifeng.com/blog/2012/03/ranking_algorithm_wilson_score_interval.html" target="_blank" rel="external">基于用户投票的排名算法（五）：威尔逊区间</a></li>
<li><a href="https://www.zhihu.com/question/26933554" target="_blank" rel="external">如何评价知乎的回答排序算法？</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/27/数据挖掘-评分卡模型/" itemprop="url">
                  数据挖掘: 评分卡模型
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-27T17:24:27+08:00" content="2017-02-27">
              2017-02-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/27/数据挖掘-评分卡模型/" class="leancloud_visitors" data-flag-title="数据挖掘: 评分卡模型">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>信用评分卡模型在国外是一种成熟的预测方法，尤其在信用风险评估以及金融风险控制领域更是得到了比较广泛的使用，其原理是将模型变量 WOE 编码方式离散化之后运用 logistic 回归模型进行的一种二分类变量的广义线性模型。本文重点介绍模型变量 WOE 以及 IV 原理.</p>
<h3 id="WOE"><a href="#WOE" class="headerlink" title="WOE"></a>WOE</h3><p>WOE的全称是 “Weight of Evidence”，即证据权重。WOE 是对原始自变量的一种编码形式。要对一个变量进行 WOE 编码，需要首先把这个变量进行分组处理。分组后，对于第 i 组，WOE的计算公式如下：</p>
<p>$$woe_i = ln\frac{P_{y_i}}{P_{n_i}} = ln\frac{y_i/y_s}{n_i/n_s}$$</p>
<p>其中，\(p_{y_i}\) 是这个组中响应客户，风险模型中，对应的是违约客户，指的是模型中预测变量取值为 1 的个体占所有样本中所有响应客户的比例，\(p_{n_i}\) 是这个组中未响应客户占样本中所有未响应客户的比例，\(y_i\) 是这个组中响应客户的数量，\(n_i\) 是这个组中未响应客户的数量，\(y_s\) 是样本中所有响应客户的数量，\(n_s\) 是样本中所有未响应客户的数量。</p>
<p>简单来说， WOE 表示的是<strong>当前分组中响应的用户占所有响应用户的比例</strong>和<strong>当前分组中没有响应的用户占所有没有响应用户的比例</strong>的差异。</p>
<p>WOE 也可以这么来理解，</p>
<p>$$woe_i = ln\frac{P_{y_i}}{P_{n_i}} = ln\frac{y_i/y_s}{n_i/n_s}$$</p>
<p>WOE 同时也等于：</p>
<p>$$woe_i = ln\frac{y_i/n_i}{y_s/n_s}$$</p>
<p>可以看出，WOE 也可以表示<strong>当前这个组中响应的客户和未响应的客户的比例</strong>和<strong>所有样本中这个比例</strong>的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE 越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE 越小，差异越小，这个分组里的样本响应的可能性就越小。WOE 蕴含了自变量取值对应目标变量（违约概率）的影响。再加上 WOE 计算形式与 logistic 回归中目标变量的 logistic 转换很相似，因此可以将自变量 WOE 替代原先的自变量值。</p>
<h3 id="IV"><a href="#IV" class="headerlink" title="IV"></a>IV</h3><p>其实 IV 衡量的是某一个变量的信息量，从公式来看的话，相当于是自变量 WOE 值的一个加权求和，其值的大小决定了自变量对于目标变量的影响程度；从另一个角度来看的话，IV 公式与信息熵的公式极其相似。</p>
<p>IV 公式如下：</p>
<p>$$IV_i = (P_{yi} - P_{ni}) \ast WOE_i $$</p>
<p>$$= (P_{yi} - P_{ni}) \ast ln \frac{P_{y_i}}{P_{n_i}}$$</p>
<p>$$= (y_i / y_s - n_i / n_s) ln \frac{y_i/y_s}{n_i/n_s}$$</p>
<p>计算了一个变量各个组的 IV 值之后，我们就可以计算整个变量的 IV 值：</p>
<p>$$IV = \sum_i^n IV_i$$</p>
<p>其中 n 为变量的分组个数。</p>
<ul>
<li>对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，IV 值越大，否则，IV 值越小；</li>
<li>极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV 值为0；</li>
<li>IV 值的取值范围是[0, 正无穷)，且当当前分组中只包含响应客户或者未响应客户时，IV = 正无穷。</li>
</ul>
<p>如果想要对变量的预测能力进行排序的话，可以按 IV 值从高到第去筛选。</p>
<h3 id="WOE-和-IV-的区别"><a href="#WOE-和-IV-的区别" class="headerlink" title="WOE 和 IV 的区别"></a>WOE 和 IV 的区别</h3><p>WOE 和 IV 都能表达着这个分组对目标变量的预测能力。但实际中是选择 IV 而不是 WOE 的和来衡量变量预测的能力，这是为什么呢？首先，因为我们在衡量一个变量的预测能力时，我们所使用的指标值不应该是负数。从这意义上来说，IV 比 WOE 多乘以前面那个因子，就保证了它不会是负数；然后，乘以 \((P_{yi} - P_{ni})\) 这个因子，体现出了变量当前分组中个体的数量占整体的比例，从而很好考虑了这个分组中样本占整体的比例，比例越低，这个分组对变量整体预测能力的贡献越低。相反，如果直接用 WOE 的绝对值加和，会因为该分组出现次数偏少的影响而得到一个很高的指标。</p>
<h3 id="IV-对极端情况的处理"><a href="#IV-对极端情况的处理" class="headerlink" title="IV 对极端情况的处理"></a>IV 对极端情况的处理</h3><p>IV 依赖 WOE，并且 IV 是一个很好的衡量自变量对目标变量影响程度的指标。但是，使用过程中应该注意一个问题：变量的任何分组中，不应该出现响应数为 0 或非响应数为 0 的情况。</p>
<p>当变量的一个分组中的响应数为 0 时，对应的 WOE 就为负无穷。此时 IV 为正无穷。</p>
<p>当变量的一个分组中的响应不为 0 时，道理也类似。</p>
<p>所以 IV 不能自动处理变量的分组中出现响应比例为 0 或 100% 的情况。遇到这种情况：</p>
<ul>
<li>如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件；</li>
<li>重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），强烈建议这样做，因为本身把一个分组个体数弄得很小就不是太合理。</li>
<li>如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为 0，可以人工调整响应数为 1，如果非响应数原本为 0，可以人工调整非响应数为 1。</li>
</ul>
<h3 id="信用卡模型"><a href="#信用卡模型" class="headerlink" title="信用卡模型"></a>信用卡模型</h3><p>因为 IV 是衡量自变量对目标变量的指标之一，也可以看做是单个自变量的评分模型，可以直接将这个自变量的取值当做事某种信用评分</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/21/机器学习常见算法总结/" itemprop="url">
                  机器学习常见算法总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-21T15:11:48+08:00" content="2017-02-21">
              2017-02-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/21/机器学习常见算法总结/" class="leancloud_visitors" data-flag-title="机器学习常见算法总结">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>事件 A 和 B 同时发生的概率为在 A 发生的情况下发生 B 或者在 B 发生的情况下发生 A</p>
<p>$$P(A\cap B) = P(B) * P(A|B)$$</p>
<p>所以，</p>
<p>$$P(A|B) = \frac{P(B|A) * P(A)}{P(B)}$$</p>
<p>对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</p>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ol>
<li>假设现在有样本 \(x = (a_1,a_2,…,a_n)\) 这个待分类项(并认为 x 里面的特征独立)。</li>
<li>再假设现在有分类目标 \(Y = (y_1,y_2,…,y_n)\)</li>
<li>那么 \(max(P(y_1|x),P(y_2|x),P(y_3|x),…,P(y_n|x))\) 就是最终的分类类别。</li>
<li>而 \(Y = (y_1,y_2,…,y_n)\) 就是最终的分类类别。</li>
</ol>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><h4 id="1-准备阶段"><a href="#1-准备阶段" class="headerlink" title="1. 准备阶段"></a>1. 准备阶段</h4><p>确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。</p>
<h4 id="2-训练阶段"><a href="#2-训练阶段" class="headerlink" title="2. 训练阶段"></a>2. 训练阶段</h4><p>计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计。</p>
<h4 id="3-应用阶段"><a href="#3-应用阶段" class="headerlink" title="3. 应用阶段"></a>3. 应用阶段</h4><p>使用分类器进行分类，输入时分类器和待分类样本，输出是样本属于的分类类别。</p>
<h3 id="属性特征"><a href="#属性特征" class="headerlink" title="属性特征"></a>属性特征</h3><ol>
<li>特征为离散值时直接统计即可。</li>
<li>特征为连续值的时候假定特征符合高斯分布：\(g(x,n,u)\)</li>
</ol>
<h3 id="拉普拉斯校准"><a href="#拉普拉斯校准" class="headerlink" title="拉普拉斯校准"></a>拉普拉斯校准</h3><p>当某个类别下某个特征划分没有出现时，会有 \(P(a|y) = 0\)，就是导致分类器质量降低，所以此时引入 拉普拉斯校验，就是对每类别下所有划分的计数加 1。</p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ol>
<li>对小规模的数据表现很好，适合多分类任务，适合增量式训练。</li>
<li>对输入数据的表达形式很敏感。(离散、连续、极大值，极小值）。</li>
</ol>
<h2 id="逻辑回归和线性回归"><a href="#逻辑回归和线性回归" class="headerlink" title="逻辑回归和线性回归"></a>逻辑回归和线性回归</h2><p>LR回归是一个线性的二分类模型，主要是计算在某个样本特征下事件发生的概率，比如根据用户的浏览购买情况作为特征来计算它是否会购买这个商品，抑或是它是否会点击这个商品。然后LR的最终值是根据一个线性和函数再通过一个 <strong>sigmoid</strong> 函数来求得，这个线性和函数权重与特征值的累加以及加上偏置求出来的，所以在训练LR时也就是在训练线性和函数的各个权重值w。</p>
<p>$$h_w(x) = \frac{1}{1 + e^{-(w^Tx + b)}}$$</p>
<p>关于这个权重值 w 一般使用最大似然法来估计，假设现在有样本 \(x_i, y_i\)，其中 xi 表示样本的特征，\(y_i \epsilon 0, 1\)表示样本的分类真实值，\(y_i = 1\) 的概率是 \(p_i\)，则 \(y_i = 0\) 的概率是 \(1 − p_i\)，那么观测概率为:</p>
<p>$$p(y_i) = p_i^{y_i} * (1 - p_i)^{1 - y_i}$$</p>
<p>则最大似然估计为：</p>
<p>$$\prod h_w(x_i)^y_i * (1 - h_w(x_i))^{1 - y_i}$$</p>
<p>对这个似然函数取对数之后就会得到表达式：</p>
<p>$$L(w) = \sum_{i}^{N} \left ( y_i \ast logh_w(x_i) + (1 - y_i) \ast log(1 - h_w(x_i))\right )$$</p>
<p>对这个 L(w) 求极大值就可以得到 w 的估计值。</p>
<p>实际计算中，常改为求极小值，在前面加个负号即可。故求解问题就变成了这个最大似然函数的最优化问题，这里通常会采取随机梯度下降和拟牛顿迭代法来进行优化。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>LR 的损失函数为：</p>
<p>$$J(w) = - \frac{1}{N} \sum_{i = 1}^{N} (y_i \ast log(h_w(x_i)) + (1 - y_i) \ast log(1 - h_w(x_i)))$$</p>
<p>这里除以 N 是因为求的是最小均方误差，这样就变成了求 min(J(w))，</p>
<p>其更新 w 的过程为:</p>
<p>$$w := w - \alpha \ast \bigtriangledown J(w)$$</p>
<p>$$w := w - \alpha \frac{1}{N} \ast \sum_{i = 1}^{N} ((h_w(x_i) - y_i) \ast x_i)$$</p>
<p>其中 \(\alpha\) 为步长，直到 \(J(w)\) 不能再小时停止。</p>
<p>批量梯度下降法的最大问题是会陷入局部最优，并且每次在对当前样本计算 cost 的时候都需要遍历全部样本，这样计算速度会很慢，计算的时候可以转为矩阵乘法去更新整个 w 值。</p>
<p>有好多方法也采用随机梯度下降，它在计算 cost 的时候只计算当前代价，最终 cost 是在全部样本迭代一次求和得出，还有他在更新当前的参数 w 的时候并不是依次遍历样本，而是从所有样本中随机选择一条进行计算，这种方法收敛速度快，也可以避免局部最优，还容易并行。</p>
<p>$$w := w - \alpha \ast \left ( (h_w(x_j) - y_i) \ast x_i \right )$$</p>
<p>$$j \epsilon 1…N$$</p>
<p>SGD 可以改进的地方是使用动态的步长。</p>
<h3 id="其他的优化方法"><a href="#其他的优化方法" class="headerlink" title="其他的优化方法"></a>其他的优化方法</h3><ul>
<li>拟牛顿法</li>
<li>BFDS</li>
<li>L-BFGS</li>
</ul>
<p>优缺点： 无需选择学习率，更快，但也更复杂</p>
<h3 id="如何避免过拟合"><a href="#如何避免过拟合" class="headerlink" title="如何避免过拟合"></a>如何避免过拟合</h3><ol>
<li>减少 feature 的个数</li>
<li>正则化 (L1, L2)</li>
</ol>
<p>添加 L2 正则化之后的损失函数为：</p>
<p>$$J(w) = - \frac{1}{N} \sum_{i = 1}^{N} (y_i \ast log(h_w(x_i)) + (1 - y_i) \ast log(1 - h_w(x_i))) +\lambda \left | w \right |_2$$</p>
<p>同时 w 的更新变为:</p>
<p>$$w := w - \alpha \ast ( h_w(x_j) - y_j) \ast x_i ) - 2\alpha \ast w_j$$</p>
<p>这里 \(w_0\) 不受正则化影响。</p>
<h3 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h3><ol>
<li>实现简单，计算量小</li>
<li>容易欠拟合</li>
</ol>
<h2 id="KNN-算法"><a href="#KNN-算法" class="headerlink" title="KNN 算法"></a>KNN 算法</h2><p>给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类.</p>
<h3 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a>三要素</h3><ol>
<li>k 值的选择</li>
<li>距离的度量</li>
<li>分类决策规则</li>
</ol>
<h3 id="k-值的选择"><a href="#k-值的选择" class="headerlink" title="k 值的选择"></a>k 值的选择</h3><ol>
<li>k值越小表明模型越复杂，更加容易过拟合</li>
<li>但是 k 值越大，模型越简单，如果 k = N 就代表什么点都是训练集中类别最多的那个类。</li>
</ol>
<p><strong>所以一般k会取一个较小的值，然后用过交叉验证来确定<br>这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k</strong></p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>计算量大；</li>
<li>样本不均时会造成问题；</li>
<li>需要大量的内存。</li>
</ol>
<h3 id="Kd-树"><a href="#Kd-树" class="headerlink" title="Kd 树"></a>Kd 树</h3><p>Kd 树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）</p>
<h4 id="构造-Kd-树"><a href="#构造-Kd-树" class="headerlink" title="构造 Kd 树"></a>构造 Kd 树</h4><p>在 k 维的空间上循环找子区域的中位数进行划分的过程</p>
<p>假设现在有 k 维空间的数据集 T = {x1, x2, x3,…,xn}，xi = {a1, a2, a3,…,ak}</p>
<ol>
<li>首先构造根节点，以坐标 a1 的中位数 b 为切分点，将根结点对应的矩形区域划分为两个区域，区域 1 中 a1 &lt; b，区域 2 中 a1 &gt; b</li>
<li>构造叶子节点，分别以上面两个区域中 a2 的中位数作为切分点，再次将他们两两划分，作为深度为 1 的叶子节点</li>
<li>不断重复 2 操作，深度为 j 的叶子节点划分的时候，索取的 ai 的 i = j % k + 1，直到两个子区域没有实例时停止。</li>
</ol>
<h4 id="Kd-树的搜索"><a href="#Kd-树的搜索" class="headerlink" title="Kd 树的搜索"></a>Kd 树的搜索</h4><ol>
<li>首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi</li>
<li>将这个叶子节点认为是当前的 “近似最近点”</li>
<li>递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“</li>
<li>重复3的步骤，直到另一子区域与球体不相交或者退回根节点</li>
<li>最后更新的 ”近似最近点“ 与x真正的最近点</li>
</ol>
<h3 id="Kd-树进行-KNN-查找"><a href="#Kd-树进行-KNN-查找" class="headerlink" title="Kd 树进行 KNN 查找"></a>Kd 树进行 KNN 查找</h3><p>通过 Kd 树的搜索找到与搜索目标最近的点，这样 KNN 的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。</p>
<h3 id="Kd-树搜索的复杂度"><a href="#Kd-树搜索的复杂度" class="headerlink" title="Kd 树搜索的复杂度"></a>Kd 树搜索的复杂度</h3><p>当实例随机分布的时候，搜索的复杂度为 log(N)，N 为实例的个数，KD树更加适用于实例数量远大于空间维度的 KNN 搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。</p>
<h2 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM (支持向量机)"></a>SVM (支持向量机)</h2><p>对于样本点 (Xi,Yi) 以及 svm 的超平面：\(w^Tx_i + b = 0\)</p>
<ul>
<li>函数间隔：\(yi \ast ( w^Tx_i + b )\)</li>
<li>几何间隔： \(\frac{(yi \ast ( w^Tx_i + b )}{||w||}\)</li>
</ul>
<p>SVM 的基本思想是求解能正确划分训练样本并且其几何间隔最大化的超平面。</p>
<h3 id="线性-SVM-问题"><a href="#线性-SVM-问题" class="headerlink" title="线性 SVM 问题"></a>线性 SVM 问题</h3><p>$$argmax_{w,b}   \gamma$$</p>
<p>同时满足：</p>
<p>$$st. \frac{y_i(w^Tx_i + b)}{||w||} \geq \gamma $$</p>
<p>那么假设 \(\widehat{\gamma } = \gamma \ast ||w||\)，则问题转换为：</p>
<p>$$argmax \frac{\widehat{\gamma }}{||w||}$$</p>
<p>$$st. y_i(w^Tx_i + b) \geq 1$$</p>
<p>由于 \(\widehat{\gamma }\) 的成比例增减不会影响实际间距，所以这里的取 \(\widehat{\gamma } = 1\)，又因为 \(max(\frac{1}{||w||}) = min(\frac{1}{2} \ast ||w||^2）\)。</p>
<p>所以最终问题就变成了</p>
<p>$$argmin_{w,b}  \frac{1}{2} \ast ||w||^2  \gamma$$</p>
<p>$$st. y_i(w^Tx_i + b) \geq 1$$</p>
<p>这样就把原始问题转换为了一个凸的二次规划，可以将其转换为拉格朗日函数，然后使用对偶算法来求解。</p>
<h3 id="对偶求解"><a href="#对偶求解" class="headerlink" title="对偶求解"></a>对偶求解</h3><p>引进拉格朗日乘子 \(a = {a_1, a_2,…}\)，定义拉格朗日函数：</p>
<p>$$L(w, b, a) = \frac{1}{2} \ast ||w||^2 - \sum_{i=1}^{N} (\alpha_i \ast y_i (w^Tx_i + b)) + \sum_{i = 1}^{N} (\alpha_i)$$</p>
<p>根据对偶性质，原始问题就是对偶问题的极大极小</p>
<p>$$max min L(w, b, a)$$</p>
<p>先求 L 对 w，b 的极小，再求对 \(\alpha\) 的极大。第一步，相当于对 \(w, b) 求偏导并且令其等于 0</p>
<p>$$\bigtriangledown_w L(w, b, a) = w - \sum_{i = 1}^{N} a_iy_ix_i$$</p>
<p>$$\bigtriangledown_b L(w, b, a) = \sum_{i = 1}^{N} a_iy_i$$</p>
<p>带入后即得，</p>
<p>$$minL(w, b, a) = - \frac{1}{2} \ast \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_ia_jy_iy_j(x_i \cdot  x_j) + \sum_{i = 1}^{N} a_i$$</p>
<p>对上式求极大，即是对偶问题：</p>
<p>$$max - \frac{1}{2} \ast \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_ia_jy_iy_j(x_i \cdot x_j) + \sum_{i = 1}^{N} a_i$$</p>
<p>$$st. \sum_{i = 1}^{N}(\alpha_i y_i) = 0$$</p>
<p>$$a \geq 0, i = 1, 2, 3…$$</p>
<p>将求最大转为求最小，得到等价的式子为：</p>
<p>$$min \frac{1}{2} \ast \sum_{i = 1}^{N} \sum_{j = 1}^{N} (a_ia_jy_iy_j(x_i \cdot x_j) - \sum_{i = 1}^{N} a_i$$</p>
<p>$$st. \sum_{i = 1}^{N}(\alpha_i y_i) = 0$$</p>
<p>$$a \geq 0, i = 1, 2, 3…$$</p>
<p>假如求解出来的 \(\alpha^* (\alpha_1^\ast,…\alpha_n^\ast)\)</p>
<p>则得到最优的 w，b 分别为</p>
<p>$$w^ \ast = \sum_{i = 1}^{N}(\alpha_i^\ast y_ix_i)$$</p>
<p>$$b^ \ast = y_i - \sum_{i = 1}^{N}(\alpha_i^\ast y_i (x_i \cdot x_j))$$</p>
<p>所以，最终的决策分类面为：</p>
<p>$$f(x) = sign(\sum_{i = 1}^{N} a_i^\ast y_i(x \cdot x_i) + b^*)$$</p>
<p>即，分类决策函数只依赖于输入 x 与训练样本输入的内积。</p>
<h4 id="SVM-软间距最大化"><a href="#SVM-软间距最大化" class="headerlink" title="SVM 软间距最大化"></a>SVM 软间距最大化</h4><p>软间距最大化引用了松弛变量 \(\xi \)，将原问题的求解转换为：</p>
<p>$$argmin_{w,b} \frac{1}{2} \ast ||w||^2  + C \sum_{i = 1}^{N} \xi_i$$</p>
<p>$$y_i(w^Tx_i + b) \geq 1 - \xi_i$$</p>
<p>$$\xi_i \geq 0, i = 1,2,…N$$</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>优化目标为：</p>
<p>$$\sum [1 - y_i(w^Tx_i + b)]_+  + \lambda ||w||^2$$</p>
<p>其中 \([1 - y_i(w^Tx_i + b)]_+\) 称为折页损失函数，当其值小于等于 0 时，返回 0。</p>
<h3 id="为何要引入对偶算法"><a href="#为何要引入对偶算法" class="headerlink" title="为何要引入对偶算法"></a>为何要引入对偶算法</h3><ol>
<li>对偶问题往往更加容易求解。</li>
<li>可以很自然的引入核函数。</li>
</ol>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>将输入特征x（线性不可分）映射到高维特征 R 空间，可以在 R 空间上让 SVM 进行线性可以变，这就是核函数的作用。常见的核函数有：</p>
<ul>
<li>多项式核函数: \( K(x, z)=(x \ast z + 1)^p\)</li>
<li>高斯核函数: \( K(x, z) = exp(\frac{-(x - z)^2}{\sigma ^2})\) </li>
</ul>
<h3 id="SVM优缺点"><a href="#SVM优缺点" class="headerlink" title="SVM优缺点"></a>SVM优缺点</h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ol>
<li>使用核函数可以向高维空间进行映射</li>
<li>使用核函数可以解决非线性的分类</li>
<li>分类思想很简单，就是将样本与决策面的间隔最大化</li>
<li>分类效果较好</li>
</ol>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ol>
<li>对大规模数据训练比较困难</li>
<li>无法直接支持多分类，但是可以使用间接的方法来做</li>
</ol>
<h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>SMO是用于快速求解SVM的。它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：</p>
<ol>
<li>其中一个是严重违反KKT条件的一个变量。</li>
<li>另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。</li>
</ol>
<h3 id="SVM多分类问题"><a href="#SVM多分类问题" class="headerlink" title="SVM多分类问题"></a>SVM多分类问题</h3><ul>
<li>直接法</li>
</ul>
<p>直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难）</p>
<ul>
<li>间接法</li>
</ul>
<ol>
<li>一对多</li>
</ol>
<p>其中某个类为一类，其余 n-1 个类为另一个类，比如 A,B,C,D 四个类，第一次 A 为一个类，{B,C,D} 为一个类训练一个分类器，第二次B为一个类，{A,C,D}为另一个类，按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x)，f2(x)，f3(x) 和 f4(x)，取其最大值为分类器 (这种方式由于是1对M分类，会存在偏置，很不实用)</p>
<ol>
<li>一对一(libsvm实现的方式)</li>
</ol>
<p>任意两个类都训练一个分类器，那么n个类就需要 n*(n-1)/2 个 SVM 分类器。<br>还是以A,B,C,D为例，那么需要 {A,B},{A,C},{A,D},{B,C},{B,D},{C,D} 为目标共 6 个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要 n*(n-1)/2 个分类器代价太大）</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树是一颗依托决策而建立起来的树。</p>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><ol>
<li>首先是针对当前的集合，计算每个特征的信息增益</li>
<li>然后选择信息增益最大的特征作为当前节点的决策决策特征</li>
<li>根据特征不同的类别划分到不同的子节点（比如年龄特征有青年，中年，老年，则划分到3颗子树）</li>
<li>然后继续对子节点进行递归，直到所有特征都被划分</li>
</ol>
<p>熵的定义：</p>
<p>$$H(D) = - \sum_{i = 1}^{n} p_i \ast log(p_i)$$</p>
<p>一个属性中的某个类别 Di 的熵，为：</p>
<p>$$H(D|D_i) = - \sum_{i}^{ } \frac{|D_i|}{|D|} \ast H(D_i)$$</p>
<p>$$H(D_i) = - \sum_i^K \frac{D_{ik}}{D_i} log_2 \frac{D_{ik}}{D_i}$$</p>
<p>信息增益表示分类目标的熵减去当前属性的熵，增益越大，分类能力越强。</p>
<p>$$Gain(C, A) = H(D) - H(D|D_i)$$</p>
<h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>设树的叶子节点个数为 \(T\)，\(t\) 为其中一个叶子节点，该叶子节点有\(N_t\)个样本，其中 k 类的样本有\(N_{tk}\)个，\(H(t)\)为叶子节点上的经验熵，则损失函数定义为：</p>
<p>$$C_t(T) = \sum (N_t \ast H(t)) + \lambda |T|$$</p>
<p>其中，</p>
<p>$$H_t(T) = - \sum \frac{N_{tk}}{N_t} \ast log \frac{N_{tk}}{N_t}$$</p>
<p>代入可以得到</p>
<p>$$C_t(T) = \sum \sum N_{tk} \ast log \frac{N_{tk}}{N_t} + \lambda |T|$$</p>
<p>其中 \(\lambda |T| \) 为正则化项， \(\lambda \) 是用于调节比率，决策树的生成只考虑了信息增益。 </p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>它是ID3的一个改进算法，使用信息增益率来进行属性的选择</p>
<p>$$splitInformation(D | A) = - \sum_{i} \frac{D_i}{D} \ast log_2 \frac{D_i}{D}$$</p>
<p>$$GainRatio(D,A) = \frac{Gain(D,A)}{splitInformation(D, A)}$$</p>
<h4 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h4><p>准确率高，但是子构造树的过程中需要进行多次的扫描和排序，所以它的运算效率较低。</p>
<h3 id="Cart"><a href="#Cart" class="headerlink" title="Cart"></a>Cart</h3><p>分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：</p>
<ul>
<li>分类树：基尼指数最小化(gini_index）</li>
<li>回归树：平方误差最小化</li>
</ul>
<h4 id="分类树："><a href="#分类树：" class="headerlink" title="分类树："></a>分类树：</h4><ol>
<li>首先是根据当前特征计算他们的基尼增益</li>
<li>选择基尼增益最小的特征作为划分特征</li>
<li>从该特征中查找基尼指数最小的分类类别作为最优划分点</li>
<li>将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于</li>
<li>针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值</li>
</ol>
<p><strong>gini</strong> 用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）</p>
<p>$$gini(a_i) = 1 - \sum_i(p_i^2)$$</p>
<p>\(p_i\) ：当前数据集中第 i 类样本的比例；</p>
<p><strong>gini</strong> 越小，表示样本越均匀，越大越不均匀。</p>
<p>基尼增益：</p>
<p>$$giniGain = \sum_i (\frac{N_i}{N} \ast gini(a_i))$$</p>
<p>\(\frac{N_i}{N}\) 表示当前类别占所有类别的概率<br>最终Cart选择GiniGain最小的特征作为划分特征。</p>
<h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><p>回归树是以平方误差最小化的准则划分为两块区域。</p>
<ol>
<li>遍历特征计算最优的划分点s，使其最小化的平方误差是：\(min(min(\sum_i^{R1}((y_i−c_1)^2)) + min(\sum_i^{R2}((y_i−c_2)^2)))\)<br>计算根据s划分到左侧和右侧子树的目标值与预测值之差的平方和最小，这里的预测值是两个子树上输入 \(x_i\) 样本对应 \(y_i\) 的均值。</li>
<li>找到最小的划分特征 j 以及其最优的划分点 s，根据特征 j 以及划分点 s 将现有的样本划分为两个区域，一个是在特征 j 上小于等于 s，另一个在在特征 j 上大于 s</li>
<li>进入两个子区域按上述方法继续划分，直到到达停止条件</li>
</ol>
<h3 id="停止条件"><a href="#停止条件" class="headerlink" title="停止条件"></a>停止条件</h3><ol>
<li>直到每个叶子节点都只有一种类型的记录时停止。（这种方式很容易过拟合）</li>
<li>另一种时当叶子节点的记录树小于一定的阈值或者节点的信息增益小于一定的阈值时停止。</li>
</ol>
<h3 id="决策树的分类与回归"><a href="#决策树的分类与回归" class="headerlink" title="决策树的分类与回归"></a>决策树的分类与回归</h3><h4 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h4><p>输出叶子节点中所属类别最多的那一类。</p>
<h4 id="回归树-1"><a href="#回归树-1" class="headerlink" title="回归树"></a>回归树</h4><p>输出叶子节点中各个样本值的平均值。</p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出（也就是投票的思想）</p>
<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><ol>
<li>现在有 N 个训练样本，每个样本的特征为 M 个，需要建 K 颗树。</li>
<li>从 N 个训练样本中有放回的取 N 个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差）。</li>
<li>从 M 个特征中取m个特征左右子集特征。 (m &lt;&lt; M)</li>
<li>对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类。</li>
<li>重复2的过程K次，即可建立森林。</li>
</ol>
<h3 id="预测过程"><a href="#预测过程" class="headerlink" title="预测过程"></a>预测过程</h3><ol>
<li>将预测样本输入到K颗树分别进行预测。</li>
<li>如果是分类问题，直接使用投票的方式选择分类频次最高的类别。</li>
<li>如果是回归问题，使用分类之后的均值作为结果。</li>
</ol>
<h3 id="参数问题"><a href="#参数问题" class="headerlink" title="参数问题"></a>参数问题</h3><ol>
<li>这里的一般取 m = sqrt(M)</li>
<li>关于树的个数K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）</li>
<li>树的最大深度，（太深可能可能导致过拟合）</li>
<li>节点上的最小样本数、最小信息增益</li>
</ol>
<h3 id="泛化误差估计"><a href="#泛化误差估计" class="headerlink" title="泛化误差估计"></a>泛化误差估计</h3><p>使用 <strong>oob（out-of-bag）</strong> 进行泛化误差的估计，将各个树的未采样样本作为预测样本（大约有36.8%），使用已经建立好的森林对各个预测样本进行预测，预测完之后最后统计误分得个数占总预测样本的比率作为 RF 的 oob 误分率。</p>
<h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><ul>
<li>ID3 算法：处理离散值的量</li>
<li>C45 算法：处理连续值的量</li>
<li>Cart 算法：离散和连续 两者都合适。</li>
</ul>
<h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><ol>
<li>能够处理大量特征的分类，并且还不用做特征选择</li>
<li>在训练完成之后能给出哪些feature的比较重要</li>
<li>训练速度很快</li>
<li>很容易并行</li>
<li>实现相对来说较为简单</li>
</ol>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>GBDT的精髓在于训练的时候都是以上一颗树的残差为目标，这个残差就是上一个树的预测值与真实值的差值。</p>
<p><strong>比如，当前样本年龄是18岁，那么第一颗会去按18岁来训练，但是训练完之后预测的年龄为12岁，差值为6，<br>所以第二颗树的会以6岁来进行训练，假如训练完之后预测出来的结果为6，那么两棵树累加起来就是真实年龄了，<br>但是假如第二颗树预测出来的结果是5，那么剩余的残差1就会交给第三个树去训练。</strong></p>
<p>Boosting 的好处就是每一步的参加就是变相了增加了分错 instance 的权重，而对已经对的 instance 趋向于0，这样后面的树就可以更加关注错分的 instance 的训练了。</p>
<h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3><p>Shrinkage 认为，每次走一小步逐步逼近的结果要比每次迈一大步逼近结果更加容易避免过拟合。</p>
<p>$$y_i = y_{i-1} + step \ast y_i$$</p>
<h3 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h3><ul>
<li>树的个数 100 ~ 10000</li>
<li>叶子的深度 3 ~ 8</li>
<li>学习速率 0.01 ~ 1</li>
<li>叶子上最大节点树 20</li>
<li>训练采样比例 0.5 ~ 1 </li>
<li>训练特征采样比例 sqrt(num)</li>
</ul>
<h3 id="优缺点：-1"><a href="#优缺点：-1" class="headerlink" title="优缺点："></a>优缺点：</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>精度高</li>
<li>能处理非线性数据</li>
<li>能处理多特征类型</li>
<li>适合低维稠密数据</li>
</ol>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>并行麻烦（因为上下两颗树有联系）</li>
<li>多分类的时候 复杂度很大                         </li>
</ol>
<h3 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h3><ul>
<li><a href="http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/" target="_blank" rel="external">机器学习常见算法个人总结（面试用）</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/13/算法面试总结：大数据题/" itemprop="url">
                  面试总结：大数据题
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-13T12:22:45+08:00" content="2017-02-13">
              2017-02-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/面试总结/" itemprop="url" rel="index">
                    <span itemprop="name">面试总结</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/13/算法面试总结：大数据题/" class="leancloud_visitors" data-flag-title="面试总结：大数据题">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3 id="1-认识布隆过滤器"><a href="#1-认识布隆过滤器" class="headerlink" title="1. 认识布隆过滤器"></a>1. 认识布隆过滤器</h3><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>不安全网页的黑名单包含 100 亿个黑名单网页，每个网页的 URL 最多占用 64 B。限制响应实现一种网页过滤系统，可以根据网页的 URL 判断该网页是否在黑名单上，请设计该系统。</p>
<h4 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h4><ol>
<li>该系统允许有万分之一以下的判断失误率。</li>
<li>使用的额外空间不要超过 30 GB。</li>
</ol>
<h4 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h4><p>提示：一个布隆过滤器精确地代表一个集合，并且可以精确判断一个元素是否在集合中。</p>
<p>补充：一个优秀的哈希函数能够做到很多不同的输入值所得到的返回值非常均匀地分布在 S 上，那么将所有的返回值对 m 取余，可以认为所有的返回值也会均匀地分布在 0 ~ m-1 的空间上。</p>
<p>布隆过滤器：假设有一个长度为 m 的 bit 类型的数组，即数组中的每一个位置只占一个 bit，每一个 bit 只有 0 和 1 两种状态。再假设一共有 k 个哈希函数，这些函数的输入域 S 都大于或等于 m，并且这些哈希函数都足够优秀，彼此之间也完全独立。那么对同一个输入对象（假设是一个字符串记为 URL），经过 k 个哈希函数算出来的结果也是独立的，可能相同，也可能不同，但彼此独立。对算出来的每一个结果都对 m 取余，然后在 bit array 上把相应的位置设为 1。我们把 bit 类型的数组记为 bitMap。至此，一个输入对象对 bitMap 的影响过程就结束了，即 bitMap 中的一些位置会被涂黑。接下来按照该方法处理所有的输入对象，每个对象都可能把 bitMap 中的一些白位置涂黑，也可能遇到已经涂黑的位置，遇到已经涂黑的位置让其继续为黑即可。处理完所有的输入对象后，可能 bitMap 中已经有相当多的位置被涂黑。至此，一个布隆过滤器生成完毕，这个布隆过滤器代表之前所有输入对象组成的集合。</p>
<p>那么在检查阶段时，假设一个对象为 a，想检查它是否是之前的输入对象，就把 a 通过 k 个哈希函数算出 k 个值，然后把 k 个值取余，就得到在 0 ~ m-1 范围上的 k 个值。接下来在 bitMap 上看这些位置是不是都为黑，如果有一个不为黑，就说明 a 一定不在这个集合里。如果都为黑，说明 a 在这个集合里。</p>
<p>如果 bitMap 的大小 m 相比输入对象的个数 n 过小，失误率会变大。接下来介绍根据 n 的大小和我们想要达到的失误率 p，如果确定布隆过滤器的大小 m 和哈希函数的个数 k，最后是布隆过滤器的失误率分析。</p>
<p>比如，黑名单样本的个数为 100 亿个，记为 n；失误率不能超过 0.01%，记为 p；每个样本的大小为 64 B，这个信息不会影响布隆过滤器的大小，只和选择哈希函数有关，一般的哈希函数都可以接收 64 B 的输入对象，所以使用布隆过滤器还有一个好处是不用顾忌单个样本的大小，它丝毫不能影响布隆过滤器的大小。</p>
<p>所以 n = 100 亿，p = 0.01%，布隆过滤器的大小 m 由以下公式确定：</p>
<p>$$m  = - \frac{n \times ln p}{(ln 2)^2}$$</p>
<p>根据公式计算出 m = 19.19n，向上取整为 20n，即需要 2000 亿个 bit，也就是 25 GB。</p>
<p>哈希函数的个数由以下公式决定：</p>
<p>$$k  = ln 2 \times \frac{m}{n} = 0.7 \times \frac{m}{n}$$</p>
<p>计算出哈希函数个数为 k = 14 个。</p>
<p>然后用 25 GB 的 bitMap 再单独实现 14 个哈希函数，根据如上描述生成布隆过滤器即可。</p>
<p>因为我们在确定布隆过滤器大小的过程中选择了向上取整，所以还要用如下公式确定布隆过滤器真实的失误率为：</p>
<p>$$（1 - e^{- \frac{nk}{m}})^k$$</p>
<p>根据这个公式算出真实的失误率为0.006%，这是比 0.01 % 更低的失误率。</p>
<h3 id="2-只用-2GB-内存在-20-亿个整数中找到出现次数最多的数"><a href="#2-只用-2GB-内存在-20-亿个整数中找到出现次数最多的数" class="headerlink" title="2. 只用 2GB 内存在 20 亿个整数中找到出现次数最多的数"></a>2. 只用 2GB 内存在 20 亿个整数中找到出现次数最多的数</h3><h4 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h4><p>在一个包含 20 亿个全是 32 位整数的大文件，在其中找到出现次数最多的数。</p>
<h4 id="要求-1"><a href="#要求-1" class="headerlink" title="要求"></a>要求</h4><p>内存限制为 2 GB</p>
<h4 id="解答-1"><a href="#解答-1" class="headerlink" title="解答"></a>解答</h4><p>想要在很多整数中找到出现次数最多的数，通常的做法是使用哈希表对出现的每一个数做词频统计，哈希表的 key 是某一个整数，value 是这个数出现的次数。就本题而言，一共有 20 亿个数，哪怕只是一个数出现 20 亿次，用 32 位的整数也可以表示其出现的次数而不会产生溢出，所以哈希表的 key 需要占用 4B，value 也是 4B。那么哈希表的一条记录（key，value）需要占用 8B，当哈希表记录数为 2 亿个时，至少需要 1.6GB 的内存。</p>
<p>但如果 20 亿个数中不同的数超过 2亿种，最极端的情况是 20 亿个数都不相同，那么在哈希表中可能需要产生 20 亿条记录，这样内存明显不够用，所以一次性用哈希表统计 20 亿个数非常冒险。</p>
<p>解决的方法是把包含 20 亿个数的大文件用哈希函数分成 16 个小文件，根据哈希函数的性质，同一种数不可能被哈希到不同的小文件上，同时每个小文件中不同的数一定不会大于 2 亿种，假设哈希函数足够好。然后对每一个小文件用哈希表来统计其中每种数出现的次数，这样我们就得到了 16 个小文件中各自出现次数最多的数，还有各自的次数统计。接下来只要选出 16 个小文件各自的第一名中谁出现的次数最多即可。</p>
<h3 id="3-40-亿个非负整数中找到没出现的数"><a href="#3-40-亿个非负整数中找到没出现的数" class="headerlink" title="3. 40 亿个非负整数中找到没出现的数"></a>3. 40 亿个非负整数中找到没出现的数</h3><h4 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h4><p>32 位无符号整数的范围是 0 ~ 4294967295，现在有一个正好包含 40 亿个无符号整数的文件，所以在整个范围中必然有没出现过的数。可以使用最多 1 GB 的内存，怎么找到所有没出现过的数。</p>
<h4 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h4><p>内存限制为 10 MB</p>
<h4 id="解答-2"><a href="#解答-2" class="headerlink" title="解答"></a>解答</h4><p>如果用哈希表来保存出现过的数，那么如果 40 亿个数都不同，则哈希表的记录数为 40 亿条，存一个 32 位整数需要 4B，所有最差情况下需要 40 亿 * 4B = 160 亿字节，大约需要 16 GB的空间，不符合要求。</p>
<p>哈希表需要占用很多空间，我们可以使用 bit map 的方式来表示数出现的情况。具体的，申请一个长度为 4294967295 的 bit 类型的数组上的每个位置只可以表示 0 或 1 状态。8 个 bit 为 1B，所以长度为 4294967295 的 bit 类型的数组占用 500 MB 空间。</p>
<p>进阶问题：现在只有 10 MB 的内存，但也只要求找到其中一个没出现的数即可，首先，0 ~ 4294967295 这个范围是可以平均分成 64 个区间的，每个区间是 67108864 个数。因为一共只有 40 亿个数，所以，如果统计落在每一个区间上的数有多少，肯定至少一个区间上的计数少于 67108864。利用这一点可以找出其中一个没出现的数。</p>
<p>具体过程如下： 第一次遍历时，先申请长度为 64 的整型数组 countArr[0…63]，countArr[i] 用来统计区间 i 上的数有多少。遍历 40 亿个数，根据当前数是多少来决定哪一个区间上的计数增加。例如，如果当前数是 3422552090，3422552090 / 67108864 = 51，所以第 51 区间上的计数增加，遍历完所有数之和遍历 countArr，必然会有某一个位置上的值小于 67108864，表示第 i 区间上至少有一个数没出现过。我们肯定会至少找到一个这样的区间。此时内存使用为 64 * 4B，是非常小的。</p>
<p>假设我们找到第 37 区间上的计数小于 67108864，以下为第二次遍历的过程：</p>
<ol>
<li>申请长为 67108864 的 bit map，这占用大约 8 MB 的空间。</li>
<li>再遍历一次 40 亿个数，此时的遍历只关注落在第 37 区间上的数。</li>
<li>之后的方法同普通方法类似。</li>
<li>遍历完 40 亿个数之后，在 bitArr 上必然存在没被设置为 1 的位置，假设第 i 个位置上的值没设置为 1，那么 67108864 * 37 + i 这个数就是一个没出现过的数。</li>
</ol>
<h3 id="4-找到-100-亿个-URL-中重复的-URL-以及搜索词汇的-top-K-问题"><a href="#4-找到-100-亿个-URL-中重复的-URL-以及搜索词汇的-top-K-问题" class="headerlink" title="4. 找到 100 亿个 URL 中重复的 URL 以及搜索词汇的 top K 问题"></a>4. 找到 100 亿个 URL 中重复的 URL 以及搜索词汇的 top K 问题</h3><h4 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a>题目</h4><p>有一个包含 100 亿个 URL 的大文件，假设每个 URL 占用 64 B，请找出其中所有重复的 URL。</p>
<h4 id="补充题目"><a href="#补充题目" class="headerlink" title="补充题目"></a>补充题目</h4><p>某搜索公司一天的用户搜索词汇是海量的，请设计一种求出每天最热 100 词汇的可行办法。</p>
<h4 id="解答-3"><a href="#解答-3" class="headerlink" title="解答"></a>解答</h4><p>解决这种问题的常规方法是，把大文件通过哈希函数分配到机器，或者通过哈希函数把大文件拆成小文件。一直进行这种划分，直到划分的结果满足机器资源的限制。</p>
<p>例如，将 100 亿字节的大文件通过哈希函数分配到 100 台机器上，然后每一台机器分别统计分给自己的 URL 中是否有重复的 URL，同时哈希函数的性质决定了同一条 URL 不可能分给不同的机器；或者可以在单机上将大文件通过哈希函数拆成 1000 个小文件，对每一个小文件再利用哈希表遍历，找出重复的 URL；或者在分给机器或拆完文件之后，进行排序，排序过会再看是否会有重复 URL 出现。大数据问题的处理都离不开分流要么是哈希函数把大文件的内容分配给不同的机器，要么是哈希函数把大文件拆成小文件，然后处理每一个小数量的集合。</p>
<p>补充题目最开始还是哈希分流的思路来处理，把包含百亿数据量的词汇文件分流到不同的机器上，具体多少台机器由题目来确定。对每一台机器来说，如果分到的数据量依然很大，可以再用哈希函数把每台机器的分流文件拆成更小的文件处理。处理每一个小文件的时候，哈希表统计每种词及其词频，哈希表记录建立完成后，再遍历哈希表，遍历哈希表的过程中使用大小为 100 的小根堆来选出每一个小文件的 top 100。每一个小文件都有自己的词频的小根堆，将小根堆的词按照词频排序，就得到了每个小文件的排序后 top 100。然后把各个小文件排序后的 top 100 进行外排序或者继续利用小根堆，就可以选出每台机器上的 top 100。不同机器之间的 top 100 再进行外排序或者继续利用小根堆，最终求出整个百亿数据量中的 top 100。对于 top k 的问题，除哈希函数分流和用哈希表做词频统计之外，还经常用堆结构和外排序的手段进行处理。</p>
<h3 id="5-40-亿个非负整数中找到出现两次的数和所有数的中位数"><a href="#5-40-亿个非负整数中找到出现两次的数和所有数的中位数" class="headerlink" title="5. 40 亿个非负整数中找到出现两次的数和所有数的中位数"></a>5. 40 亿个非负整数中找到出现两次的数和所有数的中位数</h3><h4 id="题目-4"><a href="#题目-4" class="headerlink" title="题目"></a>题目</h4><p>40 亿个非负整数中找到出现两次的数和所有数的中位数</p>
<h4 id="补充题目-1"><a href="#补充题目-1" class="headerlink" title="补充题目"></a>补充题目</h4><p>可以使用最多 10 MB 的内存，怎么找到这 40 亿个整数的中位数</p>
<h4 id="解答-4"><a href="#解答-4" class="headerlink" title="解答"></a>解答</h4><p>可以使用 bit map 的方式来表示数出现的情况。申请一个长度为 4294967295 <em> 2 的 bit 类型的数组 bitArr，用 2 个位置表示一个数出现的词频， 1B 占用 8 个 bit，所以长度为 4294967295 </em> 2 的 bit 类型的数组占用 1 GB 空间，</p>
<p>遍历这 40 亿个无符号数，如果再次遇到 num， 就把 bitArr[num<em>2 + 1] 和 bitArr[num </em> 2] 设置为 01，如果第二次遇到 num，则设置为 10，如果第三次遇到 num，就把它设置为 11，以后再遇到 num，发现此时 bitArr[num<em>2 + 1] 和 bitArr[num </em> 2] 已经被设置为 11，就不再做任何设置。遍历完成后，再次遍历 bitArr，如果发现 bitArr[num<em>2 + 1] 和 bitArr[num </em> 2] 设置为 10，那么 i 就是出现了两次的数。</p>
<p>对于补充问题，用分区间的方式处理，长度为 2 MB 的无符号整型数组占用的空间为 8 MB，所以将区间的数量定为 4294967295/2M，向上取整为 2148 个区间。</p>
<p>申请一个长度为 2148 的无符号整型数组 arr[0…2147]，arr[i]表示第 i 区间有多少个数。arr 必然小于 10 MB。然后遍历 40 亿个数，如果遍历到当前数为 num，先看 num 落在哪个区间上（num / 2M)，然后将对应的进行arr[num/2M] ++ 操作，这样遍历下来，就得到了每一个区间的数的出现状况，通过累加每个区间的出现次数，如果遍历到当前数为 num，先看 num 落在哪个区间上（num / 2M），然后将对应的进行 arr[num/2M]++ 操作。这样遍历下来，就得到了每一个区间的数的出现状况，通过累加每个区间的出现次数，就可以找到 40 亿个数的中位数到底落在哪个区间。假设为第 K 区间。</p>
<p>接下来申请一个长度为 2MB 的无符号整型数组 countArr[0..2M-1]，占用空间 8MB。然后再遍历 40 亿个数，此时只关心处在第 K 区间的数记为 numi，其他的数省略，然后将 countArr[numi-K*2M]++，也就是只对第 K 区间的数做频率统计。这次遍历完 40 亿个数之后，就得到了第 K 区间的词频统计结果 countArr，最后只在第 K 区间上找到相应的第几个数字即可。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/07/Spark实践-3-Spark-SQL-与数据仓库/" itemprop="url">
                  Spark实践 (3): Spark SQL 与数据仓库
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-07T10:19:48+08:00" content="2017-02-07">
              2017-02-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/07/Spark实践-3-Spark-SQL-与数据仓库/" class="leancloud_visitors" data-flag-title="Spark实践 (3): Spark SQL 与数据仓库">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Spark-SQL-基础"><a href="#Spark-SQL-基础" class="headerlink" title="Spark SQL 基础"></a>Spark SQL 基础</h3><p>使用 Spark SQL 有 2 种方式，一种是通过写 SQL 来进行计算，另外一种是在 Spark 程序中，通过领域 API 的形式来操作数据（被抽象为 DataFrame）。</p>
<h4 id="分布式-SQL-引擎"><a href="#分布式-SQL-引擎" class="headerlink" title="分布式 SQL 引擎"></a>分布式 SQL 引擎</h4><p>作为分布式引擎，有两种运行方式，一种是 JDBC/ODBC Server，另一种是使用 Spark SQL 命令行。在正式环境下，使用前者比较好。</p>
<h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>通过写 SQL 来使用 Spark SQL 和 hive 区别不大，这里不再详细介绍，稍微提一下的是，有一些 hive 的特性是 Spark SQL 不支持的，最主要的是 Hive 的 bucker 表，使用散列的方式对 hive 表进行分区。</p>
<p>DataFrame 具有和 RDD 类似的概念，但还增加了列的概念。</p>
<p>在 Spark 中使用 DataFrame 的过程，可以分为 4 步，</p>
<ol>
<li>初始化环境，一般是创建一个 SQLContext 对象。</li>
<li>创建一个 DataFrame，可以来源于 RDD 或其他数据源。</li>
<li>调用 DataFrame 操作，是一种领域特定的 API，可以实现所有的 SQL 功能。</li>
<li>可以直接通过函数执行 SQL 语句。</li>
</ol>
<ul>
<li><p>创建 SQLContext。</p>
<pre><code>val sc: SparkContext
val sqlContext = new SQLContext(sc)
</code></pre></li>
<li><p>创建 DataFrame</p>
</li>
</ul>
<p>环境初始化之后，就可以创建 DataFrame 了，主要有两种创建方式。</p>
<ol>
<li><p>从 RDD 创建，又分 2种：</p>
<ul>
<li><p>使用 Scala 反射。</p>
</li>
<li><p>程序指定，略繁杂，但是可以运行时指定。</p>
</li>
</ul>
</li>
<li><p>从其他数据源创建。</p>
</li>
</ol>
<h4 id="使用反射的方法从-RDD-创建-DataFrame"><a href="#使用反射的方法从-RDD-创建-DataFrame" class="headerlink" title="使用反射的方法从 RDD 创建 DataFrame"></a>使用反射的方法从 RDD 创建 DataFrame</h4><p>这个方法是先定义一个 case class，参数名即为列名，然后将 RDD 的成员转换成 case class 类型，包含 case class 的 RDD 可以通过反射方式被隐式转换成 DataFrame，case class 的参数名会成为表的列名，然后就可以注册成一张表。</p>
<p>这种方法前提是在写程序之前就已经知道了数据格式，可以预先设定表的模式。</p>
<pre><code>def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(&quot;SparkSQLSimpleExample&quot;)
    val sc = new SparkContext()

    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._
    case class Person(name: String, age: Int)
    val rdd = sc.textFile(&quot;path/to/file&quot;).map(_.split(&quot;,&quot;))
    // 包含了 case class 的 RDD
    val rddContainingCaseClass = rdd.map(p =&gt; Person(p(0), p(1).trim.toInt))
    // 被隐式转换成 DataFrame
    val people = rddContainingCaseClass.toDF()
    // 将 DataFrame 的内容打印到标准输出
    people.show()
  }
</code></pre><h4 id="使用程序动态从-RDD-创建-DataFrame"><a href="#使用程序动态从-RDD-创建-DataFrame" class="headerlink" title="使用程序动态从 RDD 创建 DataFrame"></a>使用程序动态从 RDD 创建 DataFrame</h4><p>当 case class 无法提前知道数据格式时，可以在运行时动态指定表模式来从 RDD 创建 DataFrame。具体步骤如下：</p>
<ol>
<li>从原来的 RDD 创建一个新的 RDD，成员是 Row 类型，包含所有列。</li>
<li>创建一个 StructType 类型的表模式，其结构与步骤 1 中创建的 RDD 的 Row 结构相匹配。</li>
<li><p>使用 SQLContext.createDataFrame 方法将表模式应用到步骤 1 创建的 RDD 上。</p>
<pre><code>val sqLContext = new SQLContext(sc)
// 普通的 RDD
val people = sc.textFile(&quot;path/to/file&quot;)
// 字符串格式的表模式
val schemaString = &quot;name age&quot;
// 根据字符串格式的表模式创建结构化的表模式，用 StructType 保存
val schema =
      StructType(
        schemaString.split(&quot; &quot;).map(fieldName =&gt; StructField(fieldName, StringType, true))
      )
// 将普通 RDD 的成员转换成 Row 对象
val rowRDD = people.map(_.split(&quot;,&quot;)).map(p =&gt; Row(p(0), p(1).trim))
// 将模式作用到 RDD 上，生成 DataFrame
val peopleDataFrame = sqLContext.createDataFrame(rowRDD, schema)
peopleDataFrame.show()
</code></pre></li>
</ol>
<h4 id="从其他数据源生成-DataFrame"><a href="#从其他数据源生成-DataFrame" class="headerlink" title="从其他数据源生成 DataFrame"></a>从其他数据源生成 DataFrame</h4><p>Spark 提供了统一的接口，可以很方便地从其他数据源创建 DataFrame，例如：</p>
<pre><code>val df = sqlContext.read.json(&quot;path/to/file.json&quot;)
df.show()
</code></pre><h4 id="DataFrame-基本操作"><a href="#DataFrame-基本操作" class="headerlink" title="DataFrame 基本操作"></a>DataFrame 基本操作</h4><pre><code>// select * from 
df.show()  

// select name from 
df.select(&quot;name&quot;).show()

// select name, age + 1 from
df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show()

// select * from  xxx where age &gt; 21
df.filter(df(&quot;age&quot;) &gt; 21).show()

// select age, count(*) from xxx group by age
df.groupBy(&quot;age&quot;).count().show()

// 使用 registerTempTable 方法将 Dataframe 注册成一张表：
df.registerTempTable(&quot;people&quot;)

// 之后可以使用纯 SQL 来访问
val result = sqlContext.sql(&quot;SELECT * FROM people&quot;)
</code></pre><h4 id="DataFrame-数据源"><a href="#DataFrame-数据源" class="headerlink" title="DataFrame 数据源"></a>DataFrame 数据源</h4><p>DataFrame 支持非常多类型的数据源，包括 Hive、Avro、Parquet、ORC、JSON、JDBC。而 Spark 提供了统一的读写接口。</p>
<p>通过数据源加载数据时，默认的类型是 Parquet，这是一种大数据计算中最常用的列式存储格式：</p>
<pre><code>val df = sqlContext.read.load(&quot;path/to/file.parquet&quot;)
</code></pre><p>对于其他类型，可以使用 format 指定：</p>
<pre><code>val df = 
    sqlContext.read.format(&quot;json&quot;).load(&quot;path/to/file.json&quot;)
</code></pre><p>保存数据时和加加载数据方法类似。</p>
<h3 id="Spark-SQL-原理和运行机制"><a href="#Spark-SQL-原理和运行机制" class="headerlink" title="Spark SQL 原理和运行机制"></a>Spark SQL 原理和运行机制</h3><h4 id="Catalyst-执行优化器"><a href="#Catalyst-执行优化器" class="headerlink" title="Catalyst 执行优化器"></a>Catalyst 执行优化器</h4><p>Catalyst 是 Spark SQL 执行优化器的代号，所有 Spark SQL 语句最终都能通过它来解析、优化，最终生成可以执行的 Java 字节码。</p>
<p>Catalyst 最主要的数据结构是树，所有 SQL 语句都会用树结构来存储，树中的每个节点有一个类（class），以及 0 或多个子节点。Scala 中定义的新的节点类型都是 TreeNode 这个类的子类。</p>
<p>Catalyst 另外一个重要的概念是规则。基本上，所有优化都是基于规则的。可以用规则对树进行操作，树中的节点是只读的，所以树也是只读的。规则中定义的函数可能实现从一棵树转换成一颗新树。</p>
<p>整个 Catalyst 的执行过程可以分为以下 4 个阶段：</p>
<ul>
<li>分析阶段，分析逻辑树，解决引用</li>
<li>逻辑优化阶段</li>
<li>物理计划阶段，Catalyst 会生成多个计划，并基于成本进行对比</li>
<li>代码生成阶段</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/06/Spark实践-2-Spark-内核/" itemprop="url">
                  Spark实践 (2): Spark 内核
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-06T17:43:45+08:00" content="2017-02-06">
              2017-02-06
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/06/Spark实践-2-Spark-内核/" class="leancloud_visitors" data-flag-title="Spark实践 (2): Spark 内核">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Spark-核心数据结构-RDD"><a href="#Spark-核心数据结构-RDD" class="headerlink" title="Spark 核心数据结构 RDD"></a>Spark 核心数据结构 RDD</h3><p>RDD 全称是“弹性分布式数据集”。首先，它是一个数据集；其次，RDD 是分布式存储的。里面的成员被水平切割成小的的数据块，分散在集群的多个节点上，便于对 RDD 里面的数据进行并行计算。最后，RDD 的分布式弹性的，不是固定不变的。RDD 的一些操作可以被拆分成对各数据块直接计算，不涉及其他节点，比如 map。这样的操作一般在数据块所在的节点上直接进行，不影响 RDD 的分布，除非某个节点故障需要转换到其他节点上。但是在有些操作中，例如 groupBy，必须要访问 RDD 的所有数据块。</p>
<p>RDD 还具有的特点是：</p>
<ol>
<li>RDD 是只读的，一旦生成，内容就不能修改了。这样的好处是让整个系统的设计相对简单，比如并行计算时不用考虑数据互斥的问题。</li>
<li>RDD 可指定缓存在内存中。一般计算都是流水式生成、使用 RDD，新的 RDD 生成之后，旧的不再使用，并被 Java 虚拟机回收掉。但如果后续有许多计算依赖某个 RDD，我们可以让这个 RDD 缓存在内存中，避免重复计算（尤其适用于机器学习）。</li>
<li>RDD 可以通过重新计算得到。RDD 的高可靠性不是通过复制来实现的，而是通过记录足够的计算过程。</li>
</ol>
<h4 id="RDD-的定义"><a href="#RDD-的定义" class="headerlink" title="RDD 的定义"></a>RDD 的定义</h4><p>一个 RDD 对象，包含如下的 5 个核心属性。</p>
<ul>
<li>一个分区列表，每个分区里是 RDD 的部分数据（或者称数据块）。</li>
<li>一个依赖列表，存储依赖的其他 RDD。</li>
<li>一个名为 compute 的计算函数，用于计算各 RDD 各分区的值。</li>
<li>分区器（可选），用于键/值类型的 RDD，比如某个 RDD 是按散列来分区。</li>
<li>计算各分区时优先的位置列表（可选），比如从 HDFS 上的文件生成 RDD 时，RDD 分区的位置优先选择数据所在的节点，这样可以避免数据移动带来的开销。</li>
</ul>
<h4 id="RDD-的-Transformation"><a href="#RDD-的-Transformation" class="headerlink" title="RDD 的 Transformation"></a>RDD 的 Transformation</h4><p>RDD 的 Transformation 是指由一个 RDD 生成新 RDD 的过程，比如 flapMap。filter 操作都会返回一个新的 RDD 对象，类型是 MapPartitionsRDD，它是 RDD 子类。</p>
<p>在 Spark 中，RDD 是有依赖关系的，这种依赖关系有两种类型。</p>
<ul>
<li>窄依赖。依赖上级 RDD 的部分分区。</li>
<li>Shuffle 依赖上级 RDD 的所有分区。</li>
</ul>
<p>使用窄依赖时，可以精确知道依赖的上级 RDD 的分区。一般情况下，会选择与自己在同一节点的上级 RDD 分区，这样计算过程都在同一节点进行，没有网络 IO 开销，非常高效，常见的 map、flatMap、filter操作都是这一类。而 Shuffle 依赖则无法精确定位依赖的上级 RDD 的分区，相当于依赖索引分区，计算时涉及所有节点之间的数据传输，开销巨大。所以，以 Shuffle 依赖为分隔，Task 被分成 Stage，方便计算时的管理。</p>
<h4 id="RDD-的-Action"><a href="#RDD-的-Action" class="headerlink" title="RDD 的 Action"></a>RDD 的 Action</h4><p>一次 Action 调用之后，不在生成新的 RDD，结果返回到 Driver 程序。</p>
<h4 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h4><p>Shuffle 概念来源于 Hadoop MapReduce，当对一个 RDD 的某个结果分区进行操作而无法精确知道依赖前一个 RDD 的哪个分区时，依赖关系变成了依赖前一个 RDD 的所有分区。Shuffle 本身是一个非常耗资源的操作，它的结果是一次调度的 Stage 的结果，而一次 Stage 包含许多 Task，缓存下来比较划算。Shuffle 使用的本地磁盘目录由 spark.local.dir 属性项指定。</p>
<h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><p>SparkContext 是 Spark 程序最主要的入口，用于和 Spark 集群连接。所有的 Spark 程序都必须创建 SparkContext。进行流式计算时使用 StreamingContext，进行 SQL 计算时使用 SQLContext，都会创建一个 SparkContext。每个 JVM 只允许启动一个 SparkContext。</p>
<h4 id="SparkConf-配置"><a href="#SparkConf-配置" class="headerlink" title="SparkConf 配置"></a>SparkConf 配置</h4><p>SparkContext 可以无参数配置，也可以自定义配置。SparkContext 在构造的过程中，已经完成了各项服务的启动。最重要的初始化操作之一是启动 Task 调度器和 DAG 调度器。</p>
<p>DAG 调度与 Task 调度的区别是，DAG 是高层级的调度，为每个 Job 绘制一个有向无环图，跟踪各 Stage 的输出。计算完成 Job 的最短路径，并将 Task 提交给 Task 调度器执行，而 Task 调度器只负责接收 DAG 调度器的请求，负责 Task 的实际调度执行，所以 DAGScheduler 的初始化必须在 Task 调度器之后。</p>
<p>DAG 与 Task 这种分离设计的好处是，Spark 可以灵活设计自己的 DAG 调度，同时还能与其他资源调度系统结合，比如 YARN、Mesos。</p>
<h3 id="DAG-调度"><a href="#DAG-调度" class="headerlink" title="DAG 调度"></a>DAG 调度</h3><p>SparkContext 在初始化时，创建了 DAG 调度与 Task 调度来负责 RDD Action 操作的调度执行。</p>
<h4 id="DAGScheduler"><a href="#DAGScheduler" class="headerlink" title="DAGScheduler"></a>DAGScheduler</h4><p>DAGScheduler 负责 Spark 的最高级别的任务调度，调度的粒度是 Stage，它为每个 Job 的所有 Stage 计算一个有向无环图，控制它们的并发，并找到一个最佳路径来执行它们。具体的执行过程是将 Stage 下的 Task 集提交给 TaskScheduler 对象，由它来提交到集群上去申请资源并最终完成执行。</p>
<h4 id="TaskScheduler"><a href="#TaskScheduler" class="headerlink" title="TaskScheduler"></a>TaskScheduler</h4><p>相比 DAGScheduler 而言，TaskScheduler 是低级别的调度接口，允许实现不同的 Task 调度器，除了自带的之外，还可以使用 Yarn 和 Mesos 调度器。每个 TaskScheduler 对象只服务于一个 SparkContext 的 Task 调度。TaskScheduler 从 DAGScheduler 的每个 Stage 接收一组 Task，并负责将它们发送到集群上，运行它们，如果出错还会重试，最后返回消息给 DAGScheduler。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/04/Spark实践-1-Spark-工作机制/" itemprop="url">
                  Spark实践 (1): Spark 工作机制
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-04T15:07:57+08:00" content="2017-02-04">
              2017-02-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/04/Spark实践-1-Spark-工作机制/" class="leancloud_visitors" data-flag-title="Spark实践 (1): Spark 工作机制">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Spark 工作机制主要包括调度管理、内存管理、容错机制。</p>
<h3 id="调度管理"><a href="#调度管理" class="headerlink" title="调度管理"></a>调度管理</h3><p>Spark 调度管理按照场景可以分为2类，一类是Spark程序之间的调度，这是最主要的调度场景；另外一类是Spark程序内部的调度。</p>
<h4 id="Driver-程序"><a href="#Driver-程序" class="headerlink" title="Driver 程序"></a>Driver 程序</h4><p>在集群模式下，用户编写的 Spark 程序称为 Driver 程序。每个 Driver 程序包含一个代表集群环境的 SparkContenxt 对象并与之连接，程序的执行从 Driver 程序开始，中间过程会调用 RDD 操作，这些操作通过集群资源管理器来调度执行，一般在 Worker 节点上执行，所有操作执行结束后回到 Driver 程序中，在 Driver 程序中结束。</p>
<h4 id="SparkContext-对象"><a href="#SparkContext-对象" class="headerlink" title="SparkContext 对象"></a>SparkContext 对象</h4><p>每个驱动程序里都有一个 SparkContext 对象，担负着与集群沟通的职责，其工作过程如下：</p>
<ol>
<li>SaprkContext 对象联系集群管理器、分配CPU、内存等资源。</li>
<li>集群管理器在工作节点上启动一个执行器。</li>
<li>程序代码会被分发到相应的工作节点上。</li>
<li>SparkContext 分发任务（Task）至各执行器执行。</li>
</ol>
<h4 id="集群管理器"><a href="#集群管理器" class="headerlink" title="集群管理器"></a>集群管理器</h4><p>集群管理器负责集群的资源调度。Spark 支持 3 种集群部署方式，每种部署对应一种资源管理器。</p>
<ol>
<li>Standalone 模式（资源管理器是Master结点）。最简单的一种集群模式，不依赖于其他系统，调度策略相对单一，只支持先出先进。</li>
<li>Hadoop Yarn。</li>
<li>Apache Mesos。</li>
</ol>
<h4 id="其他相关名称"><a href="#其他相关名称" class="headerlink" title="其他相关名称"></a>其他相关名称</h4><ul>
<li>Job： 一次 RDD Action 对应一次 Job，会提交至资源管理器调度执行。</li>
<li>Stage： Job 在执行过程中被分为多个阶段。介于 Job 和 Task 之间，是按 Shuffle 分隔的 Task 集合。</li>
<li>执行器： 每个 Spark 程序在每个节点上启动一个进程，专属于一个 Spark 程序，与 Spark 程序有相同的生命周期，负责 Spark 在节点上启动的 Task，管理内存和磁盘。如果一个节点上有多个 Spark 程序在执行，那么相应的就会启动多个执行器。</li>
<li>Task： 在执行器上执行的最小单元。比如 RDD Transformation 操作时对 RDD 内每个分区计算都会对应一个 Task。</li>
</ul>
<h4 id="Spark-程序之间的调度"><a href="#Spark-程序之间的调度" class="headerlink" title="Spark 程序之间的调度"></a>Spark 程序之间的调度</h4><p>主要分为两种，</p>
<ol>
<li>静态资源分配</li>
<li>动态资源分配</li>
</ol>
<h4 id="Spark-程序内部的调度"><a href="#Spark-程序内部的调度" class="headerlink" title="Spark 程序内部的调度"></a>Spark 程序内部的调度</h4><p>当 Spark 为多个用户同时提供服务时，我们可以考虑配置 Spark 程序内部的调度。</p>
<p>在 Spark 程序内部，不同线程提交的 Job 可以并行执行。Spark 的调度器是线程安全的，因此可以支持这种需要同时处理多个请求的服务型应用。</p>
<p>默认情况下，Spark的调度器以 FIFO 的方式运行 Job，前面运行的 Job 优先获得所有资源。从 Spark 0.8 开始，可以开始采用“循环”（round robin）的方式为不同 Job 之间的 Task 分配资源，这样所有的 Job 可以获取差不多相同的资源。这种模式特别适用于多用户的场景。</p>
<p>如果想要开启程序的公平调度，只需要在 SparkContext 中设置 Spark.scheduler.mode 的值为 FAIR：</p>
<pre><code>var conf = new SparkConf().setMaster(...).setAppName(...)
conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)
var sc = new SparkContext(conf);
</code></pre><h4 id="公平调度池"><a href="#公平调度池" class="headerlink" title="公平调度池"></a>公平调度池</h4><p>公平调度支持对多个 Job 进行分组，这个分组称为调度池，每个调度池可以设置不同的调度选项，当我们想要为一些更重要的 Job 设置更高的优先级时，这个功能就非常有用了。我们可以为不同的用户设置不同的调度池。然后让各个调度池平等地共享资源，而不是按 Job 来共享资源。</p>
<p>指定让 Job 进入那个调度池的具体方法是提交任务的线程在 SparkContext 中设置 spark.scheduler.pool </p>
<pre><code>sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, &quot;pool1&quot;)
</code></pre><p>这样设置之后，这个线程提交的所有 Job 会使用这个调度池。设置按线程来进行，这样可以很方便地让一个线程下的所有 Job 都在同一个用户下。如果要清空当前线程的调度池设置，可以这样设置</p>
<pre><code>sc.setLocalProperty(&quot;spark.scheduler.pool&quot;,null)
</code></pre><h4 id="调度池的默认行为"><a href="#调度池的默认行为" class="headerlink" title="调度池的默认行为"></a>调度池的默认行为</h4><p>默认情况下，所有调度池平均共享集群的资源，默认调度池也是。但在每个调度池内部，各个 Job 是按 FIFO 的顺序来执行的。</p>
<h4 id="调度池的配置"><a href="#调度池的配置" class="headerlink" title="调度池的配置"></a>调度池的配置</h4><ul>
<li>schedulingMode。（FIFO 或者 FAIR）</li>
<li>weight。（用于控制调度池相对于其他调度池的权重）</li>
<li>minShare。（最小资源值( core 的数量)）</li>
</ul>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>相比 Hadoop MapReduce，Spark 计算具有巨大的性能优势，其中很大一部分是因为 Spark  对于内存的充分利用，以及提供的缓存机制。</p>
<h4 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h4><p>如果一个 RDD 不止一次被用到，那么就可以持久化它，以大幅提升程序的性能。持久化的方法是调用 persist() 函数，除了持久化至内存中，还可以在 persist() 中指定 storage level 参数使用其他的类型。</p>
<h4 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h4><p>Spark 大部分操作都是 RDD 操作，通过传入函数给 RDD 操作函数来计算。这些函数在不同的节点上并发执行，内部的变量有不同的作用域，不能互相访问。Spark 提供 2 种共享变量–广播变量和计数器。</p>
<ol>
<li>广播变量</li>
</ol>
<p>一个只读对象，在所有节点上都有一份缓存，创建方法如下：</p>
<pre><code>val broadcastVar = sc.broadcast(Array(1, 2, 3))
</code></pre><ol>
<li>计数器</li>
</ol>
<p>计数器只能增加，可以用于计算或者求和。计数器变量的创建方法是:</p>
<pre><code>SparkContext.accumulator(v, name) 
</code></pre><p>v 是初始值，name 是名称。注意，只有 Driver 程序可以读这个计算器变量，RDD 操作中读取计数器变量是无意义的。</p>
<h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><p>Spark  以前的集群容错处理模型，像 MapReduce，将计算转换为一个有向无环图（DAG）的任务集合，这样可以通过重复执行 DAG 里的一部分任务来完成容错恢复。但是由于主要的数据存储在分布式文件系统中，没有提供其他存储的概念，容错过程中需要在网络上进行数据复制，从而增加了大量的消耗。所以，分布式编程中经常需要做检查点，即将某个时机的中间数据写到存储（通常是分布式文件系统）中。</p>
<p>RDD 也是一个 DAG，每一个 RDD 都会记住创建该数据集需要哪些操作，跟踪记录 RDD 的继承关系，这个关系在 Spark 里面叫 lineage。由于创建 RDD 的操作是相对粗粒度的变换，即单一的操作应用于许多数据元素，而不需存储真正的数据。当一个 RDD 的某个分区丢失时， RDD 有足够的信息记录其如何通过其他 RDD 进行计算，且只需重新计算该分区。</p>
<p>RDD 之间的依赖分为两种。</p>
<ul>
<li>窄依赖。父分区对应一个子分区。</li>
<li>宽依赖。父分区对应多个子分区。</li>
</ul>
<p>对应窄依赖，只需要通过重新计算丢失的那一块数据来恢复，容错成本较小。但如果是宽依赖，则当容错重算分区时，因为父分区数据只有一部分是需要重算子分区的，其余数据重算则成了冗余计算。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/26/Kaggle-比赛-德国信用卡违约数据分析/" itemprop="url">
                  Kaggle 比赛: 德国信用卡违约数据分析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-11-26T11:35:38+08:00" content="2016-11-26">
              2016-11-26
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/数据挖掘/" itemprop="url" rel="index">
                    <span itemprop="name">数据挖掘</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2016/11/26/Kaggle-比赛-德国信用卡违约数据分析/" class="leancloud_visitors" data-flag-title="Kaggle 比赛: 德国信用卡违约数据分析">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h3><p>German Credit Data， 我们来看看数据的格式,</p>
<p>A1 到 A15 为 15个不同类别的特征，A16 为 label 列，一共有 690条数据，下面列举其中一条当作例子：</p>
<table>
<thead>
<tr>
<th>A1</th>
<th>A2</th>
<th>A3</th>
<th>A4</th>
<th>A5</th>
<th>A6</th>
<th>A7</th>
<th>A8</th>
<th>A9</th>
<th>A10</th>
<th>A11</th>
<th>A12</th>
<th>A13</th>
<th>A14</th>
<th>A15</th>
<th>A16</th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>30.83</td>
<td>0</td>
<td>u</td>
<td>g</td>
<td>w</td>
<td>v</td>
<td>1.25</td>
<td>t</td>
<td>t</td>
<td>01</td>
<td>f</td>
<td>g</td>
<td>00202</td>
<td>0</td>
<td>+</td>
</tr>
</tbody>
</table>
<h4 id="Attribute-Information"><a href="#Attribute-Information" class="headerlink" title="Attribute Information:"></a>Attribute Information:</h4><pre><code>A1:    b, a.
A2:    continuous.
A3:    continuous.
A4:    u, y, l, t.
A5:    g, p, gg.
A6:    c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.
A7:    v, h, bb, j, n, z, dd, ff, o.
A8:    continuous.
A9:    t, f.
A10:    t, f.
A11:    continuous.
A12:    t, f.
A13:    g, p, s.
A14:    continuous.
A15:    continuous.
A16: +,-         (class attribute)
</code></pre><h4 id="Missing-Attribute-Values"><a href="#Missing-Attribute-Values" class="headerlink" title="Missing Attribute Values:"></a>Missing Attribute Values:</h4><pre><code>37 cases (5%) have one or more missing values.  The missing
values from particular attributes are:

A1:  12
A2:  12
A4:   6
A5:   6
A6:   9
A7:   9
A14: 13
</code></pre><h4 id="Class-Distribution"><a href="#Class-Distribution" class="headerlink" title="Class Distribution"></a>Class Distribution</h4><pre><code>+: 307 (44.5%)
-: 383 (55.5%)
</code></pre><h4 id="数据处理与数据分析"><a href="#数据处理与数据分析" class="headerlink" title="数据处理与数据分析"></a>数据处理与数据分析</h4><p>下面展示一下数据处理流程，主要是处理了一下缺失值，然后根据特征按连续型和离散型进行分别处理，使用了 sklearn 里面的 LogisticRegression 包，下面的代码都有很详细的注释。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"./crx.data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给数据增加列标签</span></span><br><span class="line">data.columns = [<span class="string">"f1"</span>, <span class="string">"f2"</span>, <span class="string">"f3"</span>, <span class="string">"f4"</span>, <span class="string">"f5"</span>, <span class="string">"f6"</span>, <span class="string">"f7"</span>, <span class="string">"f8"</span>, <span class="string">"f9"</span>, <span class="string">"f10"</span>, <span class="string">"f11"</span>, <span class="string">"f12"</span>, <span class="string">"f13"</span>, <span class="string">"f14"</span>, <span class="string">"f15"</span>, <span class="string">"label"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换 label 映射</span></span><br><span class="line">label_mapping = &#123;</span><br><span class="line">    <span class="string">"+"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"-"</span>: <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data[<span class="string">"label"</span>] = data[<span class="string">"label"</span>].map(label_mapping)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理缺省值的方法</span></span><br><span class="line">data = data.replace(<span class="string">"?"</span>, np.nan)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 object 类型的列转换为 float型</span></span><br><span class="line">data[<span class="string">"f2"</span>] = pd.to_numeric(data[<span class="string">"f2"</span>])</span><br><span class="line">data[<span class="string">"f14"</span>] = pd.to_numeric(data[<span class="string">"f14"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连续型特征如果有缺失值的话，用它们的平均值替代</span></span><br><span class="line">data[<span class="string">"f2"</span>] = data[<span class="string">"f2"</span>].fillna(data[<span class="string">"f2"</span>].mean())</span><br><span class="line">data[<span class="string">"f3"</span>] = data[<span class="string">"f3"</span>].fillna(data[<span class="string">"f3"</span>].mean())</span><br><span class="line">data[<span class="string">"f8"</span>] = data[<span class="string">"f8"</span>].fillna(data[<span class="string">"f8"</span>].mean())</span><br><span class="line">data[<span class="string">"f11"</span>] = data[<span class="string">"f11"</span>].fillna(data[<span class="string">"f11"</span>].mean())</span><br><span class="line">data[<span class="string">"f14"</span>] = data[<span class="string">"f14"</span>].fillna(data[<span class="string">"f14"</span>].mean())</span><br><span class="line">data[<span class="string">"f15"</span>] = data[<span class="string">"f15"</span>].fillna(data[<span class="string">"f15"</span>].mean())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 离散型特征如果有缺失值的话，用另外一个不同的值替代</span></span><br><span class="line">data[<span class="string">"f1"</span>] = data[<span class="string">"f1"</span>].fillna(<span class="string">"c"</span>)</span><br><span class="line">data[<span class="string">"f4"</span>] = data[<span class="string">"f4"</span>].fillna(<span class="string">"s"</span>)</span><br><span class="line">data[<span class="string">"f5"</span>] = data[<span class="string">"f5"</span>].fillna(<span class="string">"gp"</span>)</span><br><span class="line">data[<span class="string">"f6"</span>] = data[<span class="string">"f6"</span>].fillna(<span class="string">"hh"</span>)</span><br><span class="line">data[<span class="string">"f7"</span>] = data[<span class="string">"f7"</span>].fillna(<span class="string">"ee"</span>)</span><br><span class="line">data[<span class="string">"f13"</span>] = data[<span class="string">"f13"</span>].fillna(<span class="string">"ps"</span>)</span><br><span class="line"></span><br><span class="line">tf_mapping = &#123;</span><br><span class="line">    <span class="string">"t"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"f"</span>: <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data[<span class="string">"f9"</span>] = data[<span class="string">"f9"</span>].map(tf_mapping)</span><br><span class="line">data[<span class="string">"f10"</span>] = data[<span class="string">"f10"</span>].map(tf_mapping)</span><br><span class="line">data[<span class="string">"f12"</span>] = data[<span class="string">"f12"</span>].map(tf_mapping)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 给离散的特征进行 one-hot 编码</span></span><br><span class="line">data = pd.get_dummies(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打乱顺序</span></span><br><span class="line">shuffled_rows = np.random.permutation(data.index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分本地测试集和训练集</span></span><br><span class="line">highest_train_row = int(data.shape[<span class="number">0</span>] * <span class="number">0.70</span>)</span><br><span class="line">train = data.iloc[<span class="number">0</span>:highest_train_row]</span><br><span class="line">loc_test = data.iloc[highest_train_row:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉最后一列 label 之后的才是 feature</span></span><br><span class="line">features = train.drop([<span class="string">"label"</span>], axis = <span class="number">1</span>).columns</span><br><span class="line"></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">X_train = train[features]</span><br><span class="line">y_train = train[<span class="string">"label"</span>] == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line">X_test = loc_test[features]</span><br><span class="line"></span><br><span class="line">test_prob = model.predict(X_test)</span><br><span class="line">test_label = loc_test[<span class="string">'label'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地测试集上的准确率</span></span><br><span class="line">accuracy_test = (test_prob == loc_test[<span class="string">"label"</span>]).mean()</span><br><span class="line"><span class="keyword">print</span> accuracy_test</span><br></pre></td></tr></table></figure>
<pre><code>0.835748792271
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cross_validation, metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">#验证集上的auc值</span></span><br><span class="line">test_auc = metrics.roc_auc_score(test_label, test_prob)<span class="comment">#验证集上的auc值</span></span><br><span class="line"><span class="keyword">print</span> test_auc</span><br></pre></td></tr></table></figure>
<pre><code>0.835748792271
</code></pre><p>简单使用了一下逻辑回归，发现准确率是 0.835748792271，AUC 值是 0.835748792271，效果还不错，接下来对模型进行优化来进一步提高准确率。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="PengShuang" />
          <p class="site-author-name" itemprop="name">PengShuang</p>
          <p class="site-description motion-element" itemprop="description">在路上，慢慢走！</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">74</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/pengshuang" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2176899852/profile?rightmod=1&wvr=6&mod=personnumber&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://bbs.byr.cn/" title="北邮人" target="_blank">北邮人</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://coolshell.cn/" title="酷壳" target="_blank">酷壳</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.dongwm.com" title="小明明的博客" target="_blank">小明明的博客</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PengShuang</span>
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
  <p>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p>
</div>

<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("DKbLgBme7UkAx9JX6sM3D4Hj-gzGzoHsz", "GXjJ9Ox3pUGI9PJhm6CNfJGN");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
