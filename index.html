<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="在路上，慢慢走！">
<meta property="og:type" content="website">
<meta property="og:title" content="小沙文的博客">
<meta property="og:url" content="http://pengshuang.space/index.html">
<meta property="og:site_name" content="小沙文的博客">
<meta property="og:description" content="在路上，慢慢走！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小沙文的博客">
<meta name="twitter:description" content="在路上，慢慢走！">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://pengshuang.space/"/>

  <title> 小沙文的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小沙文的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/05/随机森林和GBDT/" itemprop="url">
                  集成学习：从随机森林到 GBDT
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-05T15:23:15+08:00" content="2017-03-05">
              2017-03-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/03/05/随机森林和GBDT/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/05/随机森林和GBDT/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/03/05/随机森林和GBDT/" class="leancloud_visitors" data-flag-title="集成学习：从随机森林到 GBDT">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>研究生期间参加了很多数据挖掘比赛，比赛的题目各不相同，但其实每次用的模型都差不多，其中用的最多的就是随机森林和 GBDT。所以今天打算总结一下这两个模型。</p>
<p>随机森林是一种 bagging 的方法，而 GBDT 则是一种 boosting 的方法。这两类方法统称为集成学习。</p>
<h3 id="什么是集成学习"><a href="#什么是集成学习" class="headerlink" title="什么是集成学习"></a>什么是集成学习</h3><p>集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到比单一模型更好的效果。常见的集成学习框架有：bagging，boosting和stacking。南京大学的周志华老师曾经对这三种方法有很明确的定义：</p>
<ul>
<li><strong>bagging</strong>：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。</li>
</ul>
<p><img src="/img/bagging.jpg" width="500" height="200" alt="图片名称" align="center"></p>
<ul>
<li><strong>boosting</strong>：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果。</li>
</ul>
<p><img src="/img/boosting.jpg" width="500" height="300" alt="图片名称" align="center"></p>
<ul>
<li><strong>stack</strong>：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。</li>
</ul>
<p><img src="/img/stack.jpg" width="500" height="400" alt="图片名称" align="center"></p>
<p>有了集成学习的思想，模型便有了 “集思广益” 的能力，也就不容易产生过拟合现象。但是，为什么这样就可以防止过拟合呢，这就不得不先从模型的偏差和方差入手。</p>
<h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。《Understanding the Bias-Variance Tradeoff》当中有一副图形象地向我们展示了偏差和方差的关系：</p>
<p><img src="/img/varience.png" width="500" height="400" alt="图片名称" align="center"></p>
<h4 id="模型的偏差和方差"><a href="#模型的偏差和方差" class="headerlink" title="模型的偏差和方差"></a>模型的偏差和方差</h4><p>模型的偏差：训练出来的模型在训练集上的误差。</p>
<p>模型的方差：假设模型是随机变量。设样本容量为 n 的训练集为随机变量的集合 \(X_1, X_2,…,X_n\)。那么模型是以这些随机变量为输入的随机变量函数。抽样的随机性会带来模型的随机性。</p>
<p>定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异很好量化，那么对于模型，它们的差异性是指模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。</p>
<p>我们认为方差越大的模型越容易过拟合：假设有两个训练集 A 和 B，经过 A 训练的模型 Fa 与经过 B 训练的模型Fb差异很大，这意味着 Fa 在类 A 的样本集合上有更好的性能，而 Fb 反之，则出现了过拟合。</p>
<p>集成模型中的基模型一般都是弱模型，通常而言，弱模型的偏差很大，方差小，虽然在训练集上效果不好，但是不容易过拟合。有些集成模型中的基模型不是弱模型，bagging 和 stacking 中的基模型为强模型（偏差低，方差高），boosting 中的基模型为弱模型。</p>
<p>在 bagging 和 boosting 框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging 和 boosting 的基模型都是线性组成的，那么有：</p>
<p><img src="/img/m1.png" width="400" height="400" alt="图片名称" align="center"></p>
<h4 id="bagging-的偏差和方差"><a href="#bagging-的偏差和方差" class="headerlink" title="bagging 的偏差和方差"></a>bagging 的偏差和方差</h4><p>对于 bagging 来说，每个基模型的权重等于 1/m 且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：</p>
<p><img src="/img/m2.png" width="400" height="300" alt="图片名称" align="center"></p>
<p>根据上式我们可以看到，<strong>整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似</strong>。同时，整体模型的方差小于等于基模型的方差（当相关性为 1 时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。<strong>另外，在此我们还知道了为什么 bagging 中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。</strong></p>
<p>随机森林是典型的 bagging 模型，在 bagging 的基础上，进一步降低了模型的方差。随机森林中的基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减小，第二项稍微增加，整体方差仍然是减少。</p>
<p>随机森林的子模型都拥有较低的偏差，在 sklearn 中，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None），同时，降低子模型间的相关度可以起到减少整体模型的方差的效果（max_features的默认值为auto。</p>
<h4 id="boosting-的偏差和方差"><a href="#boosting-的偏差和方差" class="headerlink" title="boosting 的偏差和方差"></a>boosting 的偏差和方差</h4><p>对于 boosting 来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于 1，故我们也可以针对 boosting 化简公式为：</p>
<p><img src="/img/m3.png" width="400" height="140" alt="图片名称" align="center"></p>
<p>通过观察整体方差的表达式，我们容易发现，<strong>若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。</strong> 因此，boosting框架中的基模型必须为弱模型。</p>
<p>因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于 1 吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</p>
<p>基于 boosting 框架的 GBDT 模型中基模型也为树模型，和随机森林一样，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<p>在 sklearn 中，GBDT 的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3），但是降低子模型间的相关度不能显著减少整体模型的方差（max_features的默认值为None）。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ol>
<li><p>使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力。</p>
</li>
<li><p>对于 bagging 来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低</p>
</li>
<li><p>对于 boosting 来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低</p>
</li>
<li><p>整体模型的偏差和方差与基模型的偏差和方差息息相关</p>
</li>
</ol>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>我们先讲 GBDT 模型的损失函数。</p>
<p>基于 boosting 框架的整体模型可以用线性组成式来描述，其中 \(h_{i}(x)\) 为基模型与其权值的乘积：</p>
<p>$$F(x) = \sum_i^m h_{i}(x)$$</p>
<p>根据上式，整体模型的训练目标是使预测值 \(F(x)\) 逼近真实值 y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p>
<p>$$F^i(x) = f^{i-1}(x) + h_i(x)$$</p>
<p>这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使 \(F^i(x)\) 逼近真实值y。</p>
<h4 id="拟合残差"><a href="#拟合残差" class="headerlink" title="拟合残差"></a>拟合残差</h4><p>使 \(F^i(x)\) 逼近真实值，其实就是使 \(h_i(x)\) 逼近真实值和上一轮迭代的预测值 \(F^{i-1}(x)\) 之差，即残差 (\( y - F^{i-1}(x))\) 。最直接的做法是构建基模型来拟合残差。</p>
<p>研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度：</p>
<p>$$- \frac{\partial (\frac{1}{2} \ast (y - F_{i-1}(x))^2)}{\partial F(x)} = y - F_{i-1}(x)$$</p>
<p>即，\(F^{i-1}(x)\) 加上反向梯度的 \(h_i(x)\) 得到 \(F^i(x)\)，该值可能导致平方差损失函数降低，预测的准确度降低。</p>
<h4 id="拟合反向梯度"><a href="#拟合反向梯度" class="headerlink" title="拟合反向梯度"></a>拟合反向梯度</h4><p>引入任意损失函数后，我们可以定义整体模型的迭代式如下：</p>
<p>$$F^i(x) = F^{i-1}(x) + argmin \sum_j^n L(y_j, F^{i-1}(x_j) + h_i(x_j))$$</p>
<h4 id="常见的损失函数"><a href="#常见的损失函数" class="headerlink" title="常见的损失函数"></a>常见的损失函数</h4><ul>
<li>最小均方误差函数：sklearn 中 GBDT 回归中的默认损失函数，刚才介绍的拟合残差其实就是改损失函数的反向梯度值。</li>
<li>logit 函数：LR 中常用的损失函数。逻辑回归的本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以用 logistics 函数表达。sklean 中 GBDT 分类模型默认采用这个损失函数。</li>
<li>指数损失函数：<br>$$L(y_j, F^{i-1}(x_j) = e^{-y_j \ast F^{i-1}(x_j)}$$<br>对该损失函数求反向梯度得：<br>$$- \frac{\partial ( y_j, F^{i-1}(x_j))}{\partial ^{i-1} F^{i-1}(x)} = y_j \ast e^{-y_j \ast F^{i-1}(x_j)}$$<br>这时，第 i 轮迭代中，新训练集如下：<br>$${(x_j, y_j \ast e^{-y_j \ast F^{i-1}(x_j)})}$$<br>这表明当损失函数是指数损失时，GBDT 相当于二分类的 Adaboost 算法。是的，指数损失仅能用于二分类的情况。</li>
</ul>
<h4 id="shrinkage"><a href="#shrinkage" class="headerlink" title="shrinkage"></a>shrinkage</h4><p>使用 GBDT 时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。</p>
<h4 id="初始模型"><a href="#初始模型" class="headerlink" title="初始模型"></a>初始模型</h4><p>我们定义损失模型为：</p>
<p>$$ F^0 (x) = argmin \sum_j^n L(y_j, \gamma)$$</p>
<p>根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。</p>
<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p><strong>偏差描述了模型在训练集准确度，而损失函数则是描述该准确度的间接量纲。</strong> 也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://www.jianshu.com/p/28604e0870d7" target="_blank" rel="external">Random Forest和Gradient Tree Boosting参数详解</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/04/机器学习中的L1-和-L2/" itemprop="url">
                  机器学习中的L1 和 L2
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-04T11:07:05+08:00" content="2017-03-04">
              2017-03-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/03/04/机器学习中的L1-和-L2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/04/机器学习中的L1-和-L2/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/03/04/机器学习中的L1-和-L2/" class="leancloud_visitors" data-flag-title="机器学习中的L1 和 L2">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>有监督的学习方法，比如说我们平常说的 LR，SVM 等，它们的目标是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。参数太多的话，模型自然而言会对训练数据拟合的更好，训练误差会很小，但是我们的目的并不是在训练数据上表现好，而是在未知的测试数据上依然有很好的表现。所以，我们需要保证模型 ”简单“ 的基础上去最小化训练误差，这样得到的模型才能有很好的 “泛化” 能力。著名的 “奥卡姆剃刀” 原理说的就是这个东西，模型简单主要是通过规则项来实现，规则项的实现相当于将我们对这个模型的先验知识加入，强行的让学习到的模型具有人想要的特性，例如稀疏、低秩或者平滑等。</p>
<p>加入了规则项之后的有监督模型的目标函数都可以写成下面这中形式：</p>
<p>$$w^* = arg min_w \sum_i L(y_i, f(x_i , w)) + \lambda \Omega(w)$$</p>
<p>其中第一项 \(L(y_i, f(x_i , w))\) 衡量我们的模型对第 i 个样本的预测值 \(f(x_i , w))\) 和真实的标签 \(y_i\) 之间的误差。因为我们的模型是要拟合我们的训练样本。为了提升我们模型的泛化能力，我们需要加上第二项去限制，即对参数 w 的规则化函数 \(\Omega(w)\) 去约束我们的模型尽量的简单。</p>
<p>规则化 \(\Omega(w)\) 函数有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数 w 的约束不同，取得的效果也不同。今天我这篇文章主要介绍最常见的 L1 和 L2。</p>
<h3 id="L1-范数"><a href="#L1-范数" class="headerlink" title="L1 范数"></a>L1 范数</h3><p>说到 L1 范数，就不得不先提一下 L0 范数，L1 是经 L0 发展而来的。如果我们用 L0 范数去规则化一个参数矩阵 W 的话，就是希望 W 的大部分元素都是 0，即希望参数尽可能稀疏；而 L1 范数值指向量中各个元素的绝对值加和，常称 “lasso”。L1 也可以实现特征权值稀疏。那这是为什么呢？因为 L1 是 L0 范数的最优凸近似。对于任何的规则化算子，如果它在 Wi = 0 的地方不可微，并且可以分解为一个 “求和” 的形式，那么这个规则化算子就可以实现稀疏。参数矩阵 W 的 L1 范数是绝对值，|w| 在 w = 0 处是不可微的，并且可以分解为一个求和的形式。那么为什么要用 L1 而不是 L0 去实现特征稀疏呢，因为 L0 范数很难优化求解。</p>
<h4 id="为什么要特征稀疏"><a href="#为什么要特征稀疏" class="headerlink" title="为什么要特征稀疏"></a>为什么要特征稀疏</h4><p>特征稀疏有一个很大的好处是它可以实现特征的自动选择。一般而言，特征矩阵中的大部元素，也就是特征都是和最终的输出 yi 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑 xi 这些额外的特征，虽然可以获得更小的训练误差，但是在预测新样本的时候，可能会干扰正常的预测。故特征稀疏是为了帮组我们去更好的完成特征选择而设定的。</p>
<h4 id="特征稀疏的可解释性"><a href="#特征稀疏的可解释性" class="headerlink" title="特征稀疏的可解释性"></a>特征稀疏的可解释性</h4><p>经过特征稀疏之后，模型更容易解释。因为，当特征稀疏之后，大部分的特征权重都是 0，我们可以相信，那些特征权重不为 0 的特征才是对模型起关键意义的。</p>
<h3 id="L2-范数"><a href="#L2-范数" class="headerlink" title="L2 范数"></a>L2 范数</h3><p>L2 范数常为称为 “Ridge”。 它的形式是 \(||w||_2\)。当用于线性回归中时，它常被称为 “岭回归”。它的主要作用是去解决机器学习中的过拟合问题。那为什么呢？</p>
<p>L2 范数是指向量各元素的平方和然后求平方根。我们让 L2 范数的规则项 \(||w||_2\) 最小，可以使得 W 的每个元素都很小，通常很解决 0。越小的参数说明模型越简单，越简单的模型则不容易产生过拟合现象。因为当限制了参数很小时，实际上就限制了多项式模型分量的影响很小。</p>
<h4 id="L2-的好处"><a href="#L2-的好处" class="headerlink" title="L2 的好处"></a>L2 的好处</h4><p>L2 除了可以避免模型陷入过拟合的困境外，还可以处理 <strong>condition number</strong> 不好情况下矩阵求逆很困难的问题。</p>
<p>在机器学习中，去最优化目标函数时，有 2 个很头疼的问题：</p>
<ol>
<li>局部最小值，如果要优化的目标的函数不是凸函数的时候，我们用梯度下降或者其他方法去优化时，可能会得到局部最小值而不是全局最小值。</li>
<li>ill condition：ill-condition对应的是 well-condition。那他们分别代表什么？假设我们有个方程组 \(AX = b\)，我们需要求解 X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是 ill-condition 的，反之就是 well-condition 的。ill-condition 是指 \(AX = b\) 的解对于系数矩阵 A 或者 b 很敏感，A 或 b 微小的变化都可能影响解。</li>
</ol>
<p><strong>condition number</strong> 就是用来衡量 ill-condition 系统的可行度。condition number 衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number 值小的就是 well-conditioned 的，大的就是 ill-conditioned 的。</p>
<p>如果方阵 A 是非奇异的，那么 A 的 <strong>condition number</strong> 定义为：</p>
<p>$$\sigma (A) = ||A||\, ||A^{-1}||$$</p>
<p>也就是矩阵A的 norm 乘以它的逆的 norm。所以具体的值是多少，就要看你选择的 norm 是什么了。如果方阵 A 是奇异的，那么A的 condition number 就是正无穷大了。实际上，每一个可逆方阵都存在一个 condition number。但如果要计算它，我们需要先知道这个方阵的 norm（范数）和 Machine Epsilon（机器的精度）。为什么要范数？范数就相当于衡量一个矩阵的大小，我们知道矩阵是没有大小的，刚才不是要衡量一个矩阵 A 或者向量 b 变化的时候，我们的解 x 变化的大小吗？所以肯定得要有一个东西来度 量矩阵和向量的大小吧？这个东西就是范数，表示矩阵大小或者向量长度。OK，经过比较简单的证明，对于 \(AX = b\)，我们可以得到以下的结论：</p>
<p>$$\frac{||\Delta x||}{||x||} \leq ||A|| \cdot ||A^{-1}|| \cdot  \frac{||\Delta b||}{||b||}$$</p>
<p>$$\frac{||\Delta x||}{||x||} \leq \sigma (A) \cdot  \frac{||\Delta b||}{||b||}$$</p>
<p>$$\frac{||\Delta x||}{||x + \Delta x||} \leq \sigma (A) \cdot  \frac{||\Delta A||}{||b||}$$</p>
<p>也就是我们的解 x 的相对变化和 A 或者 b 的相对变化是有像上面那样的关系的，其中 \(\sigma (A)\) 的值就相当于倍率，看到了吗？相当于 x 变化的界。</p>
<p>对 condition number 来个一句话总结：condition number 是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的 condition number 在 1 附近，那么它就是 well-conditioned 的，如果远大于 1，那么它就是 ill-conditioned 的，如果一个系统是 ill-conditioned 的，它的输出结果就不是很可信。</p>
<p>从优化或者数值计算的角度来说，L2 范数有助于处理 condition number 不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：</p>
<p>$$w^* = (X^TX)^{-1}X^Ty$$</p>
<p>然而，如果当我们的样本 X 的数目比每个样本的维度还要小的时候，矩阵 \(X^TX\) 将会不是满秩的，也就是 \(X^TX\) 会变得不可逆，所以 \(w^*\) 就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。</p>
<p>但是如果加上 L2 规则项，就可以直接求逆，</p>
<p>$$w^* = (X^TX + \lambda I)^{-1}X^Ty$$</p>
<p>这里面，要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是 λ=0 的情况，如果矩阵 \(X^TX\) 的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善 condition number。</p>
<p>另外，如果使用迭代优化的算法，condition number 太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成 λ-strongly convex（λ 强凸）的了。</p>
<p>关于什么是 λ 强凸，如图说一个函数是凸函数，指函数曲线位于改点处的切线之上，而强凸则进一步要求位于该处的一个二次函数之上，也就是说要求函数不要太“平坦”而是可以保证有一定的 “向上弯曲” 的趋势。如果我们有“强凸”的话，我们就可以得到一个更好的近似解。效果的好坏取决于 strongly convex性质中的常数 λ 的大小。</p>
<p>所以，如果我们想要获得 λ 强凸的话，就是往目标函数里面加上 \(\frac{\alpha}{2} ||w||^{2}\)</p>
<p>在梯度下降中，目标函数收敛速率的上界实际上是和矩阵 \(X^TX\) 的 condition number 有关，\(X^TX\) 的 condition number 越小，上界就越小，也就是收敛速度会越快。L2 范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。</p>
<p>在机器学习中，常用的正则化项多是 L2，除了防止过拟合的问题，还有一个好处就是能否改善 ill-condition问题。尤其是当训练样本相对于特征数非常少时，其矩阵便是非满秩的，往往倾向于有无数个解，且是不可逆的。其 condition number 便会很大。一方面，根据此得到的最优化值很不稳定，往往某个特征变量很小的变动都会引发最终结果较大的偏差。另外通过矩阵求逆从而求的最优解就会变的非常困难。</p>
<h3 id="L1-和-L2-的区别"><a href="#L1-和-L2-的区别" class="headerlink" title="L1 和 L2 的区别"></a>L1 和 L2 的区别</h3><p>L1 和 L2，一个让绝对值最小，一个让平方最小，他们的差距大吗？其实，L2 相对于 L1 具有更为平滑的特性，在模型预测中，往往比L1具有更好的预测特性。当遇到两个对预测有帮助的特征时，L1倾向于选择一个更大的特征。而L2更倾向把两者结合起来。</p>
<h4 id="下降速度"><a href="#下降速度" class="headerlink" title="下降速度"></a>下降速度</h4><p>我们知道，L1 和 L2 都是规则化的方式，我们将权值参数以 L1 或者 L2 了的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1 和 L2 的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在 0 附近，L1 的下降速度比 L2 的下降速度要快。所以会非常快得降到0。</p>
<p><img src="/img/l1l2.png" width="500" height="520" alt="图片名称" align="center"></p>
<h4 id="模型空间的限制"><a href="#模型空间的限制" class="headerlink" title="模型空间的限制"></a>模型空间的限制</h4><p>$$Lasso: min_w \frac{1}{n} ||y - Xw||^2,  s.t ||w||_1 \leq C$$</p>
<p>$$Ridge: min_w \frac{1}{n} ||y - Xw||^2,  s.t ||w||_2 \leq C$$</p>
<p>也就是说，我们将模型空间限制在 w 的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2) 平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为 C 的一个 norm ball。等高线与 norm ball 首次相交的地方就是最优解：如下图：</p>
<p><img src="/img/l1l2p.png" width="800" height="420" alt="图片名称" align="center"></p>
<p>可以看到，L1-ball 与 L2-ball 的不同就在于 L1 在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有 w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。</p>
<p>相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么 L1-regularization 能产生稀疏性，而 L2-regularization 不行的原因了。</p>
<h4 id="贝叶斯先验"><a href="#贝叶斯先验" class="headerlink" title="贝叶斯先验"></a>贝叶斯先验</h4><p>正则化项从贝叶斯学习理论的角度来看，其相当于一种先验函数。即当你训练一个模型时，仅仅依靠当前的训练集数据是不够的，为了实现更好的预测（泛化）效果，我们还应该加上先验项。而 L1 则相当于设置一个Laplacean先 验，去选择 MAP（maximum a posteriori）假设。而 L2 则类似于 Gaussian先验。</p>
<p>因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso 在特征选择时候非常有用，而 Ridge 就只是一种规则化而已。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><p><a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="external">机器学习中的范数规则化之（一）L0、L1与L2范数</a></p>
</li>
<li><p><a href="http://t.hengwei.me/post/%E6%B5%85%E8%B0%88l0l1l2%E8%8C%83%E6%95%B0%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html" target="_blank" rel="external">浅谈L0,L1,L2范数及其应用</a></p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/03/线性模型和非线性模型的区别/" itemprop="url">
                  线性模型和非线性模型的区别
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-03T21:59:49+08:00" content="2017-03-03">
              2017-03-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/03/03/线性模型和非线性模型的区别/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/03/线性模型和非线性模型的区别/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/03/03/线性模型和非线性模型的区别/" class="leancloud_visitors" data-flag-title="线性模型和非线性模型的区别">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>在机器学习的回归问题中，线性模型和非线性模型都可以去对曲线进行建模，那么线性模型和非线性模型有什么区别呢？</p>
<p>其实，线性模型和非线性模型的区别并不在于能不能去拟合曲线。下面我们来详细介绍一下它们两个的区别。</p>
<h3 id="线性回归的等式"><a href="#线性回归的等式" class="headerlink" title="线性回归的等式"></a>线性回归的等式</h3><p>线性回归需要一个线性的模型。这到底意味着什么呢？</p>
<p>一个模型如果是线性的，就意味着它的参数项要么是常数，要么是原参数和要预测的特征之间的乘积加和就是我们要预测的值。</p>
<pre><code>Response = constant + parameter * predictior1 + ... + parameter * predictior2
</code></pre><p>下是个典型的线性模型：</p>
<p>$$Y = b + w_1x_1 + w_2x_2 + … + w_kx_k$$</p>
<p>在统计意义上，如果一个回归等式是线性的，那么它的相对于参数就必须也是线性的。如果相对于参数是线性，那么即使性对于样本变量的特征是二次方或者多次方，这个回归模型也是线性的 (例如下面的公式)。</p>
<p>$$Y = b + w_1x_1 + w_2x_2^2$$</p>
<p>你甚至可以使用 log 或者指数去形式化特征</p>
<p>$$Y = b + w_1e^{-x_1} + w_2e^{-x_2}$$</p>
<h3 id="非线性回归的等式"><a href="#非线性回归的等式" class="headerlink" title="非线性回归的等式"></a>非线性回归的等式</h3><p>最简单的判断一个模型是不是非线性，就是关注非线性本身，判断它的参数是不是非线性的。非线性有很多种形象，这也是为什么非线性模型能够那么好的拟合那些曲折的函数曲线的原因。比如下面这个：</p>
<p>$$Y = \theta_1 \ast x^{\theta_2}$$</p>
<p>$$Y = \theta_1 + (\theta_3 - \theta_2) \ast e^{- \theta_4 X}$$</p>
<p>与线性模型不一样的是，这些非线性模型的特征因子对应的参数不止一个。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/02/天池ijcai比赛总结/" itemprop="url">
                  IJCAI SocInf'16 Contest-Brick-and-Mortar Store Recommendation 天池大数据比赛总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-02T16:40:05+08:00" content="2017-03-02">
              2017-03-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/03/02/天池ijcai比赛总结/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/02/天池ijcai比赛总结/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/03/02/天池ijcai比赛总结/" class="leancloud_visitors" data-flag-title="IJCAI SocInf'16 Contest-Brick-and-Mortar Store Recommendation 天池大数据比赛总结">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>一直想总结一下这次的比赛，拖啊拖。。。一直等到现在，趁着现在要找实习，好好总结一下。</p>
<h3 id="比赛题目"><a href="#比赛题目" class="headerlink" title="比赛题目"></a>比赛题目</h3><p>比赛的官方网站在这，<a href="https://tianchi.shuju.aliyun.com/competition/introduction.htm?spm=5176.100066.333.2.4HLjZH&amp;raceId=231532" target="_blank" rel="external">IJCAI SocInf’16</a>。</p>
<p>这次比赛的题目是给定 2015 年 7 ~ 11 月份的用户在不同地点口碑购买记录，以及 2015 年 7 ~ 11 月淘宝上用户的购物行为数据，来预测 12 月这一整月用户来到一个地点之后会光顾哪些口碑商铺。这个比赛有一个很有意思的地方，就是它关注的是一个用户来到一个他之前没有去过的新地点之后，他会去哪些店铺消费，有一点像推荐系统中经典的冷启动问题。比赛提供的数据也有这个特点：</p>
<ol>
<li>在测试集中，只有 10 % 的用户用之前使用口碑的记录；</li>
<li>有 5 % 的用户虽然有之前使用口碑的记录，但是在测试集中，这些用户来到了新的地点；</li>
<li>所有用户都有他们淘宝购物的行为数据。</li>
</ol>
<p>如果题目只是这样的，其实还不算奇怪，奇怪的是题目的评价标准上加上了 <strong>budget</strong> 这个神奇的东西，先来看看评价指标：</p>
<p>$$P = \frac{\sum_i min(|S_i \cap S_i^*, b_i|)}{\sum_i| S_i^* |}$$</p>
<p>$$R = \frac{\sum_i min(|S_i \cap S_i^*, b_i|)}{\sum_i min(|S_i, b_i|)}$$</p>
<p>$$F_1 = \frac{2 \ast P \ast R}{P + R}$$</p>
<p>最后考核的目标是 F1 值。简单来说一下这个 <strong>budget</strong>：</p>
<p>我们的目标是在用户来到一个地点之后，给他推荐他可能会去的店铺。这里，把问题转换一下，我们要给很多店铺来推荐可能会来这里购物的人。这里推荐的人数要受到  <strong>budget</strong> 的限制，即不能超过店铺的最大承载量以及口碑提供给这家店铺的优惠券的个数。</p>
<p>先详细介绍一下比赛提供的数据格式：</p>
<ul>
<li>Online user behavior before Dec. 2015. (ijcai2016 taobao.csv)</li>
</ul>
<p>2015 年 7 ~ 11 月淘宝上用户的购物行为数据，包括用户的点击、购买，购买物品种类等属性。</p>
<ul>
<li>Users shopping records at brick-and-mortar stores before Dec. 2015. (ijcai2016 Koubei train.csv)</li>
</ul>
<p>2015 年 7 ~ 11 月份的用户在不同地点口碑购买记录，数据格式：(user_id, loc_id, merchant_id, time_stamp)</p>
<ul>
<li>Merchant information (ijcai2016 merchant info.csv)</li>
</ul>
<p>不同口碑商铺的位置分布情况，有的商铺有连锁店，数据格式：（merchan_id, loc_id1:loc_id2:…)</p>
<ul>
<li>Prediction result. (ijcai2016 Koubei test.csv)</li>
</ul>
<p>最后要提交的数据格式</p>
<p>user_id, loc_id, merchant_id1:merchant_id2…</p>
<p>对于这个问题，我们的想法是，要充分利用每一个用户和每一个口碑商铺的历史数据。更准确的说，我们要从训练集中提取足够的可训练的特征，然后利用一些经典的模型，比如 <strong>Xgboost</strong> 来构建分类模型。同时最需要注意的是，我们要时刻考虑 <strong>budget</strong> 的影响。</p>
<h3 id="如何来利用历史数据"><a href="#如何来利用历史数据" class="headerlink" title="如何来利用历史数据 ?"></a>如何来利用历史数据 ?</h3><p>我们分析了题目所给的训练集，然后将数据切分为两部分，第一部分是 2015 年 11 月 23 号之前的数据，我们把这当做本地训练集；另一部分是 2015 年 11 月 23 号之后的数据，我们把这当做本地测试集。通过对这些数据的整理和分析，我发现几条条重大的规律：</p>
<ol>
<li>如果地点确定的话，对于一个用户，他最有可能去的是他曾经光顾过的店。</li>
<li>从口碑店铺的角度来讲，如果这家店铺曾经的销量很好，那么它也将在未来吸引更多的顾客去消费。</li>
</ol>
<p>从这两条观察出来的规律来看，我们将探索数据的方向划分为两条道路：一条是从用户的角度出发；另外则是从口碑商店的角度出发。</p>
<h4 id="从用户的历史信息来看"><a href="#从用户的历史信息来看" class="headerlink" title="从用户的历史信息来看"></a>从用户的历史信息来看</h4><p>基于我们的观察，当一个人来到一个他之前去过的地点，他们更倾向于去之前购物过的商店消费。因此，一个在过去的一段时间内在一家商铺的消费次数能够在很大程度上影响我们的推荐质量。频率越高，越能代表这个人在未来会再次来到这家店消费。</p>
<p>我们做了一个统计，如果一个用户在过去曾经关顾过一家口碑店铺超过 6 次的话，当我们在之后再向这位用户推荐这家店的话，我们会得到超过 90 % 的准确率。</p>
<p>不过从另一方面说，每一家口碑店铺都有 <strong>budget</strong> 限制，这意味着，如果按照用户之前光顾过哪家店铺来推荐的话，肯定会有部分店铺的 <strong>budget</strong> 超标。</p>
<p>我们在用户的数据中找寻这样一个键值对 (user, location, merchant)，通过计算这样一个键值对出现的频率，我们可以统计出 (user, location, merchant, frequency) 的键值对，将这个键值对按照 <strong>frequency</strong> 来从高到低排序，然后按照这个顺序，从高到低来给 (user, query) 这样一个查询来推荐店铺，直到这家店铺的 <strong>budget</strong> 耗尽为止。</p>
<p>经过统计发现，平均每一个用户在一个地点只会关顾 1.3 个商家，所以在这里我们限制最大的推荐个数为 4。</p>
<h4 id="从商家的历史信息来看"><a href="#从商家的历史信息来看" class="headerlink" title="从商家的历史信息来看"></a>从商家的历史信息来看</h4><p>经过之前的分析，我们发现不同的店铺有着不同的“受欢迎度”。举个例子，“820”这个商家在整个训练集中几乎出现 1/4。问题是我们怎么定义这个“受欢迎度”呢？</p>
<p>为了解决这个问题，我们首先定义，在不同地点的同一家口碑商铺是不一样。因为在题目给定的数据中，存在大量的连锁商铺，但是这些连锁店铺在不同地点的“受欢迎度”是完全不同的。</p>
<p>接下来我们设想这样一种情况，90 % 来到地点 A 的人都会去 商铺 B 消费，在这种情况下，如果我们给所有的来地点 A 的人都推荐 B 商铺的话，我们就能得到 90 % 的准确率。所以，在一个地点某家店铺消费的总人数占该地点所有人数的比例，我们称之为该店铺的 “受欢迎度” (Popularity)。</p>
<p>因此我们将所有店铺按照 Popularity 从大到小来进行排序，依次推荐来到该地点的所有用户，直到超过 <strong>budget</strong>。经过一些线下的实验，我们取这个 Popularity 的值的阈值为 0.25。</p>
<h4 id="引入淘宝数据来提升推荐质量"><a href="#引入淘宝数据来提升推荐质量" class="headerlink" title="引入淘宝数据来提升推荐质量"></a>引入淘宝数据来提升推荐质量</h4><p>我们之前的推荐完全没有用到每个人的特征，相当于无法做到“千人千面”。于是接下来我们就想办法，如何利用淘宝的数据来提升推荐质量。</p>
<p>从直观上而且，淘宝的数据应该很有帮助，比如，在淘宝上经常浏览或者购买电子产品的人往往不太会去关顾口碑商铺里面那些卖女式服装的。受到这个的启发，我们就建立了一个这样的表：如果存在这样一条记录，一个用户在淘宝上浏览或者购买的商铺 A；同时也在线下口碑上的商铺 B 消费，我们就把 (A, B) 这个关系链表放入表中。基于这样的表，我们对那些之前没有口碑消费记录的新用户，如果他们曾在淘宝上购买或浏览了商铺 A， 那么我们就只给他推荐在关系链表中与 A 相连的口碑商铺。线上的结果证明，我们的预测质量提升了。</p>
<h3 id="引入机器学习模型"><a href="#引入机器学习模型" class="headerlink" title="引入机器学习模型"></a>引入机器学习模型</h3><p>目前为止我们都没有怎么用机器学习模型，用普通的规则就可以在天池上面排一个不错的名次。但是为了取得更好的成绩，我们尝试着去探寻每一个用户、地点和商铺的各种各样可能的特征。下面我将详细介绍这些我们的做法。</p>
<p>我们将这个问题看成是一个二分类问题。我们的方法是对每一个店铺建模，比如说，在数据集中，用户 u 在地点 l 的店铺 m 消费了。我们可以产生一个三元组 (u, l, m)。对应于这个三元组，我们可以产生一些训练数据，首先，对于我们而言，正样本即是那些消费过的用户，即 (u, l, m) 是 True；第二，我们的负例是那些同样是这个地点的其他商铺，比如说 m‘，我们将 (u, l, m’) 定义为 False。按照这个方法，我们可以产生供二分类的训练集。根据这个道理，对于赛题要我们预测的用户来到一个地点之后会去哪些店铺的情况，我们也可以根据这个三元组，产生一个每一个店铺的预测概率。</p>
<p>这么做其实负样本是很多的。。。为了避免正负样本不平衡的问题，我们采取采样的方法去提取负样本。</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>特征工程对应机器学习来说十分重要，俗话说，特征是模型的上限。我们观察到，有些用户喜欢关顾那些他们之前去过的店，有些喜欢光顾该地点上最热门的店铺，有些则喜欢去那些刚开张的店铺。</p>
<p>所以，针对 (u, l, m) 这样一个三元组，我们试图找寻关于他们其中任意一个的特征。</p>
<p>我们找寻的特征如下：</p>
<ol>
<li>键值对 (u, l, m) 出现的次数，即 frequence，用户关顾这家店的频率。</li>
<li>键值对 (u, l, m) 是否出现过，True or False。</li>
<li>user_id 的 onehot 编码。</li>
<li>merchant_id 编码。</li>
<li>(l, m) 出现次数，即这个地点这个商店的总销量。</li>
<li>open_interval：店铺的开业时间。</li>
<li>用户 u 在淘宝上购物的次数</li>
<li>用户 u 在淘宝上浏览的次数</li>
<li>用户 u 在淘宝上购买过的商品的种类的 onehot 编码</li>
<li>用户 u 在淘宝上购买过的商铺的 id 的 onehot 编码</li>
<li>用户 u 在淘宝上购买的比例 （购买数 / (购买 + 点击）</li>
<li>用户 u 在淘宝上购买的次数</li>
<li>用户 u 在淘宝上点击的次数</li>
<li>通过 SVD 算出来的用户 u 的潜在矩阵</li>
<li>通过 SVD 算出来的商店 m 的潜在矩阵</li>
</ol>
<h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><p>之前的规则可以得出一个结果，之后的模型也可以得出一个结果，在比赛的最后阶段，我们对模型进行融合，尝试各种不同的参数，达到了这个名次。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/28/问答社区的回答排序算法/" itemprop="url">
                  问答社区的回答排序算法(一）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-28T15:50:26+08:00" content="2017-02-28">
              2017-02-28
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/02/28/问答社区的回答排序算法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/28/问答社区的回答排序算法/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/02/28/问答社区的回答排序算法/" class="leancloud_visitors" data-flag-title="问答社区的回答排序算法(一）">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>正好研究生的研究方向是问答社区的专家发现，所以，看过很多关于专家发现的 paper 以及一些工业界实践的经验。一直没有机会好好总结一下，借此机会梳理一下。问答社区的专家用户的发现有助于问题的顺利解决，并且也有助于提高问答社区的用户体验，有利于问答社区的长期发展。</p>
<p>现有的专家发现方法主要分为两种，一种是基于用户投票，另一种是基于用户之间的社交网络，本文将先介绍前一种。</p>
<h2 id="基于用户投票的排序算法"><a href="#基于用户投票的排序算法" class="headerlink" title="基于用户投票的排序算法"></a>基于用户投票的排序算法</h2><p>顾名思义，就是利用用户的反馈，即投票数（又细分为赞成和反对）来对答案进行排名。下面介绍三种方法，分别被 <strong>Urban Dictionary</strong>、<strong>Amazon</strong> 和 <strong>知乎</strong> 采用。</p>
<h3 id="得分-赞成票-反对票"><a href="#得分-赞成票-反对票" class="headerlink" title="得分 = 赞成票 - 反对票"></a>得分 = 赞成票 - 反对票</h3><p>假定有两个项目，项目 A 是 30 张赞成票，10 张反对票，项目 B 是 200 张赞成票，150 张反对票。如果按照上面的公式，B会排在前面，因为它的得分（200 - 150 = 50）高于 A（30 - 10 = 20）。但是实际上，B 的好评率只有 57%（200 / 350），而 A 为 75%（30 / 40），所以正确的结果应该是A排在前面。</p>
<p><strong>Urban Dictionary</strong> 就是这种错误算法的实例。</p>
<p><img src="/img/p1.png" alt=""></p>
<h3 id="得分-赞成票-总票数"><a href="#得分-赞成票-总票数" class="headerlink" title="得分 = 赞成票 / 总票数"></a>得分 = 赞成票 / 总票数</h3><p>如果总票数很大，这种算法其实是对的。问题出在如果总票数很少，这时就会出错。假定 A 有 2 张赞成票、0 张反对票，B 有 100 张赞成票、1 张反对票。这种算法会使得 A 排在 B 前面。这显然错误。</p>
<p><strong>Amazon</strong> 便采用的是这种做法。</p>
<p><img src="/img/p2.png" alt=""></p>
<h3 id="威尔逊得分"><a href="#威尔逊得分" class="headerlink" title="威尔逊得分"></a>威尔逊得分</h3><p>威尔逊得分的思想是，如果把一个回答展示给很多人看并让他们投票，内容质量不同的回答会得到不同比例的赞同和反对票数，最终得到一个反映内容质量的得分。当投票的人比较少时，可以根据已经获得的票数估计这个回答的质量得分，投票的人越多则估计结果越接近真实得分。</p>
<p>如果新一个回答获得了 1 票赞同 0 票反对，也就是说参与投票的用户 100% 都选了赞同，但是因为数量太少，所以得分也不会太高。如果一小段时间后这个回答获得了 20 次赞同 1 次反对，那么基于新算法，我们就有较强的信心把它排在另一个有 50 次赞同 20 次反对的回答前面。原因是我们预测当这个回答同样获得 50 次赞同时，它获得的反对数应该会小于 20。威尔逊得分算法最好的特性就是，即使前一步我们错了，现在这个新回答排到了前面，获得了更多展示，在它得到更多投票后，算法便会自我修正，基于更多的投票数据更准确地计算得分，从而让排序最终能够真实地反映内容的质量。</p>
<p>它有着很多优良特性：</p>
<ol>
<li>投票总数趋向于正无穷时，得分也趋向正反馈占总反馈的比例，对于内容质量的解释下很强。</li>
<li>在数据比较少的情况，总票数较少和极端参数的情况下，结果也能保持很好的鲁棒性。</li>
<li>置信区间大学可以通过参数保证。</li>
<li>虽然二项分布是离散类型，但是由于得分表达式关于正负反馈次数的函数是连续的，因此可以引入非整数的投票（加权投票），同时不改变算法性质。</li>
<li>得分的取值范围是（0，1），与投票总数无关，间接的做了归一化。</li>
</ol>
<p>威尔逊得分的计算公式如下：</p>
<p>$$score = \frac{\phi + \frac{z^2}{2n} - z * \sqrt{(\frac{\phi\ast (1 - \phi)}{n} + \frac{z^2}{4n^2}}}{(1 + \frac{z^2}{n})}$$</p>
<p>\(\phi\) 表示样本的赞成票比例，\(n\) 表示样本的大小，\(z_{1- \alpha/2}\) 表示对应某个置信水平的 \(z\) 统计量，这是一个常数，可以通过查表或统计软件包得到。一般情况下，在 95% 的置信水平下，z 统计量的值为 1.96。</p>
<p>威尔逊得分计算过程 (JavaScript 版本)：</p>
<pre><code>n = Up + Down
if (n==0) {
score = 0;
}
else {
z = 1.96
phat = Up / n
score = (phat + z*z/(2*n) - z * Math.sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n);
}
</code></pre><p>可以看到，当 \(n\) 的值足够大时，这个下限值会趋向。如果 \(n\) 非常小（投票人很少），这个下限值会大大小于。实际上，起到了降低 “赞成票比例” 的作用，使得该项目的得分变小、排名下降。</p>
<p>知乎采用的就是这种方法，下面摘一段<strong><a href="https://zhuanlan.zhihu.com/p/19902495?columnSlug=zhihu-product" target="_blank" rel="external">知乎产品总监</a></strong>对这个算法评价的一段话：</p>
<pre><code>  因此未来我们会看到更多新创作的优质内容，快速获得靠前的排序，低质内容则会长期
保持在底部。细心的你可能也想到了，并不是所有的回答最终都会获得很多投票，大体上
获得投票总数较多的回答仍然会排在投票较少的回答前面。
</code></pre><p>需要提一下，这个威尔逊算法在 x = 0 时函数取值收敛为 0，无法对赞同为 0，但反对票数不一样的回答进行排序。为了方便，知乎默认所有回答者对自己的投票投了一票赞同。这样不仅解决了这个问题，而且让回答者也将自身权重参与到排序的计算中。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://www.ruanyifeng.com/blog/2012/03/ranking_algorithm_wilson_score_interval.html" target="_blank" rel="external">基于用户投票的排名算法（五）：威尔逊区间</a></li>
<li><a href="https://www.zhihu.com/question/26933554" target="_blank" rel="external">如何评价知乎的回答排序算法？</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/27/数据挖掘-评分卡模型/" itemprop="url">
                  数据挖掘: 评分卡模型
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-27T17:24:27+08:00" content="2017-02-27">
              2017-02-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/02/27/数据挖掘-评分卡模型/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/27/数据挖掘-评分卡模型/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/02/27/数据挖掘-评分卡模型/" class="leancloud_visitors" data-flag-title="数据挖掘: 评分卡模型">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>信用评分卡模型在国外是一种成熟的预测方法，尤其在信用风险评估以及金融风险控制领域更是得到了比较广泛的使用，其原理是将模型变量 WOE 编码方式离散化之后运用 logistic 回归模型进行的一种二分类变量的广义线性模型。本文重点介绍模型变量 WOE 以及 IV 原理.</p>
<h3 id="WOE"><a href="#WOE" class="headerlink" title="WOE"></a>WOE</h3><p>WOE的全称是 “Weight of Evidence”，即证据权重。WOE 是对原始自变量的一种编码形式。要对一个变量进行 WOE 编码，需要首先把这个变量进行分组处理。分组后，对于第 i 组，WOE的计算公式如下：</p>
<p>$$woe_i = ln\frac{P_{y_i}}{P_{n_i}} = ln\frac{y_i/y_s}{n_i/n_s}$$</p>
<p>其中，\(p_{y_i}\) 是这个组中响应客户，风险模型中，对应的是违约客户，指的是模型中预测变量取值为 1 的个体占所有样本中所有响应客户的比例，\(p_{n_i}\) 是这个组中未响应客户占样本中所有未响应客户的比例，\(y_i\) 是这个组中响应客户的数量，\(n_i\) 是这个组中未响应客户的数量，\(y_s\) 是样本中所有响应客户的数量，\(n_s\) 是样本中所有未响应客户的数量。</p>
<p>简单来说， WOE 表示的是<strong>当前分组中响应的用户占所有响应用户的比例</strong>和<strong>当前分组中没有响应的用户占所有没有响应用户的比例</strong>的差异。</p>
<p>WOE 也可以这么来理解，</p>
<p>$$woe_i = ln\frac{P_{y_i}}{P_{n_i}} = ln\frac{y_i/y_s}{n_i/n_s}$$</p>
<p>WOE 同时也等于：</p>
<p>$$woe_i = ln\frac{y_i/n_i}{y_s/n_s}$$</p>
<p>可以看出，WOE 也可以表示<strong>当前这个组中响应的客户和未响应的客户的比例</strong>和<strong>所有样本中这个比例</strong>的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE 越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE 越小，差异越小，这个分组里的样本响应的可能性就越小。WOE 蕴含了自变量取值对应目标变量（违约概率）的影响。再加上 WOE 计算形式与 logistic 回归中目标变量的 logistic 转换很相似，因此可以将自变量 WOE 替代原先的自变量值。</p>
<h3 id="IV"><a href="#IV" class="headerlink" title="IV"></a>IV</h3><p>其实 IV 衡量的是某一个变量的信息量，从公式来看的话，相当于是自变量 WOE 值的一个加权求和，其值的大小决定了自变量对于目标变量的影响程度；从另一个角度来看的话，IV 公式与信息熵的公式极其相似。</p>
<p>IV 公式如下：</p>
<p>$$IV_i = (P_{yi} - P_{ni}) \ast WOE_i $$</p>
<p>$$= (P_{yi} - P_{ni}) \ast ln \frac{P_{y_i}}{P_{n_i}}$$</p>
<p>$$= (y_i / y_s - n_i / n_s) ln \frac{y_i/y_s}{n_i/n_s}$$</p>
<p>计算了一个变量各个组的 IV 值之后，我们就可以计算整个变量的 IV 值：</p>
<p>$$IV = \sum_i^n IV_i$$</p>
<p>其中 n 为变量的分组个数。</p>
<ul>
<li>对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，IV 值越大，否则，IV 值越小；</li>
<li>极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV 值为0；</li>
<li>IV 值的取值范围是[0, 正无穷)，且当当前分组中只包含响应客户或者未响应客户时，IV = 正无穷。</li>
</ul>
<p>如果想要对变量的预测能力进行排序的话，可以按 IV 值从高到第去筛选。</p>
<h3 id="WOE-和-IV-的区别"><a href="#WOE-和-IV-的区别" class="headerlink" title="WOE 和 IV 的区别"></a>WOE 和 IV 的区别</h3><p>WOE 和 IV 都能表达着这个分组对目标变量的预测能力。但实际中是选择 IV 而不是 WOE 的和来衡量变量预测的能力，这是为什么呢？首先，因为我们在衡量一个变量的预测能力时，我们所使用的指标值不应该是负数。从这意义上来说，IV 比 WOE 多乘以前面那个因子，就保证了它不会是负数；然后，乘以 \((P_{yi} - P_{ni})\) 这个因子，体现出了变量当前分组中个体的数量占整体的比例，从而很好考虑了这个分组中样本占整体的比例，比例越低，这个分组对变量整体预测能力的贡献越低。相反，如果直接用 WOE 的绝对值加和，会因为该分组出现次数偏少的影响而得到一个很高的指标。</p>
<h3 id="IV-对极端情况的处理"><a href="#IV-对极端情况的处理" class="headerlink" title="IV 对极端情况的处理"></a>IV 对极端情况的处理</h3><p>IV 依赖 WOE，并且 IV 是一个很好的衡量自变量对目标变量影响程度的指标。但是，使用过程中应该注意一个问题：变量的任何分组中，不应该出现响应数为 0 或非响应数为 0 的情况。</p>
<p>当变量的一个分组中的响应数为 0 时，对应的 WOE 就为负无穷。此时 IV 为正无穷。</p>
<p>当变量的一个分组中的响应不为 0 时，道理也类似。</p>
<p>所以 IV 不能自动处理变量的分组中出现响应比例为 0 或 100% 的情况。遇到这种情况：</p>
<ul>
<li>如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件；</li>
<li>重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），强烈建议这样做，因为本身把一个分组个体数弄得很小就不是太合理。</li>
<li>如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为 0，可以人工调整响应数为 1，如果非响应数原本为 0，可以人工调整非响应数为 1。</li>
</ul>
<h3 id="信用卡模型"><a href="#信用卡模型" class="headerlink" title="信用卡模型"></a>信用卡模型</h3><p>因为 IV 是衡量自变量对目标变量的指标之一，也可以看做是单个自变量的评分模型，可以直接将这个自变量的取值当做事某种信用评分</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/21/机器学习常见算法总结/" itemprop="url">
                  机器学习常见算法总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-21T15:11:48+08:00" content="2017-02-21">
              2017-02-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/02/21/机器学习常见算法总结/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/21/机器学习常见算法总结/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/02/21/机器学习常见算法总结/" class="leancloud_visitors" data-flag-title="机器学习常见算法总结">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>事件 A 和 B 同时发生的概率为在 A 发生的情况下发生 B 或者在 B 发生的情况下发生 A</p>
<p>$$P(A\cap B) = P(B) * P(A|B)$$</p>
<p>所以，</p>
<p>$$P(A|B) = \frac{P(B|A) * P(A)}{P(B)}$$</p>
<p>对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</p>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ol>
<li>假设现在有样本 \(x = (a_1,a_2,…,a_n)\) 这个待分类项(并认为 x 里面的特征独立)。</li>
<li>再假设现在有分类目标 \(Y = (y_1,y_2,…,y_n)\)</li>
<li>那么 \(max(P(y_1|x),P(y_2|x),P(y_3|x),…,P(y_n|x))\) 就是最终的分类类别。</li>
<li>而 \(Y = (y_1,y_2,…,y_n)\) 就是最终的分类类别。</li>
</ol>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><h4 id="1-准备阶段"><a href="#1-准备阶段" class="headerlink" title="1. 准备阶段"></a>1. 准备阶段</h4><p>确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。</p>
<h4 id="2-训练阶段"><a href="#2-训练阶段" class="headerlink" title="2. 训练阶段"></a>2. 训练阶段</h4><p>计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计。</p>
<h4 id="3-应用阶段"><a href="#3-应用阶段" class="headerlink" title="3. 应用阶段"></a>3. 应用阶段</h4><p>使用分类器进行分类，输入时分类器和待分类样本，输出是样本属于的分类类别。</p>
<h3 id="属性特征"><a href="#属性特征" class="headerlink" title="属性特征"></a>属性特征</h3><ol>
<li>特征为离散值时直接统计即可。</li>
<li>特征为连续值的时候假定特征符合高斯分布：\(g(x,n,u)\)</li>
</ol>
<h3 id="拉普拉斯校准"><a href="#拉普拉斯校准" class="headerlink" title="拉普拉斯校准"></a>拉普拉斯校准</h3><p>当某个类别下某个特征划分没有出现时，会有 \(P(a|y) = 0\)，就是导致分类器质量降低，所以此时引入 拉普拉斯校验，就是对每类别下所有划分的计数加 1。</p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ol>
<li>对小规模的数据表现很好，适合多分类任务，适合增量式训练。</li>
<li>对输入数据的表达形式很敏感。(离散、连续、极大值，极小值）。</li>
</ol>
<h2 id="逻辑回归和线性回归"><a href="#逻辑回归和线性回归" class="headerlink" title="逻辑回归和线性回归"></a>逻辑回归和线性回归</h2><p>LR回归是一个线性的二分类模型，主要是计算在某个样本特征下事件发生的概率，比如根据用户的浏览购买情况作为特征来计算它是否会购买这个商品，抑或是它是否会点击这个商品。然后LR的最终值是根据一个线性和函数再通过一个 <strong>sigmoid</strong> 函数来求得，这个线性和函数权重与特征值的累加以及加上偏置求出来的，所以在训练LR时也就是在训练线性和函数的各个权重值w。</p>
<p>$$h_w(x) = \frac{1}{1 + e^{-(w^Tx + b)}}$$</p>
<p>关于这个权重值 w 一般使用最大似然法来估计，假设现在有样本 \(x_i, y_i\)，其中 xi 表示样本的特征，\(y_i \epsilon 0, 1\)表示样本的分类真实值，\(y_i = 1\) 的概率是 \(p_i\)，则 \(y_i = 0\) 的概率是 \(1 − p_i\)，那么观测概率为:</p>
<p>$$p(y_i) = p_i^{y_i} * (1 - p_i)^{1 - y_i}$$</p>
<p>则最大似然估计为：</p>
<p>$$\prod h_w(x_i)^y_i * (1 - h_w(x_i))^{1 - y_i}$$</p>
<p>对这个似然函数取对数之后就会得到表达式：</p>
<p>$$L(w) = \sum_{i}^{N} \left ( y_i \ast logh_w(x_i) + (1 - y_i) \ast log(1 - h_w(x_i))\right )$$</p>
<p>对这个 L(w) 求极大值就可以得到 w 的估计值。</p>
<p>实际计算中，常改为求极小值，在前面加个负号即可。故求解问题就变成了这个最大似然函数的最优化问题，这里通常会采取随机梯度下降和拟牛顿迭代法来进行优化。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>LR 的损失函数为：</p>
<p>$$J(w) = - \frac{1}{N} \sum_{i = 1}^{N} (y_i \ast log(h_w(x_i)) + (1 - y_i) \ast log(1 - h_w(x_i)))$$</p>
<p>这里除以 N 是因为求的是最小均方误差，这样就变成了求 min(J(w))，</p>
<p>其更新 w 的过程为:</p>
<p>$$w := w - \alpha \ast \bigtriangledown J(w)$$</p>
<p>$$w := w - \alpha \frac{1}{N} \ast \sum_{i = 1}^{N} ((h_w(x_i) - y_i) \ast x_i)$$</p>
<p>其中 \(\alpha\) 为步长，直到 \(J(w)\) 不能再小时停止。</p>
<p>批量梯度下降法的最大问题是会陷入局部最优，并且每次在对当前样本计算 cost 的时候都需要遍历全部样本，这样计算速度会很慢，计算的时候可以转为矩阵乘法去更新整个 w 值。</p>
<p>有好多方法也采用随机梯度下降，它在计算 cost 的时候只计算当前代价，最终 cost 是在全部样本迭代一次求和得出，还有他在更新当前的参数 w 的时候并不是依次遍历样本，而是从所有样本中随机选择一条进行计算，这种方法收敛速度快，也可以避免局部最优，还容易并行。</p>
<p>$$w := w - \alpha \ast \left ( (h_w(x_j) - y_i) \ast x_i \right )$$</p>
<p>$$j \epsilon 1…N$$</p>
<p>SGD 可以改进的地方是使用动态的步长。</p>
<h3 id="其他的优化方法"><a href="#其他的优化方法" class="headerlink" title="其他的优化方法"></a>其他的优化方法</h3><ul>
<li>拟牛顿法</li>
<li>BFDS</li>
<li>L-BFGS</li>
</ul>
<p>优缺点： 无需选择学习率，更快，但也更复杂</p>
<h3 id="如何避免过拟合"><a href="#如何避免过拟合" class="headerlink" title="如何避免过拟合"></a>如何避免过拟合</h3><ol>
<li>减少 feature 的个数</li>
<li>正则化 (L1, L2)</li>
</ol>
<p>添加 L2 正则化之后的损失函数为：</p>
<p>$$J(w) = - \frac{1}{N} \sum_{i = 1}^{N} (y_i \ast log(h_w(x_i)) + (1 - y_i) \ast log(1 - h_w(x_i))) +\lambda \left | w \right |_2$$</p>
<p>同时 w 的更新变为:</p>
<p>$$w := w - \alpha \ast ( h_w(x_j) - y_j) \ast x_i ) - 2\alpha \ast w_j$$</p>
<p>这里 \(w_0\) 不受正则化影响。</p>
<h3 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h3><ol>
<li>实现简单，计算量小</li>
<li>容易欠拟合</li>
</ol>
<h2 id="KNN-算法"><a href="#KNN-算法" class="headerlink" title="KNN 算法"></a>KNN 算法</h2><p>给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类.</p>
<h3 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a>三要素</h3><ol>
<li>k 值的选择</li>
<li>距离的度量</li>
<li>分类决策规则</li>
</ol>
<h3 id="k-值的选择"><a href="#k-值的选择" class="headerlink" title="k 值的选择"></a>k 值的选择</h3><ol>
<li>k值越小表明模型越复杂，更加容易过拟合</li>
<li>但是 k 值越大，模型越简单，如果 k = N 就代表什么点都是训练集中类别最多的那个类。</li>
</ol>
<p><strong>所以一般k会取一个较小的值，然后用过交叉验证来确定<br>这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k</strong></p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>计算量大；</li>
<li>样本不均时会造成问题；</li>
<li>需要大量的内存。</li>
</ol>
<h3 id="Kd-树"><a href="#Kd-树" class="headerlink" title="Kd 树"></a>Kd 树</h3><p>Kd 树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）</p>
<h4 id="构造-Kd-树"><a href="#构造-Kd-树" class="headerlink" title="构造 Kd 树"></a>构造 Kd 树</h4><p>在 k 维的空间上循环找子区域的中位数进行划分的过程</p>
<p>假设现在有 k 维空间的数据集 T = {x1, x2, x3,…,xn}，xi = {a1, a2, a3,…,ak}</p>
<ol>
<li>首先构造根节点，以坐标 a1 的中位数 b 为切分点，将根结点对应的矩形区域划分为两个区域，区域 1 中 a1 &lt; b，区域 2 中 a1 &gt; b</li>
<li>构造叶子节点，分别以上面两个区域中 a2 的中位数作为切分点，再次将他们两两划分，作为深度为 1 的叶子节点</li>
<li>不断重复 2 操作，深度为 j 的叶子节点划分的时候，索取的 ai 的 i = j % k + 1，直到两个子区域没有实例时停止。</li>
</ol>
<h4 id="Kd-树的搜索"><a href="#Kd-树的搜索" class="headerlink" title="Kd 树的搜索"></a>Kd 树的搜索</h4><ol>
<li>首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi</li>
<li>将这个叶子节点认为是当前的 “近似最近点”</li>
<li>递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“</li>
<li>重复3的步骤，直到另一子区域与球体不相交或者退回根节点</li>
<li>最后更新的 ”近似最近点“ 与x真正的最近点</li>
</ol>
<h3 id="Kd-树进行-KNN-查找"><a href="#Kd-树进行-KNN-查找" class="headerlink" title="Kd 树进行 KNN 查找"></a>Kd 树进行 KNN 查找</h3><p>通过 Kd 树的搜索找到与搜索目标最近的点，这样 KNN 的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。</p>
<h3 id="Kd-树搜索的复杂度"><a href="#Kd-树搜索的复杂度" class="headerlink" title="Kd 树搜索的复杂度"></a>Kd 树搜索的复杂度</h3><p>当实例随机分布的时候，搜索的复杂度为 log(N)，N 为实例的个数，KD树更加适用于实例数量远大于空间维度的 KNN 搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。</p>
<h2 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM (支持向量机)"></a>SVM (支持向量机)</h2><p>对于样本点 (Xi,Yi) 以及 svm 的超平面：\(w^Tx_i + b = 0\)</p>
<ul>
<li>函数间隔：\(yi \ast ( w^Tx_i + b )\)</li>
<li>几何间隔： \(\frac{(yi \ast ( w^Tx_i + b )}{||w||}\)</li>
</ul>
<p>SVM 的基本思想是求解能正确划分训练样本并且其几何间隔最大化的超平面。</p>
<h3 id="线性-SVM-问题"><a href="#线性-SVM-问题" class="headerlink" title="线性 SVM 问题"></a>线性 SVM 问题</h3><p>$$argmax_{w,b}   \gamma$$</p>
<p>同时满足：</p>
<p>$$st. \frac{y_i(w^Tx_i + b)}{||w||} \geq \gamma $$</p>
<p>那么假设 \(\widehat{\gamma } = \gamma \ast ||w||\)，则问题转换为：</p>
<p>$$argmax \frac{\widehat{\gamma }}{||w||}$$</p>
<p>$$st. y_i(w^Tx_i + b) \geq 1$$</p>
<p>由于 \(\widehat{\gamma }\) 的成比例增减不会影响实际间距，所以这里的取 \(\widehat{\gamma } = 1\)，又因为 \(max(\frac{1}{||w||}) = min(\frac{1}{2} \ast ||w||^2）\)。</p>
<p>所以最终问题就变成了</p>
<p>$$argmin_{w,b}  \frac{1}{2} \ast ||w||^2  \gamma$$</p>
<p>$$st. y_i(w^Tx_i + b) \geq 1$$</p>
<p>这样就把原始问题转换为了一个凸的二次规划，可以将其转换为拉格朗日函数，然后使用对偶算法来求解。</p>
<h3 id="对偶求解"><a href="#对偶求解" class="headerlink" title="对偶求解"></a>对偶求解</h3><p>引进拉格朗日乘子 \(a = {a_1, a_2,…}\)，定义拉格朗日函数：</p>
<p>$$L(w, b, a) = \frac{1}{2} \ast ||w||^2 - \sum_{i=1}^{N} (\alpha_i \ast y_i (w^Tx_i + b)) + \sum_{i = 1}^{N} (\alpha_i)$$</p>
<p>根据对偶性质，原始问题就是对偶问题的极大极小</p>
<p>$$max min L(w, b, a)$$</p>
<p>先求 L 对 w，b 的极小，再求对 \(\alpha\) 的极大。第一步，相当于对 \(w, b) 求偏导并且令其等于 0</p>
<p>$$\bigtriangledown_w L(w, b, a) = w - \sum_{i = 1}^{N} a_iy_ix_i$$</p>
<p>$$\bigtriangledown_b L(w, b, a) = \sum_{i = 1}^{N} a_iy_i$$</p>
<p>带入后即得，</p>
<p>$$minL(w, b, a) = - \frac{1}{2} \ast \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_ia_jy_iy_j(x_i \cdot  x_j) + \sum_{i = 1}^{N} a_i$$</p>
<p>对上式求极大，即是对偶问题：</p>
<p>$$max - \frac{1}{2} \ast \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_ia_jy_iy_j(x_i \cdot x_j) + \sum_{i = 1}^{N} a_i$$</p>
<p>$$st. \sum_{i = 1}^{N}(\alpha_i y_i) = 0$$</p>
<p>$$a \geq 0, i = 1, 2, 3…$$</p>
<p>将求最大转为求最小，得到等价的式子为：</p>
<p>$$min \frac{1}{2} \ast \sum_{i = 1}^{N} \sum_{j = 1}^{N} (a_ia_jy_iy_j(x_i \cdot x_j) - \sum_{i = 1}^{N} a_i$$</p>
<p>$$st. \sum_{i = 1}^{N}(\alpha_i y_i) = 0$$</p>
<p>$$a \geq 0, i = 1, 2, 3…$$</p>
<p>假如求解出来的 \(\alpha^* (\alpha_1^\ast,…\alpha_n^\ast)\)</p>
<p>则得到最优的 w，b 分别为</p>
<p>$$w^ \ast = \sum_{i = 1}^{N}(\alpha_i^\ast y_ix_i)$$</p>
<p>$$b^ \ast = y_i - \sum_{i = 1}^{N}(\alpha_i^\ast y_i (x_i \cdot x_j))$$</p>
<p>所以，最终的决策分类面为：</p>
<p>$$f(x) = sign(\sum_{i = 1}^{N} a_i^\ast y_i(x \cdot x_i) + b^*)$$</p>
<p>即，分类决策函数只依赖于输入 x 与训练样本输入的内积。</p>
<h4 id="SVM-软间距最大化"><a href="#SVM-软间距最大化" class="headerlink" title="SVM 软间距最大化"></a>SVM 软间距最大化</h4><p>软间距最大化引用了松弛变量 \(\xi \)，将原问题的求解转换为：</p>
<p>$$argmin_{w,b} \frac{1}{2} \ast ||w||^2  + C \sum_{i = 1}^{N} \xi_i$$</p>
<p>$$y_i(w^Tx_i + b) \geq 1 - \xi_i$$</p>
<p>$$\xi_i \geq 0, i = 1,2,…N$$</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>优化目标为：</p>
<p>$$\sum [1 - y_i(w^Tx_i + b)]_+  + \lambda ||w||^2$$</p>
<p>其中 \([1 - y_i(w^Tx_i + b)]_+\) 称为折页损失函数，当其值小于等于 0 时，返回 0。</p>
<h3 id="为何要引入对偶算法"><a href="#为何要引入对偶算法" class="headerlink" title="为何要引入对偶算法"></a>为何要引入对偶算法</h3><ol>
<li>对偶问题往往更加容易求解。</li>
<li>可以很自然的引入核函数。</li>
</ol>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>将输入特征x（线性不可分）映射到高维特征 R 空间，可以在 R 空间上让 SVM 进行线性可以变，这就是核函数的作用。常见的核函数有：</p>
<ul>
<li>多项式核函数: \( K(x, z)=(x \ast z + 1)^p\)</li>
<li>高斯核函数: \( K(x, z) = exp(\frac{-(x - z)^2}{\sigma ^2})\) </li>
</ul>
<h3 id="SVM优缺点"><a href="#SVM优缺点" class="headerlink" title="SVM优缺点"></a>SVM优缺点</h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ol>
<li>使用核函数可以向高维空间进行映射</li>
<li>使用核函数可以解决非线性的分类</li>
<li>分类思想很简单，就是将样本与决策面的间隔最大化</li>
<li>分类效果较好</li>
</ol>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ol>
<li>对大规模数据训练比较困难</li>
<li>无法直接支持多分类，但是可以使用间接的方法来做</li>
</ol>
<h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>SMO是用于快速求解SVM的。它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：</p>
<ol>
<li>其中一个是严重违反KKT条件的一个变量。</li>
<li>另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。</li>
</ol>
<h3 id="SVM多分类问题"><a href="#SVM多分类问题" class="headerlink" title="SVM多分类问题"></a>SVM多分类问题</h3><ul>
<li>直接法</li>
</ul>
<p>直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难）</p>
<ul>
<li>间接法</li>
</ul>
<ol>
<li>一对多</li>
</ol>
<p>其中某个类为一类，其余 n-1 个类为另一个类，比如 A,B,C,D 四个类，第一次 A 为一个类，{B,C,D} 为一个类训练一个分类器，第二次B为一个类，{A,C,D}为另一个类，按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x)，f2(x)，f3(x) 和 f4(x)，取其最大值为分类器 (这种方式由于是1对M分类，会存在偏置，很不实用)</p>
<ol>
<li>一对一(libsvm实现的方式)</li>
</ol>
<p>任意两个类都训练一个分类器，那么n个类就需要 n*(n-1)/2 个 SVM 分类器。<br>还是以A,B,C,D为例，那么需要 {A,B},{A,C},{A,D},{B,C},{B,D},{C,D} 为目标共 6 个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要 n*(n-1)/2 个分类器代价太大）</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树是一颗依托决策而建立起来的树。</p>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><ol>
<li>首先是针对当前的集合，计算每个特征的信息增益</li>
<li>然后选择信息增益最大的特征作为当前节点的决策决策特征</li>
<li>根据特征不同的类别划分到不同的子节点（比如年龄特征有青年，中年，老年，则划分到3颗子树）</li>
<li>然后继续对子节点进行递归，直到所有特征都被划分</li>
</ol>
<p>熵的定义：</p>
<p>$$H(D) = - \sum_{i = 1}^{n} p_i \ast log(p_i)$$</p>
<p>一个属性中的某个类别 Di 的熵，为：</p>
<p>$$H(D|D_i) = - \sum_{i}^{ } \frac{|D_i|}{|D|} \ast H(D_i)$$</p>
<p>$$H(D_i) = - \sum_i^K \frac{D_{ik}}{D_i} log_2 \frac{D_{ik}}{D_i}$$</p>
<p>信息增益表示分类目标的熵减去当前属性的熵，增益越大，分类能力越强。</p>
<p>$$Gain(C, A) = H(D) - H(D|D_i)$$</p>
<h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>设树的叶子节点个数为 \(T\)，\(t\) 为其中一个叶子节点，该叶子节点有\(N_t\)个样本，其中 k 类的样本有\(N_{tk}\)个，\(H(t)\)为叶子节点上的经验熵，则损失函数定义为：</p>
<p>$$C_t(T) = \sum (N_t \ast H(t)) + \lambda |T|$$</p>
<p>其中，</p>
<p>$$H_t(T) = - \sum \frac{N_{tk}}{N_t} \ast log \frac{N_{tk}}{N_t}$$</p>
<p>代入可以得到</p>
<p>$$C_t(T) = \sum \sum N_{tk} \ast log \frac{N_{tk}}{N_t} + \lambda |T|$$</p>
<p>其中 \(\lambda |T| \) 为正则化项， \(\lambda \) 是用于调节比率，决策树的生成只考虑了信息增益。 </p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>它是ID3的一个改进算法，使用信息增益率来进行属性的选择</p>
<p>$$splitInformation(D | A) = - \sum_{i} \frac{D_i}{D} \ast log_2 \frac{D_i}{D}$$</p>
<p>$$GainRatio(D,A) = \frac{Gain(D,A)}{splitInformation(D, A)}$$</p>
<h4 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h4><p>准确率高，但是子构造树的过程中需要进行多次的扫描和排序，所以它的运算效率较低。</p>
<h3 id="Cart"><a href="#Cart" class="headerlink" title="Cart"></a>Cart</h3><p>分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：</p>
<ul>
<li>分类树：基尼指数最小化(gini_index）</li>
<li>回归树：平方误差最小化</li>
</ul>
<h4 id="分类树："><a href="#分类树：" class="headerlink" title="分类树："></a>分类树：</h4><ol>
<li>首先是根据当前特征计算他们的基尼增益</li>
<li>选择基尼增益最小的特征作为划分特征</li>
<li>从该特征中查找基尼指数最小的分类类别作为最优划分点</li>
<li>将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于</li>
<li>针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值</li>
</ol>
<p><strong>gini</strong> 用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）</p>
<p>$$gini(a_i) = 1 - \sum_i(p_i^2)$$</p>
<p>\(p_i\) ：当前数据集中第 i 类样本的比例；</p>
<p><strong>gini</strong> 越小，表示样本越均匀，越大越不均匀。</p>
<p>基尼增益：</p>
<p>$$giniGain = \sum_i (\frac{N_i}{N} \ast gini(a_i))$$</p>
<p>\(\frac{N_i}{N}\) 表示当前类别占所有类别的概率<br>最终Cart选择GiniGain最小的特征作为划分特征。</p>
<h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><p>回归树是以平方误差最小化的准则划分为两块区域。</p>
<ol>
<li>遍历特征计算最优的划分点s，使其最小化的平方误差是：\(min(min(\sum_i^{R1}((y_i−c_1)^2)) + min(\sum_i^{R2}((y_i−c_2)^2)))\)<br>计算根据s划分到左侧和右侧子树的目标值与预测值之差的平方和最小，这里的预测值是两个子树上输入 \(x_i\) 样本对应 \(y_i\) 的均值。</li>
<li>找到最小的划分特征 j 以及其最优的划分点 s，根据特征 j 以及划分点 s 将现有的样本划分为两个区域，一个是在特征 j 上小于等于 s，另一个在在特征 j 上大于 s</li>
<li>进入两个子区域按上述方法继续划分，直到到达停止条件</li>
</ol>
<h3 id="停止条件"><a href="#停止条件" class="headerlink" title="停止条件"></a>停止条件</h3><ol>
<li>直到每个叶子节点都只有一种类型的记录时停止。（这种方式很容易过拟合）</li>
<li>另一种时当叶子节点的记录树小于一定的阈值或者节点的信息增益小于一定的阈值时停止。</li>
</ol>
<h3 id="决策树的分类与回归"><a href="#决策树的分类与回归" class="headerlink" title="决策树的分类与回归"></a>决策树的分类与回归</h3><h4 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h4><p>输出叶子节点中所属类别最多的那一类。</p>
<h4 id="回归树-1"><a href="#回归树-1" class="headerlink" title="回归树"></a>回归树</h4><p>输出叶子节点中各个样本值的平均值。</p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出（也就是投票的思想）</p>
<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><ol>
<li>现在有 N 个训练样本，每个样本的特征为 M 个，需要建 K 颗树。</li>
<li>从 N 个训练样本中有放回的取 N 个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差）。</li>
<li>从 M 个特征中取m个特征左右子集特征。 (m &lt;&lt; M)</li>
<li>对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类。</li>
<li>重复2的过程K次，即可建立森林。</li>
</ol>
<h3 id="预测过程"><a href="#预测过程" class="headerlink" title="预测过程"></a>预测过程</h3><ol>
<li>将预测样本输入到K颗树分别进行预测。</li>
<li>如果是分类问题，直接使用投票的方式选择分类频次最高的类别。</li>
<li>如果是回归问题，使用分类之后的均值作为结果。</li>
</ol>
<h3 id="参数问题"><a href="#参数问题" class="headerlink" title="参数问题"></a>参数问题</h3><ol>
<li>这里的一般取 m = sqrt(M)</li>
<li>关于树的个数K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）</li>
<li>树的最大深度，（太深可能可能导致过拟合）</li>
<li>节点上的最小样本数、最小信息增益</li>
</ol>
<h3 id="泛化误差估计"><a href="#泛化误差估计" class="headerlink" title="泛化误差估计"></a>泛化误差估计</h3><p>使用 <strong>oob（out-of-bag）</strong> 进行泛化误差的估计，将各个树的未采样样本作为预测样本（大约有36.8%），使用已经建立好的森林对各个预测样本进行预测，预测完之后最后统计误分得个数占总预测样本的比率作为 RF 的 oob 误分率。</p>
<h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><ul>
<li>ID3 算法：处理离散值的量</li>
<li>C45 算法：处理连续值的量</li>
<li>Cart 算法：离散和连续 两者都合适。</li>
</ul>
<h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><ol>
<li>能够处理大量特征的分类，并且还不用做特征选择</li>
<li>在训练完成之后能给出哪些feature的比较重要</li>
<li>训练速度很快</li>
<li>很容易并行</li>
<li>实现相对来说较为简单</li>
</ol>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>GBDT的精髓在于训练的时候都是以上一颗树的残差为目标，这个残差就是上一个树的预测值与真实值的差值。</p>
<p><strong>比如，当前样本年龄是18岁，那么第一颗会去按18岁来训练，但是训练完之后预测的年龄为12岁，差值为6，<br>所以第二颗树的会以6岁来进行训练，假如训练完之后预测出来的结果为6，那么两棵树累加起来就是真实年龄了，<br>但是假如第二颗树预测出来的结果是5，那么剩余的残差1就会交给第三个树去训练。</strong></p>
<p>Boosting 的好处就是每一步的参加就是变相了增加了分错 instance 的权重，而对已经对的 instance 趋向于0，这样后面的树就可以更加关注错分的 instance 的训练了。</p>
<h3 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h3><p>Shrinkage 认为，每次走一小步逐步逼近的结果要比每次迈一大步逼近结果更加容易避免过拟合。</p>
<p>$$y_i = y_{i-1} + step \ast y_i$$</p>
<h3 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h3><ul>
<li>树的个数 100 ~ 10000</li>
<li>叶子的深度 3 ~ 8</li>
<li>学习速率 0.01 ~ 1</li>
<li>叶子上最大节点树 20</li>
<li>训练采样比例 0.5 ~ 1 </li>
<li>训练特征采样比例 sqrt(num)</li>
</ul>
<h3 id="优缺点：-1"><a href="#优缺点：-1" class="headerlink" title="优缺点："></a>优缺点：</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>精度高</li>
<li>能处理非线性数据</li>
<li>能处理多特征类型</li>
<li>适合低维稠密数据</li>
</ol>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>并行麻烦（因为上下两颗树有联系）</li>
<li>多分类的时候 复杂度很大                         </li>
</ol>
<h3 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h3><ul>
<li><a href="http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/" target="_blank" rel="external">机器学习常见算法个人总结（面试用）</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/13/算法面试总结：大数据题/" itemprop="url">
                  面试总结：大数据题
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-13T12:22:45+08:00" content="2017-02-13">
              2017-02-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/面试总结/" itemprop="url" rel="index">
                    <span itemprop="name">面试总结</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/02/13/算法面试总结：大数据题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/13/算法面试总结：大数据题/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/02/13/算法面试总结：大数据题/" class="leancloud_visitors" data-flag-title="面试总结：大数据题">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3 id="1-认识布隆过滤器"><a href="#1-认识布隆过滤器" class="headerlink" title="1. 认识布隆过滤器"></a>1. 认识布隆过滤器</h3><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>不安全网页的黑名单包含 100 亿个黑名单网页，每个网页的 URL 最多占用 64 B。限制响应实现一种网页过滤系统，可以根据网页的 URL 判断该网页是否在黑名单上，请设计该系统。</p>
<h4 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h4><ol>
<li>该系统允许有万分之一以下的判断失误率。</li>
<li>使用的额外空间不要超过 30 GB。</li>
</ol>
<h4 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h4><p>提示：一个布隆过滤器精确地代表一个集合，并且可以精确判断一个元素是否在集合中。</p>
<p>补充：一个优秀的哈希函数能够做到很多不同的输入值所得到的返回值非常均匀地分布在 S 上，那么将所有的返回值对 m 取余，可以认为所有的返回值也会均匀地分布在 0 ~ m-1 的空间上。</p>
<p>布隆过滤器：假设有一个长度为 m 的 bit 类型的数组，即数组中的每一个位置只占一个 bit，每一个 bit 只有 0 和 1 两种状态。再假设一共有 k 个哈希函数，这些函数的输入域 S 都大于或等于 m，并且这些哈希函数都足够优秀，彼此之间也完全独立。那么对同一个输入对象（假设是一个字符串记为 URL），经过 k 个哈希函数算出来的结果也是独立的，可能相同，也可能不同，但彼此独立。对算出来的每一个结果都对 m 取余，然后在 bit array 上把相应的位置设为 1。我们把 bit 类型的数组记为 bitMap。至此，一个输入对象对 bitMap 的影响过程就结束了，即 bitMap 中的一些位置会被涂黑。接下来按照该方法处理所有的输入对象，每个对象都可能把 bitMap 中的一些白位置涂黑，也可能遇到已经涂黑的位置，遇到已经涂黑的位置让其继续为黑即可。处理完所有的输入对象后，可能 bitMap 中已经有相当多的位置被涂黑。至此，一个布隆过滤器生成完毕，这个布隆过滤器代表之前所有输入对象组成的集合。</p>
<p>那么在检查阶段时，假设一个对象为 a，想检查它是否是之前的输入对象，就把 a 通过 k 个哈希函数算出 k 个值，然后把 k 个值取余，就得到在 0 ~ m-1 范围上的 k 个值。接下来在 bitMap 上看这些位置是不是都为黑，如果有一个不为黑，就说明 a 一定不在这个集合里。如果都为黑，说明 a 在这个集合里。</p>
<p>如果 bitMap 的大小 m 相比输入对象的个数 n 过小，失误率会变大。接下来介绍根据 n 的大小和我们想要达到的失误率 p，如果确定布隆过滤器的大小 m 和哈希函数的个数 k，最后是布隆过滤器的失误率分析。</p>
<p>比如，黑名单样本的个数为 100 亿个，记为 n；失误率不能超过 0.01%，记为 p；每个样本的大小为 64 B，这个信息不会影响布隆过滤器的大小，只和选择哈希函数有关，一般的哈希函数都可以接收 64 B 的输入对象，所以使用布隆过滤器还有一个好处是不用顾忌单个样本的大小，它丝毫不能影响布隆过滤器的大小。</p>
<p>所以 n = 100 亿，p = 0.01%，布隆过滤器的大小 m 由以下公式确定：</p>
<p>$$m  = - \frac{n \times ln p}{(ln 2)^2}$$</p>
<p>根据公式计算出 m = 19.19n，向上取整为 20n，即需要 2000 亿个 bit，也就是 25 GB。</p>
<p>哈希函数的个数由以下公式决定：</p>
<p>$$k  = ln 2 \times \frac{m}{n} = 0.7 \times \frac{m}{n}$$</p>
<p>计算出哈希函数个数为 k = 14 个。</p>
<p>然后用 25 GB 的 bitMap 再单独实现 14 个哈希函数，根据如上描述生成布隆过滤器即可。</p>
<p>因为我们在确定布隆过滤器大学的过程中选择了向上取整，所以还要用如下公式确定布隆过滤器真实的失误率为：</p>
<p>$$（1 - e^{- \frac{nk}{m}})^k$$</p>
<p>根据这个公式算出真实的失误率为0.006%，这是比 0.01 % 更低的失误率。</p>
<h3 id="2-只用-2GB-内存在-20-亿个整数中找到出现次数最多的数"><a href="#2-只用-2GB-内存在-20-亿个整数中找到出现次数最多的数" class="headerlink" title="2. 只用 2GB 内存在 20 亿个整数中找到出现次数最多的数"></a>2. 只用 2GB 内存在 20 亿个整数中找到出现次数最多的数</h3><h4 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h4><p>在一个包含 20 亿个全是 32 位整数的大文件，在其中找到出现次数最多的数。</p>
<h4 id="要求-1"><a href="#要求-1" class="headerlink" title="要求"></a>要求</h4><p>内存限制为 2 GB</p>
<h4 id="解答-1"><a href="#解答-1" class="headerlink" title="解答"></a>解答</h4><p>想要在很多整数中找到出现次数最多的数，通常的做法是使用哈希表对出现的每一个数做词频统计，哈希表的 key 是某一个整数，value 是这个数出现的次数。就本题而言，一共有 20 亿个数，哪怕只是一个数出现 20 亿次，用 32 位的整数也可以表示其出现的次数而不会产生溢出，所以哈希表的 key 需要占用 4B，value 也是 4B。那么哈希表的一条记录（key，value）需要占用 8B，当哈希表记录数为 2 亿个时，至少需要 1.6GB 的内存。</p>
<p>但如果 20 亿个数中不同的数超过 2亿种，最极端的情况是 20 亿个数都不相同，那么在哈希表中可能需要产生 20 亿条记录，这样内存明显不够用，所以一次性用哈希表统计 20 亿个数非常冒险。</p>
<p>解决的方法是把包含 20 亿个数的大文件用哈希函数分成 16 个小文件，根据哈希函数的性质，同一种数不可能被哈希到不同的小文件上，同时每个小文件中不同的数一定不会大于 2 亿种，假设哈希函数足够好。然后对每一个小文件用哈希表来统计其中每种数出现的次数，这样我们就得到了 16 个小文件中各自出现次数最多的数，还有各自的次数统计。接下来只要选出 16 个小文件各自的第一名中谁出现的次数最多即可。</p>
<h3 id="3-40-亿个非负整数中找到没出现的数"><a href="#3-40-亿个非负整数中找到没出现的数" class="headerlink" title="3. 40 亿个非负整数中找到没出现的数"></a>3. 40 亿个非负整数中找到没出现的数</h3><h4 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h4><p>32 位无符号整数的范围是 0 ~ 4294967295，现在有一个正好包含 40 亿个无符号整数的文件，所以在整个范围中必然有没出现过的数。可以使用最多 1 GB 的内存，怎么找到所有没出现过的数。</p>
<h4 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h4><p>内存限制为 10 MB</p>
<h4 id="解答-2"><a href="#解答-2" class="headerlink" title="解答"></a>解答</h4><p>如果用哈希表来保存出现过的数，那么如果 40 亿个数都不同，则哈希表的记录数为 40 亿条，存一个 32 位整数需要 4B，所有最差情况下需要 40 亿 * 4B = 160 亿字节，大约需要 16 GB的空间，不符合要求。</p>
<p>哈希表需要占用很多空间，我们可以使用 bit map 的方式来表示数出现的情况。具体的，申请一个长度为 4294967295 的 bit 类型的数组上的每个位置只可以表示 0 或 1 状态。8 个 bit 为 1B，所以长度为 4294967295 的 bit 类型的数组占用 500 MB 空间。</p>
<p>进阶问题：现在只有 10 MB 的内存，但也只要求找到其中一个没出现的数即可，首先，0 ~ 4294967295 这个范围是可以平均分成 64 个区间的，每个区间是 67108864 个数。因为一共只有 40 亿个数，所以，如果统计落在每一个区间上的数有多少，肯定至少一个区间上的计数少于 67108864。利用这一点可以找出其中一个没出现的数。</p>
<p>具体过程如下： 第一次遍历时，先申请长度为 64 的整型数组 countArr[0…63]，countArr[i] 用来统计区间 i 上的数有多少。遍历 40 亿个数，根据当前数是多少来决定哪一个区间上的计数增加。例如，如果当前数是 3422552090，3422552090 / 67108864 = 51，所以第 51 区间上的计数增加，遍历完所有数之和遍历 countArr，必然会有某一个位置上的值小于 67108864，表示第 i 区间上至少有一个数没出现过。我们肯定会至少找到一个这样的区间。此时内存使用为 64 * 4B，是非常小的。</p>
<p>假设我们找到第 37 区间上的计数小于 67108864，以下为第二次遍历的过程：</p>
<ol>
<li>申请长为 67108864 的 bit map，这占用大约 8 MB 的空间。</li>
<li>再遍历一次 40 亿个数，此时的遍历只关注落在第 37 区间上的数。</li>
<li>之后的方法同普通方法类似。</li>
<li>遍历完 40 亿个数之后，在 bitArr 上必然存在没被设置为 1 的位置，假设第 i 个位置上的值没设置为 1，那么 67108864 * 37 + i 这个数就是一个没出现过的数。</li>
</ol>
<h3 id="4-找到-100-亿个-URL-中重复的-URL-以及搜索词汇的-top-K-问题"><a href="#4-找到-100-亿个-URL-中重复的-URL-以及搜索词汇的-top-K-问题" class="headerlink" title="4. 找到 100 亿个 URL 中重复的 URL 以及搜索词汇的 top K 问题"></a>4. 找到 100 亿个 URL 中重复的 URL 以及搜索词汇的 top K 问题</h3><h4 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a>题目</h4><p>有一个包含 100 亿个 URL 的大文件，假设每个 URL 占用 64 B，请找出其中所有重复的 URL。</p>
<h4 id="补充题目"><a href="#补充题目" class="headerlink" title="补充题目"></a>补充题目</h4><p>某搜索公司一天的用户搜索词汇是海量的，请设计一种求出每天最热 100 词汇的可行办法。</p>
<h4 id="解答-3"><a href="#解答-3" class="headerlink" title="解答"></a>解答</h4><p>解决这种问题的常规方法是，把大文件通过哈希函数分配到机器，或者通过哈希函数把大文件拆成小文件。一直进行这种划分，直到划分的结果满足机器资源的限制。</p>
<p>例如，将 100 亿字节的大文件通过哈希函数分配到 100 台机器上，然后每一台机器分别统计分给自己的 URL 中是否有重复的 URL，同时哈希函数的性质决定了同一条 URL 不可能分给不同的机器；或者可以在单机上将大文件通过哈希函数拆成 1000 个小文件，对每一个小文件再利用哈希表遍历，找出重复的 URL；或者在分给机器或拆完文件之后，进行排序，排序过会再看是否会有重复 URL 出现。大数据问题的处理都离不开分流要么是哈希函数把大文件的内容分配给不同的机器，要么是哈希函数把大文件拆成小文件，然后处理每一个小数量的集合。</p>
<p>补充题目最开始还是哈希分流的思路来处理，把包含百亿数据量的词汇文件分流到不同的机器上，具体多少台机器由题目来确定。对每一台机器来说，如果分到的数据量依然很大，可以再用哈希函数把每台机器的分流文件拆成更小的文件处理。处理每一个小文件的时候，哈希表统计每种词及其词频，哈希表记录建立完成后，再遍历哈希表，遍历哈希表的过程中使用大小为 100 的小根堆来选出每一个小文件的 top 100。每一个小文件都有自己的词频的小根堆，将小根堆的词按照词频排序，就得到了每个小文件的排序后 top 100。然后把各个小文件排序后的 top 100 进行外排序或者继续利用小根堆，就可以选出每台机器上的 top 100。不同机器之间的 top 100 再进行外排序或者继续利用小根堆，最终求出整个百亿数据量中的 top 100。对于 top k 的问题，除哈希函数分流和用哈希表做词频统计之外，还经常用堆结构和外排序的手段进行处理。</p>
<h3 id="5-40-亿个非负整数中找到出现两次的数和所有数的中位数"><a href="#5-40-亿个非负整数中找到出现两次的数和所有数的中位数" class="headerlink" title="5. 40 亿个非负整数中找到出现两次的数和所有数的中位数"></a>5. 40 亿个非负整数中找到出现两次的数和所有数的中位数</h3><h4 id="题目-4"><a href="#题目-4" class="headerlink" title="题目"></a>题目</h4><p>40 亿个非负整数中找到出现两次的数和所有数的中位数</p>
<h4 id="补充题目-1"><a href="#补充题目-1" class="headerlink" title="补充题目"></a>补充题目</h4><p>可以使用最多 10 MB 的内存，怎么找到这 40 亿个整数的中位数</p>
<h4 id="解答-4"><a href="#解答-4" class="headerlink" title="解答"></a>解答</h4><p>可以使用 bit map 的方式来表示数出现的情况。申请一个长度为 4294967295 <em> 2 的 bit 类型的数组 bitArr，用 2 个位置表示一个数出现的词频， 1B 占用 8 个 bit，所以长度为 4294967295 </em> 2 的 bit 类型的数组占用 1 GB 空间，</p>
<p>遍历这 40 亿个无符号数，如果再次遇到 num， 就把 bitArr[num<em>2 + 1] 和 bitArr[num </em> 2] 设置为 01，如果第二次遇到 num，则设置为 10，如果第三次遇到 num，就把它设置为 11，以后再遇到 num，发现此时 bitArr[num<em>2 + 1] 和 bitArr[num </em> 2] 已经被设置为 11，就不再做任何设置。遍历完成后，再次遍历 bitArr，如果发现 bitArr[num<em>2 + 1] 和 bitArr[num </em> 2] 设置为 10，那么 i 就是出现了两次的数。</p>
<p>对于补充问题，用分区间的方式处理，长度为 2 MB 的无符号整型数组占用的空间为 8 MB，所以将区间的数量定为 4294967295/2M，向上取整为 2148 个区间。</p>
<p>申请一个长度为 2148 的无符号整型数组 arr[0…2147]，arr[i]表示第 i 区间有多少个数。arr 必然小于 10 MB。然后遍历 40 亿个数，如果遍历到当前数为 num，先看 num 落在哪个区间上（num / 2M)，然后将对应的进行arr[num/2M] ++ 操作，这样遍历下来，就得到了每一个区间的数的出现状况，通过累加每个区间的出现次数，如果遍历到当前数为 num，先看 num 落在哪个区间上（num / 2M），然后将对应的进行 arr[num/2M]++ 操作。这样遍历下来，就得到了每一个区间的数的出现状况，通过累加每个区间的出现次数，就可以找到 40 亿个数的中位数到底落在哪个区间。假设为第 K 区间。</p>
<p>接下来申请一个长度为 2MB 的无符号整型数组 countArr[0..2M-1]，占用空间 8MB。然后再遍历 40 亿个数，此时只关心处在第 K 区间的数记为 numi，其他的数省略，然后将 countArr[numi-K*2M]++，也就是只对第 K 区间的数做频率统计。这次遍历完 40 亿个数之后，就得到了第 K 区间的词频统计结果 countArr，最后只在第 K 区间上找到相应的第几个数字即可。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/07/Spark实践-3-Spark-SQL-与数据仓库/" itemprop="url">
                  Spark实践 (3): Spark SQL 与数据仓库
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-07T10:19:48+08:00" content="2017-02-07">
              2017-02-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/02/07/Spark实践-3-Spark-SQL-与数据仓库/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/07/Spark实践-3-Spark-SQL-与数据仓库/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/02/07/Spark实践-3-Spark-SQL-与数据仓库/" class="leancloud_visitors" data-flag-title="Spark实践 (3): Spark SQL 与数据仓库">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Spark-SQL-基础"><a href="#Spark-SQL-基础" class="headerlink" title="Spark SQL 基础"></a>Spark SQL 基础</h3><p>使用 Spark SQL 有 2 种方式，一种是通过写 SQL 来进行计算，另外一种是在 Spark 程序中，通过领域 API 的形式来操作数据（被抽象为 DataFrame）。</p>
<h4 id="分布式-SQL-引擎"><a href="#分布式-SQL-引擎" class="headerlink" title="分布式 SQL 引擎"></a>分布式 SQL 引擎</h4><p>作为分布式引擎，有两种运行方式，一种是 JDBC/ODBC Server，另一种是使用 Spark SQL 命令行。在正式环境下，使用前者比较好。</p>
<h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>通过写 SQL 来使用 Spark SQL 和 hive 区别不大，这里不再详细介绍，稍微提一下的是，有一些 hive 的特性是 Spark SQL 不支持的，最主要的是 Hive 的 bucker 表，使用散列的方式对 hive 表进行分区。</p>
<p>DataFrame 具有和 RDD 类似的概念，但还增加了列的概念。</p>
<p>在 Spark 中使用 DataFrame 的过程，可以分为 4 步，</p>
<ol>
<li>初始化环境，一般是创建一个 SQLContext 对象。</li>
<li>创建一个 DataFrame，可以来源于 RDD 或其他数据源。</li>
<li>调用 DataFrame 操作，是一种领域特定的 API，可以实现所有的 SQL 功能。</li>
<li>可以直接通过函数执行 SQL 语句。</li>
</ol>
<ul>
<li><p>创建 SQLContext。</p>
<pre><code>val sc: SparkContext
val sqlContext = new SQLContext(sc)
</code></pre></li>
<li><p>创建 DataFrame</p>
</li>
</ul>
<p>环境初始化之后，就可以创建 DataFrame 了，主要有两种创建方式。</p>
<ol>
<li><p>从 RDD 创建，又分 2种：</p>
<ul>
<li><p>使用 Scala 反射。</p>
</li>
<li><p>程序指定，略繁杂，但是可以运行时指定。</p>
</li>
</ul>
</li>
<li><p>从其他数据源创建。</p>
</li>
</ol>
<h4 id="使用反射的方法从-RDD-创建-DataFrame"><a href="#使用反射的方法从-RDD-创建-DataFrame" class="headerlink" title="使用反射的方法从 RDD 创建 DataFrame"></a>使用反射的方法从 RDD 创建 DataFrame</h4><p>这个方法是先定义一个 case class，参数名即为列名，然后将 RDD 的成员转换成 case class 类型，包含 case class 的 RDD 可以通过反射方式被隐式转换成 DataFrame，case class 的参数名会成为表的列名，然后就可以注册成一张表。</p>
<p>这种方法前提是在写程序之前就已经知道了数据格式，可以预先设定表的模式。</p>
<pre><code>def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(&quot;SparkSQLSimpleExample&quot;)
    val sc = new SparkContext()

    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._
    case class Person(name: String, age: Int)
    val rdd = sc.textFile(&quot;path/to/file&quot;).map(_.split(&quot;,&quot;))
    // 包含了 case class 的 RDD
    val rddContainingCaseClass = rdd.map(p =&gt; Person(p(0), p(1).trim.toInt))
    // 被隐式转换成 DataFrame
    val people = rddContainingCaseClass.toDF()
    // 将 DataFrame 的内容打印到标准输出
    people.show()
  }
</code></pre><h4 id="使用程序动态从-RDD-创建-DataFrame"><a href="#使用程序动态从-RDD-创建-DataFrame" class="headerlink" title="使用程序动态从 RDD 创建 DataFrame"></a>使用程序动态从 RDD 创建 DataFrame</h4><p>当 case class 无法提前知道数据格式时，可以在运行时动态指定表模式来从 RDD 创建 DataFrame。具体步骤如下：</p>
<ol>
<li>从原来的 RDD 创建一个新的 RDD，成员是 Row 类型，包含所有列。</li>
<li>创建一个 StructType 类型的表模式，其结构与步骤 1 中创建的 RDD 的 Row 结构相匹配。</li>
<li><p>使用 SQLContext.createDataFrame 方法将表模式应用到步骤 1 创建的 RDD 上。</p>
<pre><code>val sqLContext = new SQLContext(sc)
// 普通的 RDD
val people = sc.textFile(&quot;path/to/file&quot;)
// 字符串格式的表模式
val schemaString = &quot;name age&quot;
// 根据字符串格式的表模式创建结构化的表模式，用 StructType 保存
val schema =
      StructType(
        schemaString.split(&quot; &quot;).map(fieldName =&gt; StructField(fieldName, StringType, true))
      )
// 将普通 RDD 的成员转换成 Row 对象
val rowRDD = people.map(_.split(&quot;,&quot;)).map(p =&gt; Row(p(0), p(1).trim))
// 将模式作用到 RDD 上，生成 DataFrame
val peopleDataFrame = sqLContext.createDataFrame(rowRDD, schema)
peopleDataFrame.show()
</code></pre></li>
</ol>
<h4 id="从其他数据源生成-DataFrame"><a href="#从其他数据源生成-DataFrame" class="headerlink" title="从其他数据源生成 DataFrame"></a>从其他数据源生成 DataFrame</h4><p>Spark 提供了统一的接口，可以很方便地从其他数据源创建 DataFrame，例如：</p>
<pre><code>val df = sqlContext.read.json(&quot;path/to/file.json&quot;)
df.show()
</code></pre><h4 id="DataFrame-基本操作"><a href="#DataFrame-基本操作" class="headerlink" title="DataFrame 基本操作"></a>DataFrame 基本操作</h4><pre><code>// select * from 
df.show()  

// select name from 
df.select(&quot;name&quot;).show()

// select name, age + 1 from
df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show()

// select * from  xxx where age &gt; 21
df.filter(df(&quot;age&quot;) &gt; 21).show()

// select age, count(*) from xxx group by age
df.groupBy(&quot;age&quot;).count().show()

// 使用 registerTempTable 方法将 Dataframe 注册成一张表：
df.registerTempTable(&quot;people&quot;)

// 之后可以使用纯 SQL 来访问
val result = sqlContext.sql(&quot;SELECT * FROM people&quot;)
</code></pre><h4 id="DataFrame-数据源"><a href="#DataFrame-数据源" class="headerlink" title="DataFrame 数据源"></a>DataFrame 数据源</h4><p>DataFrame 支持非常多类型的数据源，包括 Hive、Avro、Parquet、ORC、JSON、JDBC。而 Spark 提供了统一的读写接口。</p>
<p>通过数据源加载数据时，默认的类型是 Parquet，这是一种大数据计算中最常用的列式存储格式：</p>
<pre><code>val df = sqlContext.read.load(&quot;path/to/file.parquet&quot;)
</code></pre><p>对于其他类型，可以使用 format 指定：</p>
<pre><code>val df = 
    sqlContext.read.format(&quot;json&quot;).load(&quot;path/to/file.json&quot;)
</code></pre><p>保存数据时和加加载数据方法类似。</p>
<h3 id="Spark-SQL-原理和运行机制"><a href="#Spark-SQL-原理和运行机制" class="headerlink" title="Spark SQL 原理和运行机制"></a>Spark SQL 原理和运行机制</h3><h4 id="Catalyst-执行优化器"><a href="#Catalyst-执行优化器" class="headerlink" title="Catalyst 执行优化器"></a>Catalyst 执行优化器</h4><p>Catalyst 是 Spark SQL 执行优化器的代号，所有 Spark SQL 语句最终都能通过它来解析、优化，最终生成可以执行的 Java 字节码。</p>
<p>Catalyst 最主要的数据结构是树，所有 SQL 语句都会用树结构来存储，树中的每个节点有一个类（class），以及 0 或多个子节点。Scala 中定义的新的节点类型都是 TreeNode 这个类的子类。</p>
<p>Catalyst 另外一个重要的概念是规则。基本上，所有优化都是基于规则的。可以用规则对树进行操作，树中的节点是只读的，所以树也是只读的。规则中定义的函数可能实现从一棵树转换成一颗新树。</p>
<p>整个 Catalyst 的执行过程可以分为以下 4 个阶段：</p>
<ul>
<li>分析阶段，分析逻辑树，解决引用</li>
<li>逻辑优化阶段</li>
<li>物理计划阶段，Catalyst 会生成多个计划，并基于成本进行对比</li>
<li>代码生成阶段</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/06/Spark实践-2-Spark-内核/" itemprop="url">
                  Spark实践 (2): Spark 内核
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-06T17:43:45+08:00" content="2017-02-06">
              2017-02-06
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/02/06/Spark实践-2-Spark-内核/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/06/Spark实践-2-Spark-内核/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/02/06/Spark实践-2-Spark-内核/" class="leancloud_visitors" data-flag-title="Spark实践 (2): Spark 内核">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Spark-核心数据结构-RDD"><a href="#Spark-核心数据结构-RDD" class="headerlink" title="Spark 核心数据结构 RDD"></a>Spark 核心数据结构 RDD</h3><p>RDD 全称是“弹性分布式数据集”。首先，它是一个数据集；其次，RDD 是分布式存储的。里面的成员被水平切割成小的的数据块，分散在集群的多个节点上，便于对 RDD 里面的数据进行并行计算。最后，RDD 的分布式弹性的，不是固定不变的。RDD 的一些操作可以被拆分成对各数据块直接计算，不涉及其他节点，比如 map。这样的操作一般在数据块所在的节点上直接进行，不影响 RDD 的分布，除非某个节点故障需要转换到其他节点上。但是在有些操作中，例如 groupBy，必须要访问 RDD 的所有数据块。</p>
<p>RDD 还具有的特点是：</p>
<ol>
<li>RDD 是只读的，一旦生成，内容就不能修改了。这样的好处是让整个系统的设计相对简单，比如并行计算时不用考虑数据互斥的问题。</li>
<li>RDD 可指定缓存在内存中。一般计算都是流水式生成、使用 RDD，新的 RDD 生成之后，旧的不再使用，并被 Java 虚拟机回收掉。但如果后续有许多计算依赖某个 RDD，我们可以让这个 RDD 缓存在内存中，避免重复计算（尤其适用于机器学习）。</li>
<li>RDD 可以通过重新计算得到。RDD 的高可靠性不是通过复制来实现的，而是通过记录足够的计算过程。</li>
</ol>
<h4 id="RDD-的定义"><a href="#RDD-的定义" class="headerlink" title="RDD 的定义"></a>RDD 的定义</h4><p>一个 RDD 对象，包含如下的 5 个核心属性。</p>
<ul>
<li>一个分区列表，每个分区里是 RDD 的部分数据（或者称数据块）。</li>
<li>一个依赖列表，存储依赖的其他 RDD。</li>
<li>一个名为 compute 的计算函数，用于计算各 RDD 各分区的值。</li>
<li>分区器（可选），用于键/值类型的 RDD，比如某个 RDD 是按散列来分区。</li>
<li>计算各分区时优先的位置列表（可选），比如从 HDFS 上的文件生成 RDD 时，RDD 分区的位置优先选择数据所在的节点，这样可以避免数据移动带来的开销。</li>
</ul>
<h4 id="RDD-的-Transformation"><a href="#RDD-的-Transformation" class="headerlink" title="RDD 的 Transformation"></a>RDD 的 Transformation</h4><p>RDD 的 Transformation 是指由一个 RDD 生成新 RDD 的过程，比如 flapMap。filter 操作都会返回一个新的 RDD 对象，类型是 MapPartitionsRDD，它是 RDD 子类。</p>
<p>在 Spark 中，RDD 是有依赖关系的，这种依赖关系有两种类型。</p>
<ul>
<li>窄依赖。依赖上级 RDD 的部分分区。</li>
<li>Shuffle 依赖上级 RDD 的所有分区。</li>
</ul>
<p>使用窄依赖时，可以精确知道依赖的上级 RDD 的分区。一般情况下，会选择与自己在同一节点的上级 RDD 分区，这样计算过程都在同一节点进行，没有网络 IO 开销，非常高效，常见的 map、flatMap、filter操作都是这一类。而 Shuffle 依赖则无法精确定位依赖的上级 RDD 的分区，相当于依赖索引分区，计算时涉及所有节点之间的数据传输，开销巨大。所以，以 Shuffle 依赖为分隔，Task 被分成 Stage，方便计算时的管理。</p>
<h4 id="RDD-的-Action"><a href="#RDD-的-Action" class="headerlink" title="RDD 的 Action"></a>RDD 的 Action</h4><p>一次 Action 调用之后，不在生成新的 RDD，结果返回到 Driver 程序。</p>
<h4 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h4><p>Shuffle 概念来源于 Hadoop MapReduce，当对一个 RDD 的某个结果分区进行操作而无法精确知道依赖前一个 RDD 的哪个分区时，依赖关系变成了依赖前一个 RDD 的所有分区。Shuffle 本身是一个非常耗资源的操作，它的结果是一次调度的 Stage 的结果，而一次 Stage 包含许多 Task，缓存下来比较划算。Shuffle 使用的本地磁盘目录由 spark.local.dir 属性项指定。</p>
<h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><p>SparkContext 是 Spark 程序最主要的入口，用于和 Spark 集群连接。所有的 Spark 程序都必须创建 SparkContext。进行流式计算时使用 StreamingContext，进行 SQL 计算时使用 SQLContext，都会创建一个 SparkContext。每个 JVM 只允许启动一个 SparkContext。</p>
<h4 id="SparkConf-配置"><a href="#SparkConf-配置" class="headerlink" title="SparkConf 配置"></a>SparkConf 配置</h4><p>SparkContext 可以无参数配置，也可以自定义配置。SparkContext 在构造的过程中，已经完成了各项服务的启动。最重要的初始化操作之一是启动 Task 调度器和 DAG 调度器。</p>
<p>DAG 调度与 Task 调度的区别是，DAG 是高层级的调度，为每个 Job 绘制一个有向无环图，跟踪各 Stage 的输出。计算完成 Job 的最短路径，并将 Task 提交给 Task 调度器执行，而 Task 调度器只负责接收 DAG 调度器的请求，负责 Task 的实际调度执行，所以 DAGScheduler 的初始化必须在 Task 调度器之后。</p>
<p>DAG 与 Task 这种分离设计的好处是，Spark 可以灵活设计自己的 DAG 调度，同时还能与其他资源调度系统结合，比如 YARN、Mesos。</p>
<h3 id="DAG-调度"><a href="#DAG-调度" class="headerlink" title="DAG 调度"></a>DAG 调度</h3><p>SparkContext 在初始化时，创建了 DAG 调度与 Task 调度来负责 RDD Action 操作的调度执行。</p>
<h4 id="DAGScheduler"><a href="#DAGScheduler" class="headerlink" title="DAGScheduler"></a>DAGScheduler</h4><p>DAGScheduler 负责 Spark 的最高级别的任务调度，调度的粒度是 Stage，它为每个 Job 的所有 Stage 计算一个有向无环图，控制它们的并发，并找到一个最佳路径来执行它们。具体的执行过程是将 Stage 下的 Task 集提交给 TaskScheduler 对象，由它来提交到集群上去申请资源并最终完成执行。</p>
<h4 id="TaskScheduler"><a href="#TaskScheduler" class="headerlink" title="TaskScheduler"></a>TaskScheduler</h4><p>相比 DAGScheduler 而言，TaskScheduler 是低级别的调度接口，允许实现不同的 Task 调度器，除了自带的之外，还可以使用 Yarn 和 Mesos 调度器。每个 TaskScheduler 对象只服务于一个 SparkContext 的 Task 调度。TaskScheduler 从 DAGScheduler 的每个 Stage 接收一组 Task，并负责将它们发送到集群上，运行它们，如果出错还会重试，最后返回消息给 DAGScheduler。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="PengShuang" />
          <p class="site-author-name" itemprop="name">PengShuang</p>
          <p class="site-description motion-element" itemprop="description">在路上，慢慢走！</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">66</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/pengshuang" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2176899852/profile?rightmod=1&wvr=6&mod=personnumber&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://lingyu.wang/" title="天镶的博客" target="_blank">天镶的博客</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://coolshell.cn/" title="酷壳" target="_blank">酷壳</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.dongwm.com" title="小明明的博客" target="_blank">小明明的博客</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PengShuang</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"pengshuang"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  






  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("DKbLgBme7UkAx9JX6sM3D4Hj-gzGzoHsz", "GXjJ9Ox3pUGI9PJhm6CNfJGN");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
