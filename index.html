<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="在路上，慢慢走！">
<meta property="og:type" content="website">
<meta property="og:title" content="小沙文的博客">
<meta property="og:url" content="http://pengshuang.space/index.html">
<meta property="og:site_name" content="小沙文的博客">
<meta property="og:description" content="在路上，慢慢走！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小沙文的博客">
<meta name="twitter:description" content="在路上，慢慢走！">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://pengshuang.space/"/>

  <title> 小沙文的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小沙文的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/23/Parameter-Server-学习/" itemprop="url">
                  Parameter Server 学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-07-23T16:36:58+08:00" content="2017-07-23">
              2017-07-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/系统架构/" itemprop="url" rel="index">
                    <span itemprop="name">系统架构</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/07/23/Parameter-Server-学习/" class="leancloud_visitors" data-flag-title="Parameter Server 学习">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>大数据机器学习系统，通常数据在 1 TB 到 1 PB 之间，参数在 10 的 9 次方和 10 的 12 次方左右，很多算法的参数只能采用分布式存储。这样的话会带来下面几个问题：</p>
<ul>
<li>访问这些参数需要很大的网络带宽。</li>
<li>很多算法是序列性的，同步会影响性能。</li>
<li>在大规模分布式的情况下，如何设计容错机制至关重要。</li>
</ul>
<p>为了解决这些问题，大神们提出了一种新的架构 - Parameter Server (简称 PS)</p>
<p>PS 整体架构图如下</p>
<p><img src="/img/parameter-server.png" alt=""></p>
<h3 id="PS-架构分为两个部分："><a href="#PS-架构分为两个部分：" class="headerlink" title="PS 架构分为两个部分："></a>PS 架构分为两个部分：</h3><p><strong>外功</strong>：把计算资源分为两个部分，参数服务器节点和工作节点。</p>
<ol>
<li>参数服务器来存储参数</li>
<li>工作节点来做算法的训练</li>
</ol>
<p><strong>内功</strong>：内功是对应的，把机器学习分为两个部分，参数部分和训练部分。</p>
<ol>
<li>参数部分即模型部分，有一致性的要求，参数服务器也可以是一个集群。</li>
<li>训练部分需要并行化。因为参数服务器的存在，每个计算节点在拿到新的一批batch数据之后，都要从参数服务器上取下最新的参数，然后计算梯度，再将梯度更新会参数服务器。</li>
</ol>
<p>在 PS 中，每个 server 实际上都只负责分到的部分参数（servers共同维持一个全局的共享参数），而每个 work 也只分到部分数据和处理任务；每个子节点都只维护自己分配到的参数，自己部分更新之后，将计算结果（例如：梯度）传回到主节点，进行全局的更新（比如平均操作之类的），主节点再向子节点传送新的参数；</p>
<p>server 节点可以跟其他 server 节点通信，每个 server 负责自己分到的参数，server group 共同维持所有参数的更新。</p>
<p>server manager node 负责维护一些元数据的一致性，比如各个节点的状态，参数的分配情况等；</p>
<p>worker 节点之间没有通信，只跟自己对应的 server 进行通信。每个 worker group 有一个 task scheduler，负责向 worker 分配任务，并且监控 worker 的运行情况。当有新的 worker 加入或者退出，task scheduler 负责重新分配任务。</p>
<h3 id="PS-架构有-5-个特点"><a href="#PS-架构有-5-个特点" class="headerlink" title="PS 架构有 5 个特点"></a>PS 架构有 5 个特点</h3><ol>
<li><p>高效的通信</p>
<p> 异步通信，使得计算不会被拖累</p>
</li>
<li><p>弹性一致性</p>
<p> 允许用户自定义一致性: 比如 Sequential（序列式的，即完全同步）、Eventual（完全不同步的）和 Bounded Delay（有条件的限制，可以允许用户在限制的次数内异步，比如限制为 3 次，如果某个节点已经超前了其他节点四次迭代了，那么要停下等待同步。在整个训练的过程中，Delay 可能是动态的，即 delay 的参数在训练过程中可以变大或变小）</p>
</li>
<li><p>扩展性强</p>
<p> 使用了一个分布式 hash 表使得新的 server 节点可以随时动态的插入到集合中；因此，新增一个节点不需要重新运行系统。</p>
</li>
<li><p>错误容忍</p>
<p> 在大规模商用服务器集群中。从非灾难性机器故障中恢复，只需要 1 秒，而且不需要中断计算。Vector Clocks 保证了经历故障之后还是能运行良好；</p>
</li>
<li><p>易用性</p>
<p> 全局共享的参数可以被表示成各种形式：vector，matrices 或者相应的 sparse 类型，这大大方便了机器学习算法的开发。并且提供的线性代数的数据类型都具有高性能的多线程库。</p>
</li>
</ol>
<h3 id="PS-的一些关键概念"><a href="#PS-的一些关键概念" class="headerlink" title="PS 的一些关键概念"></a>PS 的一些关键概念</h3><h4 id="1-key-value-，Range-Push-and-Pull"><a href="#1-key-value-，Range-Push-and-Pull" class="headerlink" title="1. (key, value)，Range Push and Pull"></a>1. (key, value)，Range Push and Pull</h4><p><img src="/img/parameter-server4.png" alt=""></p>
<p>parameter server 中，参数都是可以被表示成(key, value)的集合，比如一个最小化损失函数的问题，key 就是 feature ID，而 value 就是它的权值。对于稀疏参数，不存在的key，就可以认为是0。</p>
<p>把参数表示成 k-v， 形式更自然，易于理解，更易于编程解；</p>
<p>workers 跟 servers 之间通过 push 跟 pull 来通信。worker 通过 push 将计算好的梯度发送到 server，然后通过 pull 从 server 更新参数。为了提高计算性能和带宽效率，parameter server 允许用户使用 Range Push 跟 Range Pull 操作（使用区间更新的方式）。</p>
<h4 id="2-Key-value-vectors"><a href="#2-Key-value-vectors" class="headerlink" title="2. Key-value vectors"></a>2. Key-value vectors</h4><p>赋予每个 key 所对应的 value 一个向量概念或矩阵概念。</p>
<h4 id="3-User-Defined-Functions-on-the-Server"><a href="#3-User-Defined-Functions-on-the-Server" class="headerlink" title="3. User-Defined Functions on the Server"></a>3. User-Defined Functions on the Server</h4><p>服务器端更新参数的时候还有计算正则项，这样的操作可以由用户自定义。</p>
<h4 id="4-Asychronous-Tasks-and-Dependency"><a href="#4-Asychronous-Tasks-and-Dependency" class="headerlink" title="4. Asychronous Tasks and Dependency"></a>4. Asychronous Tasks and Dependency</h4><p><img src="/img/parameter-server3.png" alt=""></p>
<p>如图，如果 iter1 需要在 iter0 computation，push 跟 pull 都完成后才能开始，那么就是 Synchronous，反之就是Asynchronous.</p>
<p>参数服务器和工作节点之间的通信都属于远程调用。远程调用相对而言要比较耗时，因而 PS 框架让远程调用成为异步调用，比如参数的 push 和 pull 发出之后，立即使用当前值开始进行下一步的梯度计算。（失去了模型的一致性，但提升了速度）。</p>
<p>Asychronous 的优点是能够提高系统的效率（因为节省了很多等待的过程），但是，它的缺点就是容易降低算法的收敛速率；</p>
<p>所以，系统性能跟算法收敛速率之间是存在一个 trade-off 的，你需要同时考虑：</p>
<ol>
<li><p>算法对于参数非一致性的敏感度；</p>
</li>
<li><p>训练数据特征之间的关联度；</p>
</li>
<li><p>硬盘的存储容量</p>
</li>
</ol>
<h4 id="6-User-Defined-Filters（用户自定义过滤）"><a href="#6-User-Defined-Filters（用户自定义过滤）" class="headerlink" title="6. User-Defined Filters（用户自定义过滤）"></a>6. User-Defined Filters（用户自定义过滤）</h4><p>在工作节点这一端对梯度进行过滤，如果梯度并不是影响那么大，就不占用网络去更新，等积累一段时间之后再去做更新。</p>
<p>对于机器学习优化问题比如梯度下降来说，并不是每次计算的梯度对于最终优化都是有价值的，用户可以通过自定义的规则过滤一些不必要的传送，再进一步压缩带宽 cost：</p>
<ol>
<li>发送很小的梯度值是低效的：</li>
</ol>
<p>因此可以自定义设置，只在梯度值较大的时候发送；</p>
<ol>
<li>更新接近最优情况的值是低效的：</li>
</ol>
<p>因此，只在非最优的情况下发送，可通过KKT来判断；</p>
<h3 id="PS-的实现"><a href="#PS-的实现" class="headerlink" title="PS 的实现"></a>PS 的实现</h3><p><strong>Vector Clock</strong></p>
<p>为参数服务器中的每个参数加一个时间戳来跟踪参数的更新，防止重复发送数据。如果每个参数都有一个时间戳，那么参数众多，时间戳也众多。但借助于Vector概念，很多的参数可以作为向量存在k-v中，因而，时间戳的数量大大减少。</p>
<p>在刚开始的时候，所有的参数都是一个大向量，时间戳为0，每次来一个范围的更新，如果能找到对应的key，那么直接更新那个key的时间戳就可以了。否则，就可能会对某些向量进行切分，一次更新请求，最多能把一个区间切分为三个区间。</p>
<p><strong>一致性哈希</strong></p>
<p>参数服务器集群中每个节点都负责不同区域的参数，那么，类似于hash table，使用hash ring进行实现，key和server id都插入到hash ring上去。</p>
<p><strong>备份和一致性</strong></p>
<p>使用类似 hadoop 的 chain 备份方式，对于一个 master 节点，如果有更新，先更新它，然后再去更新备份的服务器。</p>
<p><img src="/img/parameter-server2.png" alt=""></p>
<p>在更新的时候，由于机器学习算法的特点，可以将多次梯度聚合之后再去更新备份服务器，从而减少带宽。</p>
<p><strong>Messages</strong></p>
<p>一条 message 包括：时间戳，len(range) 对 k-v。</p>
<p>这是 parameter server 中最基本的通信格式，不仅仅是共享的参数才有，task 的message 也是这样的格式，只要把这里的 (key, value) 改成 (task ID, 参数/返回值)。</p>
<p>由于机器学习问题通常都需要很高的网络带宽，因此信息的压缩是必须的。</p>
<p>key 的压缩：因为训练数据通常在分配之后都不会发生改变，因此worker没有必要每次都发送相同的key，只需要接收方在第一次接收的时候缓存起来就行了。第二次，worker不再需要同时发送key和value，只需要发送value 和 key list的hash就行。这样瞬间减少了一般的通信量。</p>
<p>value的压缩： 假设参数时稀疏的，那么就会有大量的0存在。因此，为了进一步压缩，我们只需要发送非0值。parameter server使用 Snappy 快速压缩库来压缩数据、高效去除 0 值。</p>
<pre><code>key 的压缩和 value 的压缩可以同时进行。
</code></pre><p><strong>Server Management</strong></p>
<p>由于 key 的 range 特性，当参数服务器集群中增加一个节点时，步骤如下：</p>
<ul>
<li>server manager 节点给新节点分配一个 key range，这可能会导致其他节点上的 key range 切分。</li>
<li>新节点从其他节点上将属于它的 key range 数据取过来，然后也将 slave 信息取过来。</li>
<li>server manager广播节点变动，其他节点得知消息后将不属于自己 key range 的数据删掉</li>
</ul>
<p>在第二步，从其他节点上取数据的时候，其他节点上的操作也分为两步，第一是拷贝数据，这可能也会导致 key range 的切分。第二是不再接受和这些数据有关的消息，而是进行转发，转发到新节点。</p>
<p>在第三步，收到广播信息后，节点会删除对应区间的数据，然后，扫描所有的和R有关发送出去的还没收到回复的消息，当这些消息回复时，转发到新节点。</p>
<p>节点的离开与节点的加入类似。</p>
<p><strong>Worker Management</strong></p>
<p>添加工作节点比添加服务器节点要简单一些，步骤如下：</p>
<ul>
<li>task scheduler 给新节点分配一些数据</li>
<li>节点从网络文件系统中载入数据，然后从服务器端拉取参数</li>
<li>task scheduler 广播变化，其他节点 free 掉一些训练数据</li>
</ul>
<p>当一个节点离开的时候，task scheduler 可能会寻找一个替代，但恢复节点是十分耗时的工作，同时，损失一些数据对最后的结果可能影响并不是很大。所以，系统会让用户进行选择，是恢复节点还是不做处理。这种机制甚至可以允许用户删掉跑的最慢的节点来提升速度。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/15/XGBoost-介绍/" itemprop="url">
                  XGBoost 介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-15T19:28:38+08:00" content="2017-03-15">
              2017-03-15
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/15/XGBoost-介绍/" class="leancloud_visitors" data-flag-title="XGBoost 介绍">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>我之前的博客提到过，Boosting 是一种将弱分类器转化为强分类器的方法，它的函数模型具有叠加性。</p>
<p>XGBoost 的目标函数由损失函数和复杂度组成。复杂度又由叶子数量和 L2 正则组成。</p>
<p>$$L(\phi) = \sum_i l(y^{*}_i, y_i) + \sum_k \Omega(f_k) $$</p>
<p>$$where \,\, \Omega(f) = \gamma T + \frac{1}{2}\lambda||w||^2$$</p>
<p>其中 i 是样本 id，k 是树 id（轮数），由于 loss 函数和复杂度项都是凸函数，所以有最小值。w 是与真实值的残差，将 w 的 L2 正则项加在目标函数中，可以有效的防止过拟合。叶子节点的数目也作为正则项加在了目标函数中，一定程度上限制了叶子数量，防止过拟合。而传统 GBDT 防止过拟合的手段是预剪枝或者后剪枝。</p>
<h3 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h3><p>对于每次迭代过程，可以将一颗树的训练目标函数写成形式如下：</p>
<p>$$L^{(t)} = \sum_{i=1}^n l(y_i, y_i^{*(t-1)} + f_t(x_i)) + \Omega(f_t)$$</p>
<p>输入是 \(t-1\) 轮后预测的值和真实值，用来拟合残差 \(f(x)\)。对于这个式子，因为无法对 \(f(x)\) 进行有效的最优估计，所以要进行如下推导：</p>
<p>首先将目标函数泰勒二阶展开：</p>
<p>$$L^{(t)} = \sum_{i=1}^n [ l(y_i, y_i^{*(t-1)}) + g_i f_i(x_i) + \frac{1}{2} h_if^2_t(x_i)] + \Omega(f_t)$$</p>
<p>去掉常数项，</p>
<p>$$L^{(t)} = \sum_{i=1}^n [g_i f_i(x_i) + \frac{1}{2} h_if^2_t(x_i)] + \Omega(f_t)$$</p>
<p>正则项展开，</p>
<p>$$L^{(t)} = \sum_{i=1}^n [g_i f_i(x_i) + \frac{1}{2} h_if^2_t(x_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2$$</p>
<p>首先做一下转换，</p>
<p>$$w_j = f(x_i) \, \, i \in _jI$$</p>
<p>所以，公式可以转化为：</p>
<p>$$L^{(t)} = \sum_{j=1}^T [(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda) w_j^2] + \gamma T$$</p>
<p>这是一个二次项形式，所以最后使得目标函数最小的 w 为，</p>
<p>$$w^*_j = - \frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$$</p>
<p>带入原方程最小值为：</p>
<p>$$L^{t}(q) = - \frac{1}{2} \sum_{j=1}^T \frac{(\sum_{i \in I_j} g_i)^2}{\sum_{i \in I_j} h_i + \lambda} + \gamma T$$</p>
<p>最后求得的w就是目标函数在一个样本集合条件下的最优解。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>为什么要用泰勒二阶近似展开。</strong></p>
<p>由于 gbdt 只用到了一阶信息，相当于 loss 函数只进行了一阶泰勒展开。在没有复杂度项的情况下，无法确定步长，所以只能用常数步长根据一阶梯度方向去逼近。这就是牛顿下降法和梯度下降法的区别。由于二阶展开用二次函数去逼近函数，所以可以利用二阶信息确定更新步长，比只利用一阶信息的 gdbt 用更少的迭代获得更好的效果。</p>
<p><strong>为何要对目标函数进行推导。</strong></p>
<ol>
<li>为了适应各种损失函数。</li>
<li>正则项的加入对参数估计产生了影响。</li>
<li>如果只考虑平方损失的条件下，在没有正则项的情况下参数的最优估计为样本均值。在没有指定损失函数情况下，我们也很容易想到均值是给定样本条件下的误差最小的最优估计。但是损失函数换成绝对值损失，那么最优估计就为中位数。可见不同损失函数下，结果并不想我们想的那么简单。</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/13/线性回归家族一览/" itemprop="url">
                  线性回归家族一览
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-13T20:59:23+08:00" content="2017-03-13">
              2017-03-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/13/线性回归家族一览/" class="leancloud_visitors" data-flag-title="线性回归家族一览">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>线性回归的目的是要得到输出向量 Y 和输入特征 X 之间的线性关系，求出线性回归系数 \(\theta\)，也就是 \(Y = X\theta\)。其中 Y 的维度是 m <em> 1，X 的维度是 m </em> n，而 \(\theta\) 的维度为 n * 1。m 代表样本个数，n 代表样本特征的维度。</p>
<p>为了得到线性回归系数 \(\theta\)，我们需要定义一个损失函数，一个极小化损失函数的优化方法，以及一个验证算法的方法。损失函数的不同，损失函数的优化方法的不同，验证方法的不同，就形成了不同的线性回归算法。</p>
<h3 id="LinearRegression"><a href="#LinearRegression" class="headerlink" title="LinearRegression"></a>LinearRegression</h3><p>损失函数：</p>
<p>LinearRegression类就是我们平时说的最常见普通的线性回归，它的损失函数也是最简单的，如下：</p>
<p>$$J(\theta) =  \frac{1}{2}(X\theta - Y)^{T}(X\theta - Y)$$</p>
<p>损失函数的优化方法：</p>
<p>对于这个函数，一般有梯度下降法和最小二乘法两种极小化损失函数的优化方法。通过最小二乘法，可以解出线性回归系数 \(\theta\) 为：</p>
<p>$$\theta = (X^TX)^{-1}X^{T}Y$$</p>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p>一般来说，只要我们觉得数据有线性关系，LinearRegression 是我们的首先应该采用的。如果发现拟合或者预测的不好，再考虑用其他的线性回归库。如果是学习线性回归，推荐先从这个类开始第一步的研究。</p>
<h3 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h3><p>由于单纯的线性回归没有考虑过拟合的问题，有可能泛化能力较差，这时损失函数可以加入正则化项，如果加入的是 L2 范数的正则化项，就是 Ridge 回归。此时，损失函数如下：</p>
<p>$$J(\theta) =  \frac{1}{2}(X\theta - Y)^{T}(X\theta - Y) + \frac{1}{2}\alpha ||\theta||^2_2$$</p>
<p>其中 \(\alpha\) 为常数系数，需要进行调优。</p>
<p>Ridge 回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，不至于过拟合。</p>
<h4 id="损失函数的优化方法"><a href="#损失函数的优化方法" class="headerlink" title="损失函数的优化方法"></a>损失函数的优化方法</h4><p>一般也用最小二乘法或者梯度下降法来优化。通过最小二乘法，可以解得系数：</p>
<p>$$\theta = (X^TX + \alpha E)^{-1}X^TY$$</p>
<h4 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h4><p>一般来说，只要我们觉得数据有线性关系，用 LinearRegression 拟合的不是特别好，需要正则化，可以考虑用 Ridge Regression。但是这个类最大的缺点是每次我们要自己指定一个超参数 \(\alpha\)，然后自己评估 \(\alpha\) 的好坏，比较麻烦。</p>
<h3 id="RidgeCV"><a href="#RidgeCV" class="headerlink" title="RidgeCV"></a>RidgeCV</h3><p>RidgeCV 的损失函数和损失函数的优化方法完全与 Ridge 类相同，区别在于验证方法。</p>
<p>RidgeCV 对超参数 \(\alpha\) 使用了交叉验证，来帮忙我们选择一个合适的 \(\alpha\)。在初始化 RidgeCV 时候，我们可以传一组备选的 \(\alpha\) 值，10个，100个都可以。RidgeCV类会帮我们选择一个合适的αα。免去了我们自己去一轮轮筛选αα的苦恼。　　</p>
<h4 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h4><p>一般来说，只要我们觉得数据有线性关系，用 LinearRegression 拟合的不是特别好，需要正则化，可以考虑用 RidgeCV。如果输入特征的维度很高，而且是稀疏线性关系的话，RidgeCV类就不合适了。这时应该主要考虑下面要讲到的 Lasso 回归类家族。</p>
<h3 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h3><p>线性回归的 L1 正则化通常称为 Lasso 回归，它和 Ridge 回归的区别是在损失函数上增加了的是 L1 正则化的项，而不是 L2 正则化项。L1 正则化的项也有一个常数系数 \(\alpha\) 来调节损失函数的均方差项和正则化项的权重，具体 Lasso 回归的损失函数表达式如下：</p>
<p>$$J(\theta) = \frac{1}{2m}(X\theta - Y)^{T}(X\theta - Y) + \alpha||\theta||_1$$</p>
<p>Lasso 回归可以使得一些特征的系数变小，甚至还是一些绝对值比较小的系数直接变为 0。增强模型的泛化能力。</p>
<h3 id="ElasticNet"><a href="#ElasticNet" class="headerlink" title="ElasticNet"></a>ElasticNet</h3><p>ElasticNet 可以看做 Lasso 和 Ridge 的结合。它也是对普通的线性回归做了正则化，但是它的损失函数记不全是 L1 的正则化，也不全是 L2 的正则化，而是一个权重 \(\rho\) 来平衡 L1 和 L2 正则化的比重，形成了一个全新的损失函数如下：</p>
<p>$$J(\theta) = \frac{1}{2m}(X\theta - Y)^{T}(X\theta - Y) + \alpha \rho||\theta||_1 + \frac{\alpha(1 - \rho)}{2}||\theta||^2_2$$</p>
<h4 id="使用场景-2"><a href="#使用场景-2" class="headerlink" title="使用场景"></a>使用场景</h4><p>ElasticNet 用在我们发现用 Lasso 回归太过（太多特征被稀疏为0），而用Ridge回归又正则化的不够（回归系数衰减的太慢）的时候。一般不推荐拿到数据就直接就 ElasticNet。</p>
<h3 id="BayesianRidge"><a href="#BayesianRidge" class="headerlink" title="BayesianRidge"></a>BayesianRidge</h3><p>贝叶斯回归模型假设先验概率，似然函数和后验概率都是正态分布。先验概率是假设模型输出 Y 是符合均值为 \(X\theta\) 的正态分布，正则化参数 \(X\alpha\) 被看作是一个需要从数据中估计得到的随机变量。回归系数 \(\theta\) 的先验分布规律为球形正态分布，超参数为 λ。我们需要通过最大化边际似然函数来估计超参数 α 和 λ，以及回归系数 \(\theta\)。</p>
<h4 id="使用场景：-1"><a href="#使用场景：-1" class="headerlink" title="使用场景："></a>使用场景：</h4><p>如果我们的数据有很多缺失或者矛盾的病态数据，可以考虑 BayesianRidge，它对病态数据鲁棒性很高，也不用交叉验证选择超参数。但是极大化似然函数的推断过程比较耗时，一般情况不推荐使用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/13/什么是单例模式/" itemprop="url">
                  (转载) 什么是单例模式?
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-13T09:16:00+08:00" content="2017-03-13">
              2017-03-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/设计模式/" itemprop="url" rel="index">
                    <span itemprop="name">设计模式</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/13/什么是单例模式/" class="leancloud_visitors" data-flag-title="(转载) 什么是单例模式?">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近找实习的时候，经常被问到什么是单例模式，能不能手写一个单例模式。。。虽然我面试的都是机器学习岗。所以，痛定思痛，今天来整理总结一下什么是单例模式，这里主要是整理一下耗子叔曾经写过的一篇文章 —- <a href="http://coolshell.cn/articles/265.html" target="_blank" rel="external">深入浅出单实例SINGLETON设计模式
</a>。</p>
<p>单例模式的目的是想在整个系统中只能出现一个类的实例。</p>
<h3 id="普通的-Singleton-版本"><a href="#普通的-Singleton-版本" class="headerlink" title="普通的 Singleton 版本"></a>普通的 Singleton 版本</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> Singleton singleton = <span class="keyword">null</span>;</span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Single <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (singleton == <span class="keyword">null</span>) &#123;</span><br><span class="line">			singleton = <span class="keyword">new</span> Singleton();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> singleton;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Singleton 的几个特点：</p>
<ol>
<li>私有（private）的构造函数，表明这个类是不可能形成实例了。这主要是怕这个类会有多个实例。</li>
<li>即然这个类是不可能形成实例，那么，我们需要一个静态的方式让其形成实例：getInstance()。注意这个方法是在new自己，因为其可以访问私有的构造函数，所以他是可以保证实例被创建出来的。</li>
<li>在 getInstance()中，先做判断是否已形成实例，如果已形成则直接返回，否则创建实例。</li>
<li>所形成的实例保存在自己类中的私有成员中。</li>
<li>我们取实例时，只需要使用 Singleton.getInstance() 就行了。</li>
</ol>
<h3 id="改进下的-Singleton"><a href="#改进下的-Singleton" class="headerlink" title="改进下的 Singleton"></a>改进下的 Singleton</h3><p>上面这个例子因为是全局性的实例，所以，在多线程情况下，所有的全局共享的东西都会变得非常危险，如果在多线程情况下同时调用 gerInstance() 的话，那么，可能会有多个进程同时通过 singleton == null 的条件检查，于是多个实例就创建出来，并且可能造成内存泄露问题。所以改进版本如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> Singleton singleton = <span class="keyword">null</span>;</span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Single <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (singleton == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="keyword">synchronized</span> (Singleton.class) &#123;</span><br><span class="line">				<span class="keyword">if</span> (singleton == <span class="keyword">null</span>) &#123;</span><br><span class="line">					singleton = <span class="keyword">new</span> Singleton();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> singleton;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>第一个条件是说，如果实例创建了，那就不需要同步了，直接返回就好了。</li>
<li>不然，我们就开始同步线程。</li>
<li>第二个条件是说，如果被同步的线程中，有一个线程创建了对象，那么别的线程就不用再创建了。</li>
</ol>
<p>但是， single = new Singleton() 这句，并非是一个原子操作。事实上，在 JVM 中，这句话大概做了下面 3 件事情。</p>
<ol>
<li>给 singleton 分配内存</li>
<li>调用 Singleton 的构造函数来初始化成员变量，形成实例</li>
<li>将 singleton 对象指向分配的内存空间</li>
</ol>
<p>但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后便会报错。</p>
<p>因此，修改的方法是把 singleton 声明成 volatile 就可以了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">static</span> Singleton singleton = <span class="keyword">null</span>;</span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Single <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (singleton == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="keyword">synchronized</span> (Singleton.class) &#123;</span><br><span class="line">				<span class="keyword">if</span> (singleton == <span class="keyword">null</span>) &#123;</span><br><span class="line">					singleton = <span class="keyword">new</span> Singleton();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> singleton;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用 volatile 有两个功用：</p>
<ol>
<li>这个变量不会在多个线程中存在复本，直接从内存读取。</li>
<li>这个关键字会禁止指令重排序优化。也就是说，在 volatile 变量的赋值操作后面会有一个内存屏障（生成的汇编代码上），读操作不会被重排序到内存屏障之前。</li>
</ol>
<h3 id="Singleton-的简化版本"><a href="#Singleton-的简化版本" class="headerlink" title="Singleton 的简化版本"></a>Singleton 的简化版本</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">static</span> Singleton singleton = <span class="keyword">new</span> Singleton();</span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> singleton;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在第一次加载类到内存中时就会初始化，所以创建实例本身是线程安全的。</p>
<p>但是，这种方法的最大问题是，当这个类被加载的时候，new Singleton() 这句话就会被执行，就算是 getInstance() 没有被调用，类也被初始化了。</p>
<p>于是，这个可能会与我们想要的行为不一样，比如，我的类的构造函数中，有一些事可能需要依赖于别的类干的一些事（比如某个配置文件，或是某个被其它类创建的资源），我们希望他能在我第一次getInstance()时才被真正的创建。这样，我们可以控制真正的类创建的时刻，而不是把类的创建委托给了类装载器。</p>
<p>于是，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonHolder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton INSTANCE = <span class="keyword">new</span> Singleton();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span> <span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SingletonHolder.INSTANCE;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>上面这种方式，仍然使用 JVM 本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它只有在 getInstance() 被调用时才会真正创建；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/07/Adaboost-算法总结/" itemprop="url">
                  Adaboost 算法总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-07T20:01:42+08:00" content="2017-03-07">
              2017-03-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/07/Adaboost-算法总结/" class="leancloud_visitors" data-flag-title="Adaboost 算法总结">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>Adaboost 是 boosting 算法的其中之一。它既可以作为分类，也可以作为回归。一般我们都是提它怎么用于分类，对它应用在回归比较少提到，其实它也是可以用于回归的。今天我不想写太多 Adaboost 的原理，主要是写一下 Adaboost 如何用于分类和回归问题。</p>
<h3 id="Adaboost-二元分类问题的算法流程"><a href="#Adaboost-二元分类问题的算法流程" class="headerlink" title="Adaboost 二元分类问题的算法流程"></a>Adaboost 二元分类问题的算法流程</h3><p>输入为样本集 \( (x_1, y_1) ,…, (x_n, y_n) \)，输出为 {+1，-1}。</p>
<p>弱分类器迭代次数为 K。输出为最终的强分类器 \(f(x)\)</p>
<p>1) 初始化样本集权重为：</p>
<p>$$D(1) = (w_{11}, w_{12},…, w_{1m});$$</p>
<p>$$w_{1i} = \frac{1}{m}; i = 1,2,…,m$$</p>
<p>2）对于 \(k = 1, 2, 3 … K\):</p>
<p>a) 使用具有权重 \(D_k\) 的样本集来训练数据，得到弱分类器 \(G_{k}(x)\)</p>
<p>b) 计算 \(G_{k}(x)\) 的分类误差率</p>
<p>$$e_{k} = P(G_k(x_i) \neq y_i); i = 1,2,…,m$$</p>
<p>c) 计算弱分类器的系数</p>
<p>$$\alpha_k = \frac{1}{2} log \frac{1 - e_k}{e_k}$$</p>
<p>d) 更新样本集的权重分布</p>
<p>$$w_{k+1,i} = \frac{w_{ki}}{Z_k} exp(- \alpha_k y_i G(x_i))$$</p>
<p>这里 \(Z_k\) 是规范因子：</p>
<p>$$Z_k = \sum_{i=1}^m w_{ki} exp(-\alpha_k y_i G_k(x_i)) \,\,  i = 1,2,…,m$$</p>
<p>3) 构建最终的分类器为：</p>
<p>$$f(x) = sign \sum_{k=1}^K \alpha_k G_k(K)$$</p>
<p>对于 Adaboost 多元分类算法，其实原理和二分类很类似，最主要的区别是在弱分类器的系数上，比如 Adaboost SAMME 算法，它的弱分类器的系数：</p>
<p>$$ \alpha_k = \frac{1}{2} log \frac{1 - e_k}{e_k} + log(R - 1)$$</p>
<p>其中 R 为类别数，从上式可以看出，如果是二元分类，R = 2，则上式和我们的二元分类算法中的弱分类器的系数一致。</p>
<h3 id="Adaboost-回归问题的算法流程"><a href="#Adaboost-回归问题的算法流程" class="headerlink" title="Adaboost 回归问题的算法流程"></a>Adaboost 回归问题的算法流程</h3><p>AdaBoost回归算法变种很多，下面的算法为Adaboost R2 回归算法过程。</p>
<p>输入为样本集 \( (x_1, y_1) ,…, (x_n, y_n) \)，</p>
<p>最终输出为强学习器 \(f(x)\)</p>
<p>1) 初始化样本集权重为：</p>
<p>$$D(1) = (w_{11}, w_{12},…, w_{1m});$$</p>
<p>$$w_{1i} = \frac{1}{m}; i = 1,2,…,m$$</p>
<p>2）对于 \(k = 1, 2, 3 … K\):</p>
<p>a) 使用具有权重 \(D_k\) 的样本集来训练数据，得到弱分类器 \(G_{k}(x)\)</p>
<p>b) 计算训练集上的最大误差</p>
<p>$$E_k = max|y_i - G_k(x_i)|; i = 1, 2, 3 … m$$</p>
<p>c) 计算每个样本的相对误差：</p>
<p>如果是线性误差，则 \(e_{ki} = \frac{|y_i - G_k(x_i)|}{E_k}\)</p>
<p>如果是平方误差，则 \(e_{ki} = \frac{(y_i - G_k(x_i))^2}{E_k}\)</p>
<p>如果是指数误差，则 \(e_{ki} = 1 -  exp(\frac{-y_i + G_k(x_i)}{E_k})\)</p>
<p>d) 计算回归误差率</p>
<p>$$e_k = \sum_{i=1}^m w_{ki}e_{ki}$$</p>
<p>e) 计算弱学习器的系数</p>
<p>$$\alpha_k = \frac{e_k}{1 - e_k}$$</p>
<p>f) 更新样本集的权重分布为</p>
<p>$$w_{k+1, j} = \frac{w_{ki}}{Z_k} \alpha_k^{1 - e_{ki}}$$</p>
<p>这里 \(Z_k\) 是规范化因子：</p>
<p>$$ Z_k = \sum_{i=1}^m w_{ki} \alpha_k ^{1 - e_{ki}}$$</p>
<p>3) 构建最终的强学习器：</p>
<p>$$f(x) = \sum_{k=1}^K = (ln \frac{1}{\alpha_k}) G_k(x)$$</p>
<h3 id="Adaboost算法的正则化"><a href="#Adaboost算法的正则化" class="headerlink" title="Adaboost算法的正则化"></a>Adaboost算法的正则化</h3><p>为了防止 Adaboost 过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长(learning rate)。定义为 ν，对于前面的弱学习器的迭代：</p>
<p>$$f_k(x) = f_{k-1}(x) + \alpha_k G_k (x)$$</p>
<p>如果我们加上了正则项，则有：</p>
<p>$$f_k(x) = f_{k-1}(x) + \nu \alpha_k G_k(x)$$</p>
<p>ν 的取值范围为 \(0&lt;ν≤10&lt;ν≤1\)。对于同样的训练集学习效果，较小的 ν 意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<h3 id="Adaboost-小结"><a href="#Adaboost-小结" class="headerlink" title="Adaboost 小结"></a>Adaboost 小结</h3><p>理论上任何学习器都可以用于 Adaboost。但一般来说，使用最广泛的 Adaboost 弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了 CART 分类树，而 Adaboost 回归用了 CART 回归树。</p>
<p>这里对Adaboost算法的优缺点做一个总结。</p>
<p>Adaboost的主要优点有：</p>
<ol>
<li>Adaboost作为分类器时，分类精度很高</li>
<li>在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。</li>
<li>作为简单的二元分类器时，构造简单，结果可理解。</li>
<li>不容易发生过拟合</li>
</ol>
<p>Adaboost的主要缺点有：</p>
<ol>
<li>对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/05/随机森林和GBDT/" itemprop="url">
                  集成学习：从随机森林到 GBDT
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-05T15:23:15+08:00" content="2017-03-05">
              2017-03-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/05/随机森林和GBDT/" class="leancloud_visitors" data-flag-title="集成学习：从随机森林到 GBDT">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>研究生期间参加了很多数据挖掘比赛，比赛的题目各不相同，但其实每次用的模型都差不多，其中用的最多的就是随机森林和 GBDT。所以今天打算总结一下这两个模型。</p>
<p>随机森林是一种 bagging 的方法，而 GBDT 则是一种 boosting 的方法。这两类方法统称为集成学习。</p>
<h3 id="什么是集成学习"><a href="#什么是集成学习" class="headerlink" title="什么是集成学习"></a>什么是集成学习</h3><p>集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到比单一模型更好的效果。常见的集成学习框架有：bagging，boosting和stacking。南京大学的周志华老师曾经对这三种方法有很明确的定义：</p>
<ul>
<li><strong>bagging</strong>：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。</li>
</ul>
<p><img src="/img/bagging.jpg" width="500" height="200" alt="图片名称" align="center"></p>
<ul>
<li><strong>boosting</strong>：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果。</li>
</ul>
<p><img src="/img/boosting.jpg" width="500" height="300" alt="图片名称" align="center"></p>
<ul>
<li><strong>stack</strong>：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。</li>
</ul>
<p><img src="/img/stack.jpg" width="500" height="400" alt="图片名称" align="center"></p>
<p>有了集成学习的思想，模型便有了 “集思广益” 的能力，也就不容易产生过拟合现象。但是，为什么这样就可以防止过拟合呢，这就不得不先从模型的偏差和方差入手。</p>
<h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。《Understanding the Bias-Variance Tradeoff》当中有一副图形象地向我们展示了偏差和方差的关系：</p>
<p><img src="/img/varience.png" width="500" height="400" alt="图片名称" align="center"></p>
<h4 id="模型的偏差和方差"><a href="#模型的偏差和方差" class="headerlink" title="模型的偏差和方差"></a>模型的偏差和方差</h4><p>模型的偏差：训练出来的模型在训练集上的误差。</p>
<p>模型的方差：假设模型是随机变量。设样本容量为 n 的训练集为随机变量的集合 \(X_1, X_2,…,X_n\)。那么模型是以这些随机变量为输入的随机变量函数。抽样的随机性会带来模型的随机性。</p>
<p>定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异很好量化，那么对于模型，它们的差异性是指模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。</p>
<p>我们认为方差越大的模型越容易过拟合：假设有两个训练集 A 和 B，经过 A 训练的模型 Fa 与经过 B 训练的模型Fb差异很大，这意味着 Fa 在类 A 的样本集合上有更好的性能，而 Fb 反之，则出现了过拟合。</p>
<p>集成模型中的基模型一般都是弱模型，通常而言，弱模型的偏差很大，方差小，虽然在训练集上效果不好，但是不容易过拟合。有些集成模型中的基模型不是弱模型，bagging 和 stacking 中的基模型为强模型（偏差低，方差高），boosting 中的基模型为弱模型。</p>
<p>在 bagging 和 boosting 框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging 和 boosting 的基模型都是线性组成的，那么有：</p>
<p><img src="/img/m1.png" width="400" height="400" alt="图片名称" align="center"></p>
<h4 id="bagging-的偏差和方差"><a href="#bagging-的偏差和方差" class="headerlink" title="bagging 的偏差和方差"></a>bagging 的偏差和方差</h4><p>对于 bagging 来说，每个基模型的权重等于 1/m 且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：</p>
<p><img src="/img/m2.png" width="400" height="300" alt="图片名称" align="center"></p>
<p>根据上式我们可以看到，<strong>整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似</strong>。同时，整体模型的方差小于等于基模型的方差（当相关性为 1 时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。<strong>另外，在此我们还知道了为什么 bagging 中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。</strong></p>
<p>随机森林是典型的 bagging 模型，在 bagging 的基础上，进一步降低了模型的方差。随机森林中的基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减小，第二项稍微增加，整体方差仍然是减少。</p>
<p>随机森林的子模型都拥有较低的偏差，在 sklearn 中，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None），同时，降低子模型间的相关度可以起到减少整体模型的方差的效果（max_features的默认值为auto。</p>
<h4 id="boosting-的偏差和方差"><a href="#boosting-的偏差和方差" class="headerlink" title="boosting 的偏差和方差"></a>boosting 的偏差和方差</h4><p>对于 boosting 来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于 1，故我们也可以针对 boosting 化简公式为：</p>
<p><img src="/img/m3.png" width="400" height="140" alt="图片名称" align="center"></p>
<p>通过观察整体方差的表达式，我们容易发现，<strong>若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。</strong> 因此，boosting框架中的基模型必须为弱模型。</p>
<p>因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于 1 吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</p>
<p>基于 boosting 框架的 GBDT 模型中基模型也为树模型，和随机森林一样，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<p>在 sklearn 中，GBDT 的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3），但是降低子模型间的相关度不能显著减少整体模型的方差（max_features的默认值为None）。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ol>
<li><p>使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力。</p>
</li>
<li><p>对于 bagging 来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低</p>
</li>
<li><p>对于 boosting 来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低</p>
</li>
<li><p>整体模型的偏差和方差与基模型的偏差和方差息息相关</p>
</li>
</ol>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>我们先讲 GBDT 模型的损失函数。</p>
<p>基于 boosting 框架的整体模型可以用线性组成式来描述，其中 \(h_{i}(x)\) 为基模型与其权值的乘积：</p>
<p>$$F(x) = \sum_i^m h_{i}(x)$$</p>
<p>根据上式，整体模型的训练目标是使预测值 \(F(x)\) 逼近真实值 y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p>
<p>$$F^i(x) = f^{i-1}(x) + h_i(x)$$</p>
<p>这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使 \(F^i(x)\) 逼近真实值y。</p>
<h4 id="拟合残差"><a href="#拟合残差" class="headerlink" title="拟合残差"></a>拟合残差</h4><p>使 \(F^i(x)\) 逼近真实值，其实就是使 \(h_i(x)\) 逼近真实值和上一轮迭代的预测值 \(F^{i-1}(x)\) 之差，即残差 (\( y - F^{i-1}(x))\) 。最直接的做法是构建基模型来拟合残差。</p>
<p>研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度：</p>
<p>$$- \frac{\partial (\frac{1}{2} \ast (y - F_{i-1}(x))^2)}{\partial F(x)} = y - F_{i-1}(x)$$</p>
<p>即，\(F^{i-1}(x)\) 加上反向梯度的 \(h_i(x)\) 得到 \(F^i(x)\)，该值可能导致平方差损失函数降低，预测的准确度降低。</p>
<h4 id="拟合反向梯度"><a href="#拟合反向梯度" class="headerlink" title="拟合反向梯度"></a>拟合反向梯度</h4><p>引入任意损失函数后，我们可以定义整体模型的迭代式如下：</p>
<p>$$F^i(x) = F^{i-1}(x) + argmin \sum_j^n L(y_j, F^{i-1}(x_j) + h_i(x_j))$$</p>
<h4 id="常见的损失函数"><a href="#常见的损失函数" class="headerlink" title="常见的损失函数"></a>常见的损失函数</h4><ul>
<li>最小均方误差函数：sklearn 中 GBDT 回归中的默认损失函数，刚才介绍的拟合残差其实就是改损失函数的反向梯度值。</li>
<li>logit 函数：LR 中常用的损失函数。逻辑回归的本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以用 logistics 函数表达。sklean 中 GBDT 分类模型默认采用这个损失函数。</li>
<li>指数损失函数：<br>$$L(y_j, F^{i-1}(x_j) = e^{-y_j \ast F^{i-1}(x_j)}$$<br>对该损失函数求反向梯度得：<br>$$- \frac{\partial ( y_j, F^{i-1}(x_j))}{\partial ^{i-1} F^{i-1}(x)} = y_j \ast e^{-y_j \ast F^{i-1}(x_j)}$$<br>这时，第 i 轮迭代中，新训练集如下：<br>$${(x_j, y_j \ast e^{-y_j \ast F^{i-1}(x_j)})}$$<br>这表明当损失函数是指数损失时，GBDT 相当于二分类的 Adaboost 算法。是的，指数损失仅能用于二分类的情况。</li>
</ul>
<h4 id="shrinkage"><a href="#shrinkage" class="headerlink" title="shrinkage"></a>shrinkage</h4><p>使用 GBDT 时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。</p>
<h4 id="初始模型"><a href="#初始模型" class="headerlink" title="初始模型"></a>初始模型</h4><p>我们定义损失模型为：</p>
<p>$$ F^0 (x) = argmin \sum_j^n L(y_j, \gamma)$$</p>
<p>根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。</p>
<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p><strong>偏差描述了模型在训练集准确度，而损失函数则是描述该准确度的间接量纲。</strong> 也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://www.jianshu.com/p/28604e0870d7" target="_blank" rel="external">Random Forest和Gradient Tree Boosting参数详解</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/04/机器学习中的L1-和-L2/" itemprop="url">
                  机器学习中的L1 和 L2
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-04T11:07:05+08:00" content="2017-03-04">
              2017-03-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/04/机器学习中的L1-和-L2/" class="leancloud_visitors" data-flag-title="机器学习中的L1 和 L2">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>有监督的学习方法，比如说我们平常说的 LR，SVM 等，它们的目标是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。参数太多的话，模型自然而言会对训练数据拟合的更好，训练误差会很小，但是我们的目的并不是在训练数据上表现好，而是在未知的测试数据上依然有很好的表现。所以，我们需要保证模型 ”简单“ 的基础上去最小化训练误差，这样得到的模型才能有很好的 “泛化” 能力。著名的 “奥卡姆剃刀” 原理说的就是这个东西，模型简单主要是通过规则项来实现，规则项的实现相当于将我们对这个模型的先验知识加入，强行的让学习到的模型具有人想要的特性，例如稀疏、低秩或者平滑等。</p>
<p>加入了规则项之后的有监督模型的目标函数都可以写成下面这中形式：</p>
<p>$$w^* = arg min_w \sum_i L(y_i, f(x_i , w)) + \lambda \Omega(w)$$</p>
<p>其中第一项 \(L(y_i, f(x_i , w))\) 衡量我们的模型对第 i 个样本的预测值 \(f(x_i , w))\) 和真实的标签 \(y_i\) 之间的误差。因为我们的模型是要拟合我们的训练样本。为了提升我们模型的泛化能力，我们需要加上第二项去限制，即对参数 w 的规则化函数 \(\Omega(w)\) 去约束我们的模型尽量的简单。</p>
<p>规则化 \(\Omega(w)\) 函数有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数 w 的约束不同，取得的效果也不同。今天我这篇文章主要介绍最常见的 L1 和 L2。</p>
<h3 id="L1-范数"><a href="#L1-范数" class="headerlink" title="L1 范数"></a>L1 范数</h3><p>说到 L1 范数，就不得不先提一下 L0 范数，L1 是经 L0 发展而来的。如果我们用 L0 范数去规则化一个参数矩阵 W 的话，就是希望 W 的大部分元素都是 0，即希望参数尽可能稀疏；而 L1 范数值指向量中各个元素的绝对值加和，常称 “lasso”。L1 也可以实现特征权值稀疏。那这是为什么呢？因为 L1 是 L0 范数的最优凸近似。对于任何的规则化算子，如果它在 Wi = 0 的地方不可微，并且可以分解为一个 “求和” 的形式，那么这个规则化算子就可以实现稀疏。参数矩阵 W 的 L1 范数是绝对值，|w| 在 w = 0 处是不可微的，并且可以分解为一个求和的形式。那么为什么要用 L1 而不是 L0 去实现特征稀疏呢，因为 L0 范数很难优化求解。</p>
<h4 id="为什么要特征稀疏"><a href="#为什么要特征稀疏" class="headerlink" title="为什么要特征稀疏"></a>为什么要特征稀疏</h4><p>特征稀疏有一个很大的好处是它可以实现特征的自动选择。一般而言，特征矩阵中的大部元素，也就是特征都是和最终的输出 yi 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑 xi 这些额外的特征，虽然可以获得更小的训练误差，但是在预测新样本的时候，可能会干扰正常的预测。故特征稀疏是为了帮组我们去更好的完成特征选择而设定的。</p>
<h4 id="特征稀疏的可解释性"><a href="#特征稀疏的可解释性" class="headerlink" title="特征稀疏的可解释性"></a>特征稀疏的可解释性</h4><p>经过特征稀疏之后，模型更容易解释。因为，当特征稀疏之后，大部分的特征权重都是 0，我们可以相信，那些特征权重不为 0 的特征才是对模型起关键意义的。</p>
<h3 id="L2-范数"><a href="#L2-范数" class="headerlink" title="L2 范数"></a>L2 范数</h3><p>L2 范数常为称为 “Ridge”。 它的形式是 \(||w||_2\)。当用于线性回归中时，它常被称为 “岭回归”。它的主要作用是去解决机器学习中的过拟合问题。那为什么呢？</p>
<p>L2 范数是指向量各元素的平方和然后求平方根。我们让 L2 范数的规则项 \(||w||_2\) 最小，可以使得 W 的每个元素都很小，通常很解决 0。越小的参数说明模型越简单，越简单的模型则不容易产生过拟合现象。因为当限制了参数很小时，实际上就限制了多项式模型分量的影响很小。</p>
<h4 id="L2-的好处"><a href="#L2-的好处" class="headerlink" title="L2 的好处"></a>L2 的好处</h4><p>L2 除了可以避免模型陷入过拟合的困境外，还可以处理 <strong>condition number</strong> 不好情况下矩阵求逆很困难的问题。</p>
<p>在机器学习中，去最优化目标函数时，有 2 个很头疼的问题：</p>
<ol>
<li>局部最小值，如果要优化的目标的函数不是凸函数的时候，我们用梯度下降或者其他方法去优化时，可能会得到局部最小值而不是全局最小值。</li>
<li>ill condition：ill-condition对应的是 well-condition。那他们分别代表什么？假设我们有个方程组 \(AX = b\)，我们需要求解 X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是 ill-condition 的，反之就是 well-condition 的。ill-condition 是指 \(AX = b\) 的解对于系数矩阵 A 或者 b 很敏感，A 或 b 微小的变化都可能影响解。</li>
</ol>
<p><strong>condition number</strong> 就是用来衡量 ill-condition 系统的可行度。condition number 衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number 值小的就是 well-conditioned 的，大的就是 ill-conditioned 的。</p>
<p>如果方阵 A 是非奇异的，那么 A 的 <strong>condition number</strong> 定义为：</p>
<p>$$\sigma (A) = ||A||\, ||A^{-1}||$$</p>
<p>也就是矩阵A的 norm 乘以它的逆的 norm。所以具体的值是多少，就要看你选择的 norm 是什么了。如果方阵 A 是奇异的，那么A的 condition number 就是正无穷大了。实际上，每一个可逆方阵都存在一个 condition number。但如果要计算它，我们需要先知道这个方阵的 norm（范数）和 Machine Epsilon（机器的精度）。为什么要范数？范数就相当于衡量一个矩阵的大小，我们知道矩阵是没有大小的，刚才不是要衡量一个矩阵 A 或者向量 b 变化的时候，我们的解 x 变化的大小吗？所以肯定得要有一个东西来度 量矩阵和向量的大小吧？这个东西就是范数，表示矩阵大小或者向量长度。OK，经过比较简单的证明，对于 \(AX = b\)，我们可以得到以下的结论：</p>
<p>$$\frac{||\Delta x||}{||x||} \leq ||A|| \cdot ||A^{-1}|| \cdot  \frac{||\Delta b||}{||b||}$$</p>
<p>$$\frac{||\Delta x||}{||x||} \leq \sigma (A) \cdot  \frac{||\Delta b||}{||b||}$$</p>
<p>$$\frac{||\Delta x||}{||x + \Delta x||} \leq \sigma (A) \cdot  \frac{||\Delta A||}{||b||}$$</p>
<p>也就是我们的解 x 的相对变化和 A 或者 b 的相对变化是有像上面那样的关系的，其中 \(\sigma (A)\) 的值就相当于倍率，看到了吗？相当于 x 变化的界。</p>
<p>对 condition number 来个一句话总结：condition number 是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的 condition number 在 1 附近，那么它就是 well-conditioned 的，如果远大于 1，那么它就是 ill-conditioned 的，如果一个系统是 ill-conditioned 的，它的输出结果就不是很可信。</p>
<p>从优化或者数值计算的角度来说，L2 范数有助于处理 condition number 不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：</p>
<p>$$w^* = (X^TX)^{-1}X^Ty$$</p>
<p>然而，如果当我们的样本 X 的数目比每个样本的维度还要小的时候，矩阵 \(X^TX\) 将会不是满秩的，也就是 \(X^TX\) 会变得不可逆，所以 \(w^*\) 就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。</p>
<p>但是如果加上 L2 规则项，就可以直接求逆，</p>
<p>$$w^* = (X^TX + \lambda I)^{-1}X^Ty$$</p>
<p>这里面，要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是 λ=0 的情况，如果矩阵 \(X^TX\) 的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善 condition number。</p>
<p>另外，如果使用迭代优化的算法，condition number 太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成 λ-strongly convex（λ 强凸）的了。</p>
<p>关于什么是 λ 强凸，如图说一个函数是凸函数，指函数曲线位于改点处的切线之上，而强凸则进一步要求位于该处的一个二次函数之上，也就是说要求函数不要太“平坦”而是可以保证有一定的 “向上弯曲” 的趋势。如果我们有“强凸”的话，我们就可以得到一个更好的近似解。效果的好坏取决于 strongly convex性质中的常数 λ 的大小。</p>
<p>所以，如果我们想要获得 λ 强凸的话，就是往目标函数里面加上 \(\frac{\alpha}{2} ||w||^{2}\)</p>
<p>在梯度下降中，目标函数收敛速率的上界实际上是和矩阵 \(X^TX\) 的 condition number 有关，\(X^TX\) 的 condition number 越小，上界就越小，也就是收敛速度会越快。L2 范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。</p>
<p>在机器学习中，常用的正则化项多是 L2，除了防止过拟合的问题，还有一个好处就是能否改善 ill-condition问题。尤其是当训练样本相对于特征数非常少时，其矩阵便是非满秩的，往往倾向于有无数个解，且是不可逆的。其 condition number 便会很大。一方面，根据此得到的最优化值很不稳定，往往某个特征变量很小的变动都会引发最终结果较大的偏差。另外通过矩阵求逆从而求的最优解就会变的非常困难。</p>
<h3 id="L1-和-L2-的区别"><a href="#L1-和-L2-的区别" class="headerlink" title="L1 和 L2 的区别"></a>L1 和 L2 的区别</h3><p>L1 和 L2，一个让绝对值最小，一个让平方最小，他们的差距大吗？其实，L2 相对于 L1 具有更为平滑的特性，在模型预测中，往往比L1具有更好的预测特性。当遇到两个对预测有帮助的特征时，L1倾向于选择一个更大的特征。而L2更倾向把两者结合起来。</p>
<h4 id="下降速度"><a href="#下降速度" class="headerlink" title="下降速度"></a>下降速度</h4><p>我们知道，L1 和 L2 都是规则化的方式，我们将权值参数以 L1 或者 L2 了的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1 和 L2 的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在 0 附近，L1 的下降速度比 L2 的下降速度要快。所以会非常快得降到0。</p>
<p><img src="/img/l1l2.png" width="500" height="520" alt="图片名称" align="center"></p>
<h4 id="模型空间的限制"><a href="#模型空间的限制" class="headerlink" title="模型空间的限制"></a>模型空间的限制</h4><p>$$Lasso: min_w \frac{1}{n} ||y - Xw||^2,  s.t ||w||_1 \leq C$$</p>
<p>$$Ridge: min_w \frac{1}{n} ||y - Xw||^2,  s.t ||w||_2 \leq C$$</p>
<p>也就是说，我们将模型空间限制在 w 的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2) 平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为 C 的一个 norm ball。等高线与 norm ball 首次相交的地方就是最优解：如下图：</p>
<p><img src="/img/l1l2p.png" width="800" height="420" alt="图片名称" align="center"></p>
<p>可以看到，L1-ball 与 L2-ball 的不同就在于 L1 在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有 w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。</p>
<p>相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么 L1-regularization 能产生稀疏性，而 L2-regularization 不行的原因了。</p>
<h4 id="贝叶斯先验"><a href="#贝叶斯先验" class="headerlink" title="贝叶斯先验"></a>贝叶斯先验</h4><p>正则化项从贝叶斯学习理论的角度来看，其相当于一种先验函数。即当你训练一个模型时，仅仅依靠当前的训练集数据是不够的，为了实现更好的预测（泛化）效果，我们还应该加上先验项。而 L1 则相当于设置一个Laplacean先 验，去选择 MAP（maximum a posteriori）假设。而 L2 则类似于 Gaussian先验。</p>
<p>因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso 在特征选择时候非常有用，而 Ridge 就只是一种规则化而已。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><p><a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="external">机器学习中的范数规则化之（一）L0、L1与L2范数</a></p>
</li>
<li><p><a href="http://t.hengwei.me/post/%E6%B5%85%E8%B0%88l0l1l2%E8%8C%83%E6%95%B0%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html" target="_blank" rel="external">浅谈L0,L1,L2范数及其应用</a></p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/03/线性模型和非线性模型的区别/" itemprop="url">
                  线性模型和非线性模型的区别
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-03T21:59:49+08:00" content="2017-03-03">
              2017-03-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/03/线性模型和非线性模型的区别/" class="leancloud_visitors" data-flag-title="线性模型和非线性模型的区别">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>在机器学习的回归问题中，线性模型和非线性模型都可以去对曲线进行建模，那么线性模型和非线性模型有什么区别呢？</p>
<p>其实，线性模型和非线性模型的区别并不在于能不能去拟合曲线。下面我们来详细介绍一下它们两个的区别。</p>
<h3 id="线性回归的等式"><a href="#线性回归的等式" class="headerlink" title="线性回归的等式"></a>线性回归的等式</h3><p>线性回归需要一个线性的模型。这到底意味着什么呢？</p>
<p>一个模型如果是线性的，就意味着它的参数项要么是常数，要么是原参数和要预测的特征之间的乘积加和就是我们要预测的值。</p>
<pre><code>Response = constant + parameter * predictior1 + ... + parameter * predictior2
</code></pre><p>下是个典型的线性模型：</p>
<p>$$Y = b + w_1x_1 + w_2x_2 + … + w_kx_k$$</p>
<p>在统计意义上，如果一个回归等式是线性的，那么它的相对于参数就必须也是线性的。如果相对于参数是线性，那么即使性对于样本变量的特征是二次方或者多次方，这个回归模型也是线性的 (例如下面的公式)。</p>
<p>$$Y = b + w_1x_1 + w_2x_2^2$$</p>
<p>你甚至可以使用 log 或者指数去形式化特征</p>
<p>$$Y = b + w_1e^{-x_1} + w_2e^{-x_2}$$</p>
<h3 id="非线性回归的等式"><a href="#非线性回归的等式" class="headerlink" title="非线性回归的等式"></a>非线性回归的等式</h3><p>最简单的判断一个模型是不是非线性，就是关注非线性本身，判断它的参数是不是非线性的。非线性有很多种形象，这也是为什么非线性模型能够那么好的拟合那些曲折的函数曲线的原因。比如下面这个：</p>
<p>$$Y = \theta_1 \ast x^{\theta_2}$$</p>
<p>$$Y = \theta_1 + (\theta_3 - \theta_2) \ast e^{- \theta_4 X}$$</p>
<p>与线性模型不一样的是，这些非线性模型的特征因子对应的参数不止一个。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/02/天池ijcai比赛总结/" itemprop="url">
                  IJCAI SocInf'16 Contest-Brick-and-Mortar Store Recommendation 天池大数据比赛总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-02T16:40:05+08:00" content="2017-03-02">
              2017-03-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/02/天池ijcai比赛总结/" class="leancloud_visitors" data-flag-title="IJCAI SocInf'16 Contest-Brick-and-Mortar Store Recommendation 天池大数据比赛总结">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>一直想总结一下这次的比赛，拖啊拖。。。一直等到现在，趁着现在要找实习，好好总结一下。</p>
<h3 id="比赛题目"><a href="#比赛题目" class="headerlink" title="比赛题目"></a>比赛题目</h3><p>比赛的官方网站在这，<a href="https://tianchi.shuju.aliyun.com/competition/introduction.htm?spm=5176.100066.333.2.4HLjZH&amp;raceId=231532" target="_blank" rel="external">IJCAI SocInf’16</a>。</p>
<p>这次比赛的题目是给定 2015 年 7 ~ 11 月份的用户在不同地点口碑购买记录，以及 2015 年 7 ~ 11 月淘宝上用户的购物行为数据，来预测 12 月这一整月用户来到一个地点之后会光顾哪些口碑商铺。这个比赛有一个很有意思的地方，就是它关注的是一个用户来到一个他之前没有去过的新地点之后，他会去哪些店铺消费，有一点像推荐系统中经典的冷启动问题。比赛提供的数据也有这个特点：</p>
<ol>
<li>在测试集中，只有 10 % 的用户用之前使用口碑的记录；</li>
<li>有 5 % 的用户虽然有之前使用口碑的记录，但是在测试集中，这些用户来到了新的地点；</li>
<li>所有用户都有他们淘宝购物的行为数据。</li>
</ol>
<p>如果题目只是这样的，其实还不算奇怪，奇怪的是题目的评价标准上加上了 <strong>budget</strong> 这个神奇的东西，先来看看评价指标：</p>
<p>$$P = \frac{\sum_i min(|S_i \cap S_i^*, b_i|)}{\sum_i| S_i^* |}$$</p>
<p>$$R = \frac{\sum_i min(|S_i \cap S_i^*, b_i|)}{\sum_i min(|S_i, b_i|)}$$</p>
<p>$$F_1 = \frac{2 \ast P \ast R}{P + R}$$</p>
<p>最后考核的目标是 F1 值。简单来说一下这个 <strong>budget</strong>：</p>
<p>我们的目标是在用户来到一个地点之后，给他推荐他可能会去的店铺。这里，把问题转换一下，我们要给很多店铺来推荐可能会来这里购物的人。这里推荐的人数要受到  <strong>budget</strong> 的限制，即不能超过店铺的最大承载量以及口碑提供给这家店铺的优惠券的个数。</p>
<p>先详细介绍一下比赛提供的数据格式：</p>
<ul>
<li>Online user behavior before Dec. 2015. (ijcai2016 taobao.csv)</li>
</ul>
<p>2015 年 7 ~ 11 月淘宝上用户的购物行为数据，包括用户的点击、购买，购买物品种类等属性。</p>
<ul>
<li>Users shopping records at brick-and-mortar stores before Dec. 2015. (ijcai2016 Koubei train.csv)</li>
</ul>
<p>2015 年 7 ~ 11 月份的用户在不同地点口碑购买记录，数据格式：(user_id, loc_id, merchant_id, time_stamp)</p>
<ul>
<li>Merchant information (ijcai2016 merchant info.csv)</li>
</ul>
<p>不同口碑商铺的位置分布情况，有的商铺有连锁店，数据格式：（merchan_id, loc_id1:loc_id2:…)</p>
<ul>
<li>Prediction result. (ijcai2016 Koubei test.csv)</li>
</ul>
<p>最后要提交的数据格式</p>
<p>user_id, loc_id, merchant_id1:merchant_id2…</p>
<p>对于这个问题，我们的想法是，要充分利用每一个用户和每一个口碑商铺的历史数据。更准确的说，我们要从训练集中提取足够的可训练的特征，然后利用一些经典的模型，比如 <strong>Xgboost</strong> 来构建分类模型。同时最需要注意的是，我们要时刻考虑 <strong>budget</strong> 的影响。</p>
<h3 id="如何来利用历史数据"><a href="#如何来利用历史数据" class="headerlink" title="如何来利用历史数据 ?"></a>如何来利用历史数据 ?</h3><p>我们分析了题目所给的训练集，然后将数据切分为两部分，第一部分是 2015 年 11 月 23 号之前的数据，我们把这当做本地训练集；另一部分是 2015 年 11 月 23 号之后的数据，我们把这当做本地测试集。通过对这些数据的整理和分析，我发现几条条重大的规律：</p>
<ol>
<li>如果地点确定的话，对于一个用户，他最有可能去的是他曾经光顾过的店。</li>
<li>从口碑店铺的角度来讲，如果这家店铺曾经的销量很好，那么它也将在未来吸引更多的顾客去消费。</li>
</ol>
<p>从这两条观察出来的规律来看，我们将探索数据的方向划分为两条道路：一条是从用户的角度出发；另外则是从口碑商店的角度出发。</p>
<h4 id="从用户的历史信息来看"><a href="#从用户的历史信息来看" class="headerlink" title="从用户的历史信息来看"></a>从用户的历史信息来看</h4><p>基于我们的观察，当一个人来到一个他之前去过的地点，他们更倾向于去之前购物过的商店消费。因此，一个在过去的一段时间内在一家商铺的消费次数能够在很大程度上影响我们的推荐质量。频率越高，越能代表这个人在未来会再次来到这家店消费。</p>
<p>我们做了一个统计，如果一个用户在过去曾经关顾过一家口碑店铺超过 6 次的话，当我们在之后再向这位用户推荐这家店的话，我们会得到超过 90 % 的准确率。</p>
<p>不过从另一方面说，每一家口碑店铺都有 <strong>budget</strong> 限制，这意味着，如果按照用户之前光顾过哪家店铺来推荐的话，肯定会有部分店铺的 <strong>budget</strong> 超标。</p>
<p>我们在用户的数据中找寻这样一个键值对 (user, location, merchant)，通过计算这样一个键值对出现的频率，我们可以统计出 (user, location, merchant, frequency) 的键值对，将这个键值对按照 <strong>frequency</strong> 来从高到低排序，然后按照这个顺序，从高到低来给 (user, query) 这样一个查询来推荐店铺，直到这家店铺的 <strong>budget</strong> 耗尽为止。</p>
<p>经过统计发现，平均每一个用户在一个地点只会关顾 1.3 个商家，所以在这里我们限制最大的推荐个数为 4。</p>
<h4 id="从商家的历史信息来看"><a href="#从商家的历史信息来看" class="headerlink" title="从商家的历史信息来看"></a>从商家的历史信息来看</h4><p>经过之前的分析，我们发现不同的店铺有着不同的“受欢迎度”。举个例子，“820”这个商家在整个训练集中几乎出现 1/4。问题是我们怎么定义这个“受欢迎度”呢？</p>
<p>为了解决这个问题，我们首先定义，在不同地点的同一家口碑商铺是不一样。因为在题目给定的数据中，存在大量的连锁商铺，但是这些连锁店铺在不同地点的“受欢迎度”是完全不同的。</p>
<p>接下来我们设想这样一种情况，90 % 来到地点 A 的人都会去 商铺 B 消费，在这种情况下，如果我们给所有的来地点 A 的人都推荐 B 商铺的话，我们就能得到 90 % 的准确率。所以，在一个地点某家店铺消费的总人数占该地点所有人数的比例，我们称之为该店铺的 “受欢迎度” (Popularity)。</p>
<p>因此我们将所有店铺按照 Popularity 从大到小来进行排序，依次推荐来到该地点的所有用户，直到超过 <strong>budget</strong>。经过一些线下的实验，我们取这个 Popularity 的值的阈值为 0.25。</p>
<h4 id="引入淘宝数据来提升推荐质量"><a href="#引入淘宝数据来提升推荐质量" class="headerlink" title="引入淘宝数据来提升推荐质量"></a>引入淘宝数据来提升推荐质量</h4><p>我们之前的推荐完全没有用到每个人的特征，相当于无法做到“千人千面”。于是接下来我们就想办法，如何利用淘宝的数据来提升推荐质量。</p>
<p>从直观上而且，淘宝的数据应该很有帮助，比如，在淘宝上经常浏览或者购买电子产品的人往往不太会去关顾口碑商铺里面那些卖女式服装的。受到这个的启发，我们就建立了一个这样的表：如果存在这样一条记录，一个用户在淘宝上浏览或者购买的商铺 A；同时也在线下口碑上的商铺 B 消费，我们就把 (A, B) 这个关系链表放入表中。基于这样的表，我们对那些之前没有口碑消费记录的新用户，如果他们曾在淘宝上购买或浏览了商铺 A， 那么我们就只给他推荐在关系链表中与 A 相连的口碑商铺。线上的结果证明，我们的预测质量提升了。</p>
<h3 id="引入机器学习模型"><a href="#引入机器学习模型" class="headerlink" title="引入机器学习模型"></a>引入机器学习模型</h3><p>目前为止我们都没有怎么用机器学习模型，用普通的规则就可以在天池上面排一个不错的名次。但是为了取得更好的成绩，我们尝试着去探寻每一个用户、地点和商铺的各种各样可能的特征。下面我将详细介绍这些我们的做法。</p>
<p>我们将这个问题看成是一个二分类问题。我们的方法是对每一个店铺建模，比如说，在数据集中，用户 u 在地点 l 的店铺 m 消费了。我们可以产生一个三元组 (u, l, m)。对应于这个三元组，我们可以产生一些训练数据，首先，对于我们而言，正样本即是那些消费过的用户，即 (u, l, m) 是 True；第二，我们的负例是那些同样是这个地点的其他商铺，比如说 m‘，我们将 (u, l, m’) 定义为 False。按照这个方法，我们可以产生供二分类的训练集。根据这个道理，对于赛题要我们预测的用户来到一个地点之后会去哪些店铺的情况，我们也可以根据这个三元组，产生一个每一个店铺的预测概率。</p>
<p>这么做其实负样本是很多的。。。为了避免正负样本不平衡的问题，我们采取采样的方法去提取负样本。</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>特征工程对应机器学习来说十分重要，俗话说，特征是模型的上限。我们观察到，有些用户喜欢关顾那些他们之前去过的店，有些喜欢光顾该地点上最热门的店铺，有些则喜欢去那些刚开张的店铺。</p>
<p>所以，针对 (u, l, m) 这样一个三元组，我们试图找寻关于他们其中任意一个的特征。</p>
<p>我们找寻的特征如下：</p>
<ol>
<li>键值对 (u, l, m) 出现的次数，即 frequence，用户关顾这家店的频率。</li>
<li>键值对 (u, l, m) 是否出现过，True or False。</li>
<li>user_id 的 onehot 编码。</li>
<li>merchant_id 编码。</li>
<li>(l, m) 出现次数，即这个地点这个商店的总销量。</li>
<li>open_interval：店铺的开业时间。</li>
<li>用户 u 在淘宝上购物的次数</li>
<li>用户 u 在淘宝上浏览的次数</li>
<li>用户 u 在淘宝上购买过的商品的种类的 onehot 编码</li>
<li>用户 u 在淘宝上购买过的商铺的 id 的 onehot 编码</li>
<li>用户 u 在淘宝上购买的比例 （购买数 / (购买 + 点击）</li>
<li>用户 u 在淘宝上购买的次数</li>
<li>用户 u 在淘宝上点击的次数</li>
<li>通过 SVD 算出来的用户 u 的潜在矩阵</li>
<li>通过 SVD 算出来的商店 m 的潜在矩阵</li>
</ol>
<h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><p>之前的规则可以得出一个结果，之后的模型也可以得出一个结果，在比赛的最后阶段，我们对模型进行融合，尝试各种不同的参数，达到了这个名次。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/28/问答社区的回答排序算法/" itemprop="url">
                  问答社区的回答排序算法(一）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-28T15:50:26+08:00" content="2017-02-28">
              2017-02-28
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/02/28/问答社区的回答排序算法/" class="leancloud_visitors" data-flag-title="问答社区的回答排序算法(一）">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>正好研究生的研究方向是问答社区的专家发现，所以，看过很多关于专家发现的 paper 以及一些工业界实践的经验。一直没有机会好好总结一下，借此机会梳理一下。问答社区的专家用户的发现有助于问题的顺利解决，并且也有助于提高问答社区的用户体验，有利于问答社区的长期发展。</p>
<p>现有的专家发现方法主要分为两种，一种是基于用户投票，另一种是基于用户之间的社交网络，本文将先介绍前一种。</p>
<h2 id="基于用户投票的排序算法"><a href="#基于用户投票的排序算法" class="headerlink" title="基于用户投票的排序算法"></a>基于用户投票的排序算法</h2><p>顾名思义，就是利用用户的反馈，即投票数（又细分为赞成和反对）来对答案进行排名。下面介绍三种方法，分别被 <strong>Urban Dictionary</strong>、<strong>Amazon</strong> 和 <strong>知乎</strong> 采用。</p>
<h3 id="得分-赞成票-反对票"><a href="#得分-赞成票-反对票" class="headerlink" title="得分 = 赞成票 - 反对票"></a>得分 = 赞成票 - 反对票</h3><p>假定有两个项目，项目 A 是 30 张赞成票，10 张反对票，项目 B 是 200 张赞成票，150 张反对票。如果按照上面的公式，B会排在前面，因为它的得分（200 - 150 = 50）高于 A（30 - 10 = 20）。但是实际上，B 的好评率只有 57%（200 / 350），而 A 为 75%（30 / 40），所以正确的结果应该是A排在前面。</p>
<p><strong>Urban Dictionary</strong> 就是这种错误算法的实例。</p>
<p><img src="/img/p1.png" alt=""></p>
<h3 id="得分-赞成票-总票数"><a href="#得分-赞成票-总票数" class="headerlink" title="得分 = 赞成票 / 总票数"></a>得分 = 赞成票 / 总票数</h3><p>如果总票数很大，这种算法其实是对的。问题出在如果总票数很少，这时就会出错。假定 A 有 2 张赞成票、0 张反对票，B 有 100 张赞成票、1 张反对票。这种算法会使得 A 排在 B 前面。这显然错误。</p>
<p><strong>Amazon</strong> 便采用的是这种做法。</p>
<p><img src="/img/p2.png" alt=""></p>
<h3 id="威尔逊得分"><a href="#威尔逊得分" class="headerlink" title="威尔逊得分"></a>威尔逊得分</h3><p>威尔逊得分的思想是，如果把一个回答展示给很多人看并让他们投票，内容质量不同的回答会得到不同比例的赞同和反对票数，最终得到一个反映内容质量的得分。当投票的人比较少时，可以根据已经获得的票数估计这个回答的质量得分，投票的人越多则估计结果越接近真实得分。</p>
<p>如果新一个回答获得了 1 票赞同 0 票反对，也就是说参与投票的用户 100% 都选了赞同，但是因为数量太少，所以得分也不会太高。如果一小段时间后这个回答获得了 20 次赞同 1 次反对，那么基于新算法，我们就有较强的信心把它排在另一个有 50 次赞同 20 次反对的回答前面。原因是我们预测当这个回答同样获得 50 次赞同时，它获得的反对数应该会小于 20。威尔逊得分算法最好的特性就是，即使前一步我们错了，现在这个新回答排到了前面，获得了更多展示，在它得到更多投票后，算法便会自我修正，基于更多的投票数据更准确地计算得分，从而让排序最终能够真实地反映内容的质量。</p>
<p>它有着很多优良特性：</p>
<ol>
<li>投票总数趋向于正无穷时，得分也趋向正反馈占总反馈的比例，对于内容质量的解释下很强。</li>
<li>在数据比较少的情况，总票数较少和极端参数的情况下，结果也能保持很好的鲁棒性。</li>
<li>置信区间大学可以通过参数保证。</li>
<li>虽然二项分布是离散类型，但是由于得分表达式关于正负反馈次数的函数是连续的，因此可以引入非整数的投票（加权投票），同时不改变算法性质。</li>
<li>得分的取值范围是（0，1），与投票总数无关，间接的做了归一化。</li>
</ol>
<p>威尔逊得分的计算公式如下：</p>
<p>$$score = \frac{\phi + \frac{z^2}{2n} - z * \sqrt{(\frac{\phi\ast (1 - \phi)}{n} + \frac{z^2}{4n^2}}}{(1 + \frac{z^2}{n})}$$</p>
<p>\(\phi\) 表示样本的赞成票比例，\(n\) 表示样本的大小，\(z_{1- \alpha/2}\) 表示对应某个置信水平的 \(z\) 统计量，这是一个常数，可以通过查表或统计软件包得到。一般情况下，在 95% 的置信水平下，z 统计量的值为 1.96。</p>
<p>威尔逊得分计算过程 (JavaScript 版本)：</p>
<pre><code>n = Up + Down
if (n==0) {
score = 0;
}
else {
z = 1.96
phat = Up / n
score = (phat + z*z/(2*n) - z * Math.sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n);
}
</code></pre><p>可以看到，当 \(n\) 的值足够大时，这个下限值会趋向。如果 \(n\) 非常小（投票人很少），这个下限值会大大小于。实际上，起到了降低 “赞成票比例” 的作用，使得该项目的得分变小、排名下降。</p>
<p>知乎采用的就是这种方法，下面摘一段<strong><a href="https://zhuanlan.zhihu.com/p/19902495?columnSlug=zhihu-product" target="_blank" rel="external">知乎产品总监</a></strong>对这个算法评价的一段话：</p>
<pre><code>  因此未来我们会看到更多新创作的优质内容，快速获得靠前的排序，低质内容则会长期
保持在底部。细心的你可能也想到了，并不是所有的回答最终都会获得很多投票，大体上
获得投票总数较多的回答仍然会排在投票较少的回答前面。
</code></pre><p>需要提一下，这个威尔逊算法在 x = 0 时函数取值收敛为 0，无法对赞同为 0，但反对票数不一样的回答进行排序。为了方便，知乎默认所有回答者对自己的投票投了一票赞同。这样不仅解决了这个问题，而且让回答者也将自身权重参与到排序的计算中。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://www.ruanyifeng.com/blog/2012/03/ranking_algorithm_wilson_score_interval.html" target="_blank" rel="external">基于用户投票的排名算法（五）：威尔逊区间</a></li>
<li><a href="https://www.zhihu.com/question/26933554" target="_blank" rel="external">如何评价知乎的回答排序算法？</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="PengShuang" />
          <p class="site-author-name" itemprop="name">PengShuang</p>
          <p class="site-description motion-element" itemprop="description">在路上，慢慢走！</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">71</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/pengshuang" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2176899852/profile?rightmod=1&wvr=6&mod=personnumber&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://lingyu.wang/" title="天镶的博客" target="_blank">天镶的博客</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://coolshell.cn/" title="酷壳" target="_blank">酷壳</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.dongwm.com" title="小明明的博客" target="_blank">小明明的博客</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PengShuang</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("DKbLgBme7UkAx9JX6sM3D4Hj-gzGzoHsz", "GXjJ9Ox3pUGI9PJhm6CNfJGN");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
