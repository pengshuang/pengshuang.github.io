<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="在路上，慢慢走！">
<meta property="og:type" content="website">
<meta property="og:title" content="小沙文的博客">
<meta property="og:url" content="http://pengshuang.space/index.html">
<meta property="og:site_name" content="小沙文的博客">
<meta property="og:description" content="在路上，慢慢走！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小沙文的博客">
<meta name="twitter:description" content="在路上，慢慢走！">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://pengshuang.space/"/>

  <title> 小沙文的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小沙文的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/22/Spark为什么有时候不适合做大规模机器学习/" itemprop="url">
                  Spark 为什么有时候不适合做大规模机器学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-22T08:47:12+08:00" content="2017-08-22">
              2017-08-22
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/22/Spark为什么有时候不适合做大规模机器学习/" class="leancloud_visitors" data-flag-title="Spark 为什么有时候不适合做大规模机器学习">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在 Spark 中，计算被建模为一种有向无环图（DAG），图中的每个顶点表示一个 RDD，每条边表示了 RDD 上的一个操作。RDD 由一系列被切分的对象（Partition）组成，这些被切分的对象在内存中存储并完成计算，也会在 Shuffle 过程中溢出（Overflow）到磁盘上<br>在 DAG 中，一条从顶点 A 到 B 的有向边 E，表示了 RDD B 是在 RDD A 上执行操作 E 的结果。操作分为转换（Transformation）和动作（Action）两类。转换操作（例如 map、filter 和 join）应用于某个 RDD 上，转换操作的输出是一个新的 RDD。</p>
<p>Spark 用户将计算建模为 DAG，该 DAG 表示了在 RDD 上执行的转换和动作。DAG 进而被编译为多个 Stage。每个 Stage 执行为一系列并行运行的任务（Task），每个分区（Partition）对应于一个任务。这里，窄依赖将有利于计算的高效执行，而宽依赖则会引入瓶颈，因为这样的依赖关系引入了通信密集的 Shuffle 操作，这打断了操作流。</p>
<p>Spark 的分布式执行是通过将 DAG Stage 划分到不同的计算节点实现的。驱动器（Driver）包含有两个调度器（Scheduler）组件，即 DAG 调度器和任务调度器。调度器对工作者分配任务，并协调工作者。</p>
<p>Spark 是为通用数据处理而设计的，并非专用于机器学习任务。要在 Spark 上运行机器学习任务，可以使用 MLlib for Spark。如果采用基本设置的 Spark，那么模型参数存储在驱动器节点上，在每次迭代后通过工作者和驱动器间的通信更新参数。如果是大规模部署机器学习任务，那么驱动器可能无法存储所有的模型参数，这时就需要使用 RDD 去容纳所有的参数。这将引入大量的额外开销，因为为了容纳更新的模型参数，需要在每次迭代中创建新的 RDD。更新模型会涉及在机器和磁盘间的数据 Shuffle，进而限制了 Spark 的扩展性。这正是基本数据流模型（即 DAG）的短板所在。Spark 并不能很好地支持机器学习中的迭代运算。</p>
<p>Spark 的核心概念是 RDD，而 RDD 的关键特性之一是其不可变性，来规避分布式环境下复杂的各种并行问题。这个抽象，在数据分析的领域是没有问题的，它能最大化的解决分布式问题，简化各种算子的复杂度，并提供高性能的分布式数据处理运算能力。</p>
<p>然而在机器学习领域，RDD的弱点很快也暴露了。机器学习的核心是迭代和参数更新。RDD凭借着逻辑上不落地的内存计算特性，可以很好的解决迭代的问题，然而 RDD 的不可变性，却非常不适合参数反复多次更新的需求。这本质上的不匹配性，导致了 Spark 的 MLlib 库，发展一直非常缓慢，从 2015 年开始就没有实质性的创新，性能也不好。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5NzkxMzg1Nw==&amp;mid=2653162826&amp;idx=1&amp;sn=c3faeb97640aff247a6ca8a5ff1bc4f2&amp;chksm=8b493024bc3eb932a773017f326891951ce0d91a7ee18a47def0190b4412245e44a8be71df03&amp;mpshare=1&amp;scene=1&amp;srcid=0821xpuPs81Z5r2tgGxZOKOI&amp;key=c95b8826c6a30de1fe4acefb9904c8a8837f3f2a4eb3153438d0434df5583e05d3e0f9e91d90af3c2178b9d7231f1234b449660e2c9e5551d4c0d580db00f48b48cce38d091dd3073e377ad9d30a7225&amp;ascene=0&amp;uin=MTk0NzAwMTc4MA%3D%3D&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.10.5+build(14F2109" target="_blank" rel="external">分布式机器学习平台大比拼：Spark、PMLS、TensorFlow、MXNet</a>&amp;version=12020810&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=z4lREgrF4ApaeNLlKcLiOrvyRDiCkvP%2BqlXAr6TO6tqbH4gGO3xXBQRWkz4D5YF9)</p>
</li>
<li><p><a href="https://www.qcloud.com/community/article/709568" target="_blank" rel="external">Spark 机器学习的加速器：Spark on Angel</a></p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/21/深度学习训练中的-large-batch-size-和-learning-rate/" itemprop="url">
                  深度学习训练中的 large batch size 和 learning rate
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-21T16:20:59+08:00" content="2017-08-21">
              2017-08-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/21/深度学习训练中的-large-batch-size-和-learning-rate/" class="leancloud_visitors" data-flag-title="深度学习训练中的 large batch size 和 learning rate">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>今天逛知乎的时候看到这个很经典的问题，相信很多人在训练 DNN 的时候都会遇到过，顺便记录总结下各位知乎大佬的回答。</p>
<h3 id="理解SGD、minibatch-SGD-和-GD"><a href="#理解SGD、minibatch-SGD-和-GD" class="headerlink" title="理解SGD、minibatch-SGD 和 GD"></a>理解SGD、minibatch-SGD 和 GD</h3><p>在机器学习优化算法中，梯度下降（gradient descent，简称 GD）是最常用的方法之一，简单来说就是在整个训练集中计算当前的梯度，选定一个步长进行更新。GD 的优点是，基于整个数据集得到的梯度，梯度估计相对较准，更新过程更准确。但也有几个缺点，一个是当训练集较大时，GD 的梯度计算较为耗时，二是现代深度学习网络的 loss function 往往是非凸的，基于凸优化理论，这种情况下优化算法只能收敛到局部最小值，因此使用 GD 训练深度神经网络，最终收收敛点很容易落在初始点附近的一个局部最小值区域，不太容易达到较好的收敛性能。</p>
<p>另一个极端是随机梯度下降（stochastic gradient descent），每次计算梯度只用一个样本，这样做的好处是计算快，而且很适合 online-learning 数据流式到达的场景，但缺点是单个实例产生的梯度估计往往很不准，所以得采用很小的 learning rate，而且由于现代的计算框架 CPU/GPU 的多线程工作，单个实例往往很难占满 CPU/GPU 的使用率，导致计算资源浪费。</p>
<p>折中的方案就是 mini-batch，一次采用 batch size 的实例来估计梯度，这样梯度估计相对于 SGD 更准，同时 batch size 能占满 CPU/GPU 的计算资源，又不像 GD 那样计算整个训练集。同时也由于 mini batch 能有适当的梯度噪声，一定程度上缓解 GD 直接掉进了初始点附近的局部最小导致收敛不好的缺点，所以 mini-batch 的方法也最为常用。</p>
<p>关于增大 batch size 对于梯度估计准确度的影响，分析如下：假设 batch size 为 m，对于一个 mini-batch，loss 为：</p>
<p>$$L = \frac{1}{m}\sum_{i=1}^{m} L(x_i, y_i)$$</p>
<p>梯度：</p>
<p>$$g = \frac{1}{m}\sum_{i=1}^{m} g(x_i, y_i)$$</p>
<p>整个 mini-batch 的梯度方差为</p>
<p>$$Var(g) = Var(\frac{1}{m}\sum_{i=1}^{m} g(x_i, y_i))<br>         = \frac{1}{m} Var(g(x_1, y_1))$$</p>
<p>由于每个样本 (xi, yi) 是随机从训练样本集采样得到的，因此样本梯度的方差相等。可以看到 batch size 增大 m 倍，相当于将梯度的方差减少 m 倍，因此梯度更加准确。</p>
<p>如果要保持方差和原来 SGD 一样，相当于给定了这么大的方差带宽容量，那么就可以增大 learning rate，充分利用这个方差容量，在上式中添加 learning rate，同时利用方差的变化公式，得到等式：</p>
<p>$$\frac{1}{m} Var(\sqrt{m} \ast lr \ast g(x_1, y_1)) = Var(lr \ast g(x_1, y_1))$$</p>
<p>因此可将 learning rate 增加 sqrt(m) 倍，以提高训练速度。</p>
<h3 id="large-batch-与-learning-rate"><a href="#large-batch-与-learning-rate" class="headerlink" title="large batch 与 learning rate"></a>large batch 与 learning rate</h3><p>在分布式训练中，batch size 随着数据并行的 worker 增加而增大，假设 baseline 的batch size 为 B，learning rate 为 lr，训练 epoch 数为 N。如果保持 baseline 的 learning rate，一般不会有较好的收敛速度和精度。原因如下：对于收敛速度，假设 k 个 worker，每次过的 sample 数量为 kB，因此一个 epoch 下的更新次数为 baseline 的 1/k，而每次更新的 lr 不变，所以要达到 baseline 相同的更新次数，则需要增加 epoch 数量，最大需要增加 k*N 个epoch，因此收敛加速倍数会远远低于 k。对于收敛精度，由于增大了 batch size 使梯度估计相较于 badeline 的梯度更加准确，噪音减少，更容易收敛到附近的局部最小，类似于 GD 的效果。为了解决这个问题，一个方法就是增大 lr，因为 batch 变大梯度估计更准，理应比 baseline 的梯度更确信一些，所以增大 lr，利用更准确的梯度多走一点，提高收敛速度。同时增大lr，让每次走的幅度尽量大一些，如果遇到了尖锐的局部最小，还有可能逃出收敛到更好的地方。</p>
<p>但是 lr 不能无限制的增大，原因分析如下。深度神经网络的 loss surface 往往是高维高度非线性的，可以理解为 loss surface 表面凹凸不平，坑坑洼洼，不像 y = x^2 曲线这样光滑，因此基于当前 weight 计算出来的梯度，往前更新的 learing rate 很大的时候，沿着 loss surface 的切线就走了很大一步，有可能大大偏于原有的 loss surface，示例如下图（a）所示，虚线是当前梯度的方向，也就是当前 loss surface 的切线方向，如果learning rate 过大，那这一步沿切线方向就走了很大一步，如果一直持续这样，那很可能就走向了一个错误的loss surface，如图(b)所示。如果是较小的learning rate，每次只沿切线方向走一小步，虽然有些偏差，依然能大致沿着 loss sourface steepest descent曲线想下降，最终收敛到一个不错的局部最小，如图(c)所示。</p>
<p><img src="/img/sgd.png" alt=""></p>
<p>因此，如何确定 large batch size 与 learing rate 的关系呢？分别比较 baseline 和 k 个 worker 的 large batch 的更新公式，如下：</p>
<p>$$w_{t+k} = w_t - \eta \frac{1}{n}\sum_{j &lt; k}\sum_{x \epsilon \beta_j} \nabla l(x, w_{t + j})$$</p>
<p>$$w_{t+k} = w_t - \eta \frac{1}{kn}\sum_{j &lt; k}\sum_{x \epsilon \beta_j} \nabla l(x, w_{t})$$</p>
<p>以上是 baseline (batch size B) 和 large batch(batch size kB) 的更新公式，第二个中 large batch 过一步的数据量相当于第一个 baseline k 步过的数据量，loss 和梯度都按找过的数据量取平均，因此，为了保证相同的数据量利用率，第二个中的 learning  rate 应该为 baseline 的 k 倍，也就是 learning rate 的 linear scale rule。</p>
<p>linear scale rule 有几个约束，其中一个约束是关于 weight 的约束，第一个公式中每一步更新基于的 weight 都是前一步更新过后的 weight，因此相当于小碎步的走，每走一步都是基于目前真实的 weight 计算梯度做更新的，而第二个公式的这一大步（相比 baseline 相当于 k 步）是基于 t 时刻的 weight 来做更新的。如果在这 k 步之内，W(t+j) ~ W(t) 的话，两者近似没有太大问题，也就是 linear scale rule 问题不大，但在 weight 变化较快的时候，会有问题，尤其是模型在刚开始训练的时候，loss 下特别快，weight 变化很快，W(t+j) ~ W(t) 就不满足。因此在初始训练阶段，一般不会直接将 lr 增大为 k 倍，而是从 baseline 的 lr 慢慢 warmup 到 k 倍，让 linear scale rule 不至于违背得那么明显。第二个约束是 lr 不能无限的放大，根据上面的分析，lr 太大直接沿 loss 切线跑得太远，导致收敛出现问题。同时，有文献指出，当 batch size 变大后，得到好的测试结果所能允许的 lr 范围在变小，也就是说，当 batch size 很小时，比较容易找打一个合适的 lr 达到不错的结果，当 batch size 变大后，可能需要精细地找一个合适的 lr 才能达到较好的结果，这也给实际的 large batch 分布式训练带来了困难。</p>
<p>从理论上来说，lr = batch_size * base lr，因为 batch_size 的增大会导致你 update 次数的减少，所以为了达到相同的效果，应该是同比例增大的。但是更大的 lr 可能会导致收敛的不够好，尤其是在刚开始的时候，如果你使用很大的 lr，可能会直接爆炸，所以可能会需要一些 warmup 来逐步的把 lr 提高到你想设定的 lr。实际应用中发现不一定要同比例增长，有时候可能增大到 batch_size/2 倍的效果已经很不错了。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="https://www.zhihu.com/question/64134994/answer/216895968" target="_blank" rel="external">如何理解深度学习分布式训练中的large batch size与learning rate的关系?</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/21/Spark-计算过程解读/" itemprop="url">
                  Spark 计算过程解读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-21T08:53:03+08:00" content="2017-08-21">
              2017-08-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/21/Spark-计算过程解读/" class="leancloud_visitors" data-flag-title="Spark 计算过程解读">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>Spark 是一个分布式的内存计算框架，其特点是能处理大规模数据，计算速度快，Spark 延续了 Hadoop 的 MapReduce 计算模型，相比之下 Spark 的计算过程保持在内存中，减少了硬盘读写，能够将多个操作进行合并计算，因此提升了计算速度，同时 Spark 也提供了更丰富的计算 API。</p>
<p>MapReduce 是 Hadoop 和 Spark 的计算模型，其特点是 Map 和 Reduce 过程的高度可并行化；过程间耦合度低，单个过程的失败后可以重新计算，而不会导致整体失败；最重要的是数据处理中的计算逻辑可以很好的转换为 Map 和 Reduce 操作。对于一个数据集来说，Map 对每条数据做相同的转换操作，Reduce 可以按条件对数据分组，然后在分组上做操作。除了 Map 和 Reduce 操作之外，Spark 还延伸出了如 filter，flatMap，count，distinct 等更丰富的操作。</p>
<p>RDD 是 Spark 中最基本也是最重要的数据结构，可以直观的认为 RDD 就是要处理的数据集。RDD 是分布式的数据集，每个 RDD 都支持 MapReduce 类操作，经过 MapReduce 操作后会产生新的 RDD，而不会修改原有 RDD。RDD 的数据集是分区的，因此可以把每个数据分区放到不同的分区上进行计算，而实际上大多数 MapReduce 操作都是在分区上进行计算的。Spark 不会把每一个 MapReduce 操作都发起计算，而是尽量的把操作累计起来一起计算。Spark 把操作划分为转换和动作，对 RDD 进行的转换操作会叠加起来，直到对 RDD 进行动作操作时才会发起计算。这种特性使得 Spark 可以减少中间结果的吞吐，可以快速的进行多次迭代计算。</p>
<h3 id="和-MapReduce-相比"><a href="#和-MapReduce-相比" class="headerlink" title="和 MapReduce 相比"></a>和 MapReduce 相比</h3><p>MapReduce 比较坑的地方:</p>
<ol>
<li>仅支持 Map 和 Reduce 操作。</li>
<li>处理效率低，Map 中间结果要写磁盘，Reduce 写 HDFS，多个 MR 之间通过 HDFS 交互数据，任务调度和启动的开销大，无法充分利用内存，Map 端和 Reduce 端均需要排序。</li>
<li>不适合迭代计算（如机器学习，图计算等），交互式处理（数据挖掘）和流式处理（点击日志分析)。</li>
</ol>
<p>Spark 的特点：</p>
<ol>
<li>高效（比 MapReduce 快 10 ~ 100 倍）。</li>
<li>内存计算引擎，提供 Cache 机制来支持需要反复迭代计算或者多次数据共享，减少数据读取的 IO 开销。</li>
<li>DAG 引擎，减少多次计算之间中间结果写到 HDFS 上的开销。</li>
<li>使用多线程池模型来减少 task 启动开销，shuffle 过程中避免不必要的 sort 操作已经减少磁盘 IO 操作。</li>
</ol>
<h3 id="系统结构"><a href="#系统结构" class="headerlink" title="系统结构"></a>系统结构</h3><p>Spark 自身只对计算负责，其计算资源的管理和调度由第三方框架来实现。常用的有 Yarn 和 Mesos。Spark on Yarn 的系统结构图如下:</p>
<p><img src="/img/yarn.png" alt=""></p>
<p>图中共分为三大部分：Spark Driver，Worker，Cluster manager。其中 Driver program 负责将 RDD 转换为任务，并进行任务调度。Worker 负责任务的执行。Yarn 负责计算资源的维护和分配。Driver 可以运行在用户程序中，或者运行在其中一个 Worker 上。Spark 中的每一个应用对应着一个 Driver。这个 Driver 可以接收 RDD 上的计算请求，每个动作（action）类型的操作被作为一个 Job 进行计算。Spark 会根据 RDD 的依赖关系构建计算阶段（Stage）的有向无环图，每个阶段有与分区数相同的任务（Task）。这些任务将在每个分区（Partition）上进行计算，任务划分完成后 Driver 将任务提交到运行于 Worker 上的 Executor 中进行计算，并对任务的成功、失败进行记录和重启等处理。</p>
<p>Worker 一般对应一台物理机，每个 Worker 上可以运行多个 Executor，每个 Executor 都是独立的 JVM 进程，Driver 提交的任务就是以线程的形式运行在 Executor 中的。如果使用 YARN 作为资源调度框架的话，其中一个 Worker 上还会有 Executor launcher 作为 Yarn 的 Application Master，用于向 Yarn 申请计算资源，并启动、监测、重启 Executor。</p>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>以 RDD 到输出结果的整个计算过程为主线，探究 Spark 的计算过程。这个计算过程可以分为：</p>
<ol>
<li>RDD 构建：构建 RDD 之间的依赖关系，将 RDD 转换为阶段的有向无环图。</li>
<li>任务调度：根据空闲计算资源情况进行任务提交，并对任务的运行状态进行监测和处理。</li>
<li>任务计算：搭建任务运行环境，执行任务并返回任务结果。</li>
<li>Shuffle 过程：两个阶段之间有宽依赖时，需要进行 Shuffle 操作。</li>
<li>计算结果收集：从每个任务收集并汇总结果。</li>
</ol>
<h4 id="RDD-构建和转换"><a href="#RDD-构建和转换" class="headerlink" title="RDD 构建和转换"></a>RDD 构建和转换</h4><p>RDD 按照其作用可以分为两种类型，一种是对数据源的封装，可以把数据源转换为 RDD，这种类型的 RDD 包括 NewHadoopRDD，ParallelCollectionRDD，JdbcRDD 等。另一种是对 RDD 的转换，从而实现一种计算方法，这种类型的 RDD 包括 MapperRDD，ShuffledRDD，FilteredRDD等。数据源类型的 RDD 不依赖于其他 RDD，计算类的 RDD 拥有自己 RDD 依赖。</p>
<p>RDD 有三个要素：分区，依赖关系，计算逻辑。分区是保证 RDD 分布式的特性，分区可以对 RDD 的数据进行划分，划分后的分区可以分布到不同的 Executor 中，大部分对 RDD 的计算都是在分区上进行的。依赖关系维护着 RDD 的计算过程，每个计算类型的 RDD 在计算时，会将所依赖的 RDD 作为数据源进行计算。根据一个分区的输出是否被多分区使用，Spark 将依赖分为窄依赖和宽依赖。RDD 的计算逻辑是其功能的体现，其计算过程是所依赖的 RDD 为数据源进行的。</p>
<h4 id="RDD-的依赖关系"><a href="#RDD-的依赖关系" class="headerlink" title="RDD 的依赖关系"></a>RDD 的依赖关系</h4><p>Spark 在遇到动作类操作时，就会发起计算 Job，把 RDD 转换为任务，并发送任务到 Executor 上执行。从 RDD 到任务的转换过程是在 DAGScheduler 中进行的。其总体思路是根据 RDD 的依赖关系，把窄依赖合并到一个阶段中，遇到宽依赖则划分出新的阶段，最终形成一个阶段的有向无环图，并根据图的依赖关系先后提交阶段。每个阶段按照分区数量划分为多个任务，最终任务被序列化并提交到 Executor 上执行。</p>
<h4 id="RDD-到-Task-的构建过程"><a href="#RDD-到-Task-的构建过程" class="headerlink" title="RDD 到 Task 的构建过程"></a>RDD 到 Task 的构建过程</h4><p>当 RDD 的动作类操作被调用时，RDD 将调用 SparkContext 开始提交 Job，SparkContext 将调用 DAGScheduler 将 RDD 转换为阶段的有向无环图，然后首先将有向无环图中没有未完成的依赖的阶段进行提交。在阶段被提交时，每个阶段将产生与分区数量相同的任务，这些任务称之为 TaskSet。任务的类型分为 ShuffleMapTask 和 ResultTask，如果阶段的输出将用于下个阶段的输入，也就是需要进行 Shuffle 操作，则任务类型为 ShuffleMapTask。如果阶段的输入即为 Job 结果，则任务类型为 ResultTask。任务创建完成后会交给 TaskSchedulerImpl 进行 TaskSet 级别的调度执行。</p>
<h4 id="Spark的任务调度"><a href="#Spark的任务调度" class="headerlink" title="Spark的任务调度"></a>Spark的任务调度</h4><p>Spark 中的任务调度实际上分了三个层次。第一层次是基于阶段的有向无环图进行 Stage 的调度，第二层次是根据调度策略（FIFO，FAIR）进行 TaskSet 调度，第三层次是根据数据本地性（Process，Node，Rack）在 TaskSet<br>内进行调度。</p>
<h3 id="任务计算"><a href="#任务计算" class="headerlink" title="任务计算"></a>任务计算</h3><p>任务的计算过程是在 Executor 上完成的，Executor 监听来自SchedulerBackend 的指令，接收到任务时会启动 TaskRunner 线程进行任务执行。在 TaskRunner 中首先将任务和相关信息反序列化，然后根据相关信息获取任务所依赖的 Jar 包和所需文件，完成准备工作后执行任务的 run 方法，实际上就是执行 ShuffleMapTask 或 ResultTask 的 run 方法。任务执行完毕后将结果发送给 Driver 进行处理。</p>
<p>在 Task.run 方法中可以看到 ShuffleMapTask 和 ResultTask 有着不同的计算逻辑。ShuffleMapTask 是将所依赖 RDD 的输出写入到 ShuffleWriter 中，为后面的 Shuffle 过程做准备。ResultTask 是在所依赖 RDD 上应用一个函数，并返回函数的计算结果。在这两个 Task 中只能看到数据的输出方式，而看不到应有的计算逻辑。实际上计算过程是包含在 RDD 中的，调用 RDD.Iterator 方法获取 RDD 的数据将触发这个 RDD 的计算动作（RDD.Iterator），由于此 RDD 的计算过程中也会使用所依赖 RDD 的数据。从而 RDD 的计算过程将递归向上直到一个数据源类型的 RDD，再递归向下计算每个 RDD 的值。需要注意的是，以上的计算过程都是在分区上进行的，而不是整个数据集，计算完成得到的是此分区上的结果，而不是最终结果。</p>
<p>从 RDD 的计算过程可以看出，RDD 的计算过程是包含在 RDD 的依赖关系中的，只要 RDD 之间是连续窄依赖，那么多个计算过程就可以在同一个 Task 中进行计算，中间结果可以立即被下个操作使用，而无需在进程间、节点间、磁盘上进行交换。</p>
<h3 id="Shuffle-过程"><a href="#Shuffle-过程" class="headerlink" title="Shuffle 过程"></a>Shuffle 过程</h3><p>Shuffle 是一个对数据进行分组聚合的操作过程，原数据将按照规则进行分组，然后使用一个聚合函数应用于分组上，从而产生新数据。Shuffle 操作的目的是把同组数据分配到相同分区上，从而能够在分区上进行聚合计算。为了提高 Shuffle 性能，还可以先在原分区对数据进行聚合（mapSideCombine），然后再分配部分聚合的数据到新分区，第三步在新分区上再次进行聚合。</p>
<p>在划分阶段时，只有遇到宽依赖才会产生新阶段，才需要 Shuffle 操作。宽依赖与窄依赖取决于原分区被新分区的使用关系，只要一个原分区会被多个新分区使用，则为宽依赖，需要 Shuffle。否则为窄依赖，不需要 Shuffle。</p>
<p>以上也就是说只有阶段与阶段之间需要 Shuffle，最后一个阶段会输出结果，因此不需要 Shuffle。Shuffle 是通过 Map 阶段的 ShuffleMapTask 与 Reduce 阶段的 ShuffledRDD 配合完成的。其中 ShuffleMapTask 会把任务的计算结果写入 ShuffleWriter，ShuffledRDD 从 ShuffleReader 中读取数据，Shuffle 过程会在写入和读取过程中完成。以 HashShuffle 为例，HashShuffleWriter 在写入数据时，会决定是否在原分区做聚合，然后根据数据的Hash 值写入相应新分区。HashShuffleReader 再根据分区号取出相应数据，然后对数据进行聚合。</p>
<h3 id="计算结果收集"><a href="#计算结果收集" class="headerlink" title="计算结果收集"></a>计算结果收集</h3><p>ResultTask 任务计算完成后可以得到每个分区的计算结果，此时需要在 Driver上对结果进行汇总从而得到最终结果。</p>
<p>RDD 在执行 collect，count 等动作时，会给出两个函数，一个函数在分区上执行，一个函数在分区结果集上执行。例如 collect 动作在分区上（Executor中）执行将 Iterator 转换为 Array 的函数，并将此函数结果返回到 Driver。Driver 从多个分区上得到 Array 类型的分区结果集，然后在结果集上（Driver中）执行合并 Array 的操作，从而得到最终结果。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/20/AUC、ROC-以及-KS-介绍/" itemprop="url">
                  AUC、ROC 以及 KS 介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-20T09:37:36+08:00" content="2017-08-20">
              2017-08-20
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/20/AUC、ROC-以及-KS-介绍/" class="leancloud_visitors" data-flag-title="AUC、ROC 以及 KS 介绍">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>ROC（Receiver Operating Characteristic）曲线和 AUC 常被用来评价一个二值分类器（binary classifier）的优劣。</p>
<p>KS（Kolmogorov-Smirnov）检验：K-S 检验主要是验证模型的区分能力，通常是在模型预测全体样本的 label 后，将全体样本按 label 为正与 label 为负分为两部分，然后用 K-S 统计量来检验这两组样本 label 预测概率的分布是否有显著差异。</p>
<h3 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h3><p>对于分类器，或者说分类算法，评价指标主要有 precision，recall，F-score1，以及这篇文章里面介绍的 ROC 和 AUC。下图是一个 ROC 曲线的示例：</p>
<p><img src="/img/roc.png" alt=""></p>
<p>ROC 曲线的横坐标为 false positive rate（FPR），纵坐标为true positive rate（TPR），关于 FPR 和 TPR 的定义如下：</p>
<p><img src="/img/fpr-and-tpr.png" alt=""></p>
<p>ROC 曲线图中有四个特殊的点和一条特殊的线。第一个点，(0,1)，即 FPR = 0, TPR = 1，这意味着 FN = 0，并且 FP = 0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即 FPR = 1，TPR = 0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即 FPR = TPR = 0，即 FP = TP = 0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>ROC 曲线图中的特殊的线是 y = x。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如 (0.5，0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。</p>
<h3 id="绘制-ROC-曲线"><a href="#绘制-ROC-曲线" class="headerlink" title="绘制 ROC 曲线"></a>绘制 ROC 曲线</h3><p>对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组 FPR 和 TPR 结果，而要得到一个曲线，则需要一系列 FPR 和 TPR 的值。如何得到这么一系列的值，我们需要去调整分类器的阈值，这里的阈值指的是大于这个阈值则判为正，小于则判为负，然后计算得到的 FPR 和 TPR。</p>
<p>当我们将阈值设置为 1 和 0 时，分别可以得到 ROC 曲线上的 (0,0) 和 (1,1) 两个点。将这些 (FPR, TPR) 对连接起来，就得到了 ROC 曲线。当阈值取值越多，ROC 曲线越平滑。</p>
<h3 id="AUC-值"><a href="#AUC-值" class="headerlink" title="AUC 值"></a>AUC 值</h3><p>AUC（Area Under Curve）被定义为 ROC 曲线下的面积，显然这个面积的数值不会大于 1。又由于 ROC 曲线一般都处于 y = x 这条直线的上方，所以 AUC 的取值范围在 0.5 和 1 之间。使用 AUC 值作为评价标准是因为很多时候 ROC 曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应 AUC 更大的分类器效果更好。</p>
<h3 id="为什么使用-ROC-曲线"><a href="#为什么使用-ROC-曲线" class="headerlink" title="为什么使用 ROC 曲线"></a>为什么使用 ROC 曲线</h3><p>相比其他评价指标，ROC 曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC 曲线能够保持不变。在实际的数据集中经常会出现类不平衡现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图对比了 ROC 图和其他评价指标图：</p>
<p><img src="/img/roc-and-precall.png" alt=""></p>
<p>在上图中，(a) 和 (c) 为ROC曲线，(b) 和 (d) 为 Precision-Recall 曲线。(a) 和 (b) 展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c) 和 (d) 是将测试集中负样本的数量增加到原来的 10 倍后，分类器的结果。可以明显的看出，ROC 曲线基本保持原貌，而 Precision-Recall 曲线则变化较大。</p>
<h3 id="K-S-曲线绘制方法"><a href="#K-S-曲线绘制方法" class="headerlink" title="K-S 曲线绘制方法"></a>K-S 曲线绘制方法</h3><p>K-S 也可以作为常用的模型评价指标。它和 ROC 曲线的画法有很大不同。以 LR 模型为例，首先把 LR 模型输出的概率从大到小排序，然后取前 10% 的值（也就是概率值）作为阀值，同理把 10% * k（k=1,2,3, … ,9）处的值作为阀值，计算出不同的 FPR 和 TPR 值，以10% * k（k=1,2,3, … ,9）为横坐标，分别以 TPR 和 FPR 的值为纵坐标，就可以画出两个曲线，这就是K-S曲线。</p>
<p>从 K-S 曲线就能衍生出 KS 值，KS = max(TPR - FPR)，即是两条曲线之间的最大间隔距离。当 (TPR - FPR) 最大时，即 ΔTPR - ΔFPR = 0，这和 ROC 曲线上找最优阀值的条件 ΔTPR = ΔFPR 是一样的。从这点也可以看出，ROC 曲线、K-S 曲线、KS 值的本质是相同的。</p>
<p>K-S 曲线能直观地找出模型中差异最大的一个分段，比如评分模型就比较适合用 KS 值进行评估；但同时，KS 值只能反映出哪个分段是区分度最大的，不能反映出所有分段的效果。所以，在实际应用中，模型评价一般需要将 ROC 曲线、K-S 曲线、KS 值、AUC 指标结合起来使用。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><p><a href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/" target="_blank" rel="external">ROC和AUC介绍以及如何计算AUC</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/24327437" target="_blank" rel="external">关于模型检验的ROC值和KS值的异同_ROC曲线和KS值</a></p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/06/CTR-中-LR、GBDT-LR、FM-DNN-比较/" itemprop="url">
                  CTR 中 LR、GBDT + LR、FM 和 DNN 比较
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-06T19:03:59+08:00" content="2017-08-06">
              2017-08-06
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/工程经验/" itemprop="url" rel="index">
                    <span itemprop="name">工程经验</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/06/CTR-中-LR、GBDT-LR、FM-DNN-比较/" class="leancloud_visitors" data-flag-title="CTR 中 LR、GBDT + LR、FM 和 DNN 比较">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近在知乎上看到这个问题，觉得很有意思，顺便整理总结了一下各位大佬的回答。</p>
<h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><p><strong>优点</strong>：LR，算法简单，容易并行和工程化实现。FTRL 是 LR 在线学习版本，目前广泛应用在工业界。优点就是简单，能够处理超高纬度稀疏问题，能够做到实时。</p>
<p><strong>缺点</strong>：因为 LR 是线性模型，基本靠人工特征工程，来选择交叉特征。一般都是 wrapper 方法选择，每轮可能都要进行很长时间的运算，理论上要进行 2 的 n 次方轮(n 是特征数)，但因为离线分析的指标和线上效果不一定是强相关的，所以分析起来有些痛苦。更头疼的是点击率预估的数据变化是比较大的，线上线下结果不一定对应，离线选出来的特征，参数都不一定适用于未来。而且在线预测时，因为大量的特征都要与广告ID交叉，所以在线拼装特征的成本很高，可能会存在性能问题。</p>
<h3 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT + LR"></a>GBDT + LR</h3><p><strong>优点</strong>：GBDT 加 LR。GBDT 对连续特征划分能力超强，主要来提取特征，再加上一些稀疏特征，补足了 LR 的不足。优点是把 LR 中人工构造特征的大部分工作给做了。</p>
<p><strong>缺点</strong>：离线处理和在线处理都复杂。不同于比赛，在实践中 ID 类特征还是非常重要的，广告 ID 可能就有几十万个，树的深度很难控制，模型实现也很难。另外，在点击率预估中如果特征本身没问题，加上去一般都不会降效果，所以这种做法有待验证。</p>
<h3 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h3><p><strong>优点</strong>：优点是拟合能力强，样本足够的情况下一般效果都不会差。本质上讲，该方法是通过前面多层的隐藏网络学习抽象特征（特征之间的组合），由数据驱动的模式学习到了人工特征工程难以学到的隐含特征，并在最后输出层使用上述抽象得到的特征完成最终的学习任务（全连接层）。</p>
<p><strong>缺点</strong>：计算复杂，工程化和实时化难度大。而且模型如果哪天出问题了（线上 AUC 突降），难以排查。</p>
<h3 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h3><p><strong>优点</strong>：相较与 LR，能够捕捉特征之间的交叉关系。原理是在一阶拟合的基础上加入二阶拟合，可以自动的学习任意两维特征的交叉。而且，交叉是以 embedding 向量的形式表达。跟 LR 一样，他可以吞吐超大规模的稀疏特征空间的样本集合。这种形式可以比较好的提高模型的表达能力，把性能和学习非线性结构的能力结合在一起，在工业界应用的也很广泛。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="https://www.zhihu.com/question/62109451" target="_blank" rel="external">广告点击率模型中，LR, GBDT+LR, FM, DNN等模型的优点和缺点？实际效果如何?</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/05/Resnet-学习笔记/" itemprop="url">
                  Resnet 学习笔记
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-05T15:50:59+08:00" content="2017-08-05">
              2017-08-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/05/Resnet-学习笔记/" class="leancloud_visitors" data-flag-title="Resnet 学习笔记">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>深度学习网络的深度对最后的分类和识别的效果有着很大的影响，所以一般把网络设计的越深效果越好，但是事实上却不是这样，常规的网络的堆叠（plain network）在网络很深的时候，效果却越来越差了。<strong>这里的效果差不是单指在测试集上的效果差，而是在训练集和测试集上的效果都变差。</strong></p>
<p>产生这种问题的原因之一即是网络越深，梯度消失的现象就越来越明显，网络的训练效果也不会很好。但是现在浅层的网络（shallower network）又无法明显提升网络的识别效果了，所以 Resnet（残差网络）的目标是在网络深度加深的情况下解决梯度消失的问题。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/img/resnet1.jpeg" alt=""></p>
<p>常规的神经网络结构如上图所示。</p>
<p><img src="/img/resnet2.jpeg" alt=""></p>
<p>ResNet 引入了残差网络结构（residual network），通过残差网络，可以把网络层弄的很深，据说可以达到了1000多层，最终的网络分类的效果也是非常好，残差网络的基本结构如上图所示。</p>
<p>通过增加一个恒等映射（identity mapping），同时在输出个输入之间引入一个 shortcut connection，而不是简单的堆叠网络。将原始所需要学习的函数 <strong>H(x)</strong> 转换成 <strong>F(x) + x</strong>。这样可以解决网络由于很深出现梯度消失的问题，从而可以把网络做的很深，ResNet 其中一个网络结构如下图所示：</p>
<p><img src="/img/resnet3.jpeg" alt=""></p>
<p>Resnet 网络结构的基本设计方案 — VGG-style：</p>
<ol>
<li>所有的3x3卷积层（几乎所有）</li>
<li>空间规模/2 =&gt; #过滤器x2 (~每一层的复杂度相同)</li>
<li>简约的设计风格</li>
</ol>
<p>训练方法：</p>
<ol>
<li><p>所有的平原/残差网络都是从头开始训练的。</p>
</li>
<li><p>所有的平原/残差网络都运用组归一化（Batch Normalization）。</p>
</li>
<li><p>标准化的超参数&amp;增强。</p>
</li>
</ol>
<p>论文中介绍了一个深层次的残差学习框架来解决精准度下降问题。并且明确地让这些层适合残差映射，而不是寄希望于每一个堆叠层直接适合一个所需的底层映射。形式上，把 H(x) 作为所需的基本映射，让堆叠的非线性层适合另一个映射 F(x) := H(x) - x。</p>
<p>公式 F(x) + x 可以通过 shortcut 前馈神经网络实现。shortcut 是那些跳过中的一层或更多层。在我们的情景中，shortcut 简单的执行身份映射，并将它们的输出添加到叠加层的输出。身份快捷连接添加既不产生额外的参数，也会增加不计算的复杂度。通过反向传播的SGD，整个网络仍然可以被训练成终端到端的形式。</p>
<h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p>残差网络的精美之处在于那个 shortcut 的设计。</p>
<p>增加一个恒等映射这一步，将原始所需要学的函数 H(x) 转换成 F(x) + x。论文认为这两种表达的效果是相同的，但是优化的难度却并不相同。</p>
<p>首先作者假设 F(x) 的优化会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个 reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。</p>
<p>这个 Residual block 通过 shortcut connection 实现，通过 shortcut 将这个 block 的输入和输出进行一个 element-wise 的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。</p>
<p><img src="/img/resnet4.png" alt=""></p>
<p>残差网络也可以从另一个角度来理解，如上图所示。</p>
<p>残差网络单元其中可以分解成右边的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，即，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling）。</p>
<h3 id="为了证明，如果删除残差网络的一部分"><a href="#为了证明，如果删除残差网络的一部分" class="headerlink" title="为了证明，如果删除残差网络的一部分"></a>为了证明，如果删除残差网络的一部分</h3><p>如果把残差网络理解成一个Ensambling系统，那么网络的一部分就相当于少一些投票的人，如果只是删除一个基本的残差单元，对最后的分类结果应该影响很小；而最后的分类错误率应该是和删除的残差单元的个数成正比的，这个结论也被学者实验证明。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/01/数据库中视图的作用/" itemprop="url">
                  数据库中视图的作用
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-08-01T15:42:55+08:00" content="2017-08-01">
              2017-08-01
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/数据库/" itemprop="url" rel="index">
                    <span itemprop="name">数据库</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/08/01/数据库中视图的作用/" class="leancloud_visitors" data-flag-title="数据库中视图的作用">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>视图是从一个或几个基本表（或视图）导出的表。它与基本表不同，是一个虚表。数据库只存放视图的定义，而不存放视图对应的数据，这些数据仍存放在原来的基本表中。所以基本表中的数据发生变化，从视图中查询出的数据也就随之改变了。从这个意义上讲，视图就像一个窗口，透过它可以看到数据库中自己感兴趣的数据及其变化。</p>
<p>这是因为合理地使用视图能够带来许多好处：</p>
<h3 id="1-视图能简化用户操作"><a href="#1-视图能简化用户操作" class="headerlink" title="1. 视图能简化用户操作"></a>1. 视图能简化用户操作</h3><p>视图机制使用户可以将注意力集中在所关心地数据上。如果这些数据不是直接来自基本表，则可以通过定义视图，使数据库看起来结构简单、清晰，并且可以简化用户的的数据查询操作。例如，那些定义了若干张表连接的视图，就将表与表之间的连接操作对用户隐藏起来了。换句话说，用户所作的只是对一个虚表的简单查询，而这个虚表是怎样得来的，用户无需了解。</p>
<h3 id="2-视图使用户能以多种角度看待同一数据"><a href="#2-视图使用户能以多种角度看待同一数据" class="headerlink" title="2. 视图使用户能以多种角度看待同一数据"></a>2. 视图使用户能以多种角度看待同一数据</h3><p>视图机制能使不同的用户以不同的方式看待同一数据，当许多不同种类的用户共享同一个数据库时，这种灵活性是非常必要的。</p>
<h3 id="3-视图对重构数据库提供了一定程度的逻辑独立性"><a href="#3-视图对重构数据库提供了一定程度的逻辑独立性" class="headerlink" title="3. 视图对重构数据库提供了一定程度的逻辑独立性"></a>3. 视图对重构数据库提供了一定程度的逻辑独立性</h3><p>数据的物理独立性是指用户的应用程序不依赖于数据库的物理结构。数据的逻辑独立性是指当数据库重构造时，如增加新的关系或对原有的关系增加新的字段，用户的应用程序不会受影响。层次数据库和网状数据库一般能较好地支持数据的物理独立性，而对于逻辑独立性则不能完全的支持。</p>
<p>在关许数据库中，数据库的重构造往往是不可避免的。重构数据库最常见的是将一个基本表“垂直”地分成多个基本表。例如：将学生关系Student（Sno，Sname，Ssex，Sage，Sdept），</p>
<p>分为SX（Sno，Sname，Sage）和SY（Sno，Ssex，Sdept）两个关系。这时原表Student为SX表和SY表自然连接的结果。如果建立一个视图Student：</p>
<pre><code>CREATE VIEW Student（Sno，Sname，Ssex，Sage，Sdept）
AS
SELECT SX.Sno，SX.Sname，SY.Ssex，SX.Sage，SY.Sdept
FROM SX，SY
WHERE SX.Sno=SY.Sno;
</code></pre><p>这样尽管数据库的逻辑结构改变了（变为SX和SY两个表了），但应用程序不必修改，因为新建立的视图定义为用户原来的关系，使用户的外模式保持不变，用户的应用程序通过视图仍然能够查找数据。</p>
<p>当然，视图只能在一定程度上提供数据的逻辑独立，比如由于视图的更新是有条件的，因此应用程序中修改数据的语句可能仍会因为基本表构造的改变而改变。</p>
<h3 id="4-视图能够对机密数据提供安全保护"><a href="#4-视图能够对机密数据提供安全保护" class="headerlink" title="4. 视图能够对机密数据提供安全保护"></a>4. 视图能够对机密数据提供安全保护</h3><p>有了视图机制，就可以在设计数据库应用系统时，对不同的用户定义不同的视图，使机密数据不出现在不应该看到这些数据的用户视图上。这样视图机制就自动提供了对机密数据的安全保护功能。例如，Student 表涉及全校 15 个院系学生数据，可以在其上定义15个视图，每个视图只包含一个院系的学生数据，并只允许每个院系的主任查询和修改本原系学生视图。</p>
<h3 id="5-适当的利用视图可以更清晰地表达查询"><a href="#5-适当的利用视图可以更清晰地表达查询" class="headerlink" title="5. 适当的利用视图可以更清晰地表达查询"></a>5. 适当的利用视图可以更清晰地表达查询</h3><p>例如经常需要执行这样的查询“对每个学生找出他获得最高成绩的课程号”。可以先定义一个视图，求出每个同学获得的最高成绩：</p>
<pre><code>CREATE VIEW VMGRADE
AS
SELECT Sno，MAX(Grade) Mgrade
FROM SC
GROUP BY Sno;
</code></pre><p>然后用如下的查询语句完成查询：</p>
<pre><code>SELECT SC.Sno，Cno FROM SC，VMGRADE WHERE SC.Sno = VMGRADE.Sno AND 
SC.Grade = VMGRADE.Mgrade;
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/23/Parameter-Server-学习/" itemprop="url">
                  Parameter Server 学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-07-23T16:36:58+08:00" content="2017-07-23">
              2017-07-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/系统架构/" itemprop="url" rel="index">
                    <span itemprop="name">系统架构</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/07/23/Parameter-Server-学习/" class="leancloud_visitors" data-flag-title="Parameter Server 学习">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>大数据机器学习系统，通常数据在 1 TB 到 1 PB 之间，参数在 10 的 9 次方和 10 的 12 次方左右，很多算法的参数只能采用分布式存储。这样的话会带来下面几个问题：</p>
<ul>
<li>访问这些参数需要很大的网络带宽。</li>
<li>很多算法是序列性的，同步会影响性能。</li>
<li>在大规模分布式的情况下，如何设计容错机制至关重要。</li>
</ul>
<p>为了解决这些问题，大神们提出了一种新的架构 - Parameter Server (简称 PS)</p>
<p>PS 整体架构图如下</p>
<p><img src="/img/parameter-server.png" alt=""></p>
<h3 id="PS-架构分为两个部分："><a href="#PS-架构分为两个部分：" class="headerlink" title="PS 架构分为两个部分："></a>PS 架构分为两个部分：</h3><p><strong>外功</strong>：把计算资源分为两个部分，参数服务器节点和工作节点。</p>
<ol>
<li>参数服务器来存储参数</li>
<li>工作节点来做算法的训练</li>
</ol>
<p><strong>内功</strong>：内功是对应的，把机器学习分为两个部分，参数部分和训练部分。</p>
<ol>
<li>参数部分即模型部分，有一致性的要求，参数服务器也可以是一个集群。</li>
<li>训练部分需要并行化。因为参数服务器的存在，每个计算节点在拿到新的一批batch数据之后，都要从参数服务器上取下最新的参数，然后计算梯度，再将梯度更新会参数服务器。</li>
</ol>
<p>在 PS 中，每个 server 实际上都只负责分到的部分参数（servers共同维持一个全局的共享参数），而每个 work 也只分到部分数据和处理任务；每个子节点都只维护自己分配到的参数，自己部分更新之后，将计算结果（例如：梯度）传回到主节点，进行全局的更新（比如平均操作之类的），主节点再向子节点传送新的参数；</p>
<p>server 节点可以跟其他 server 节点通信，每个 server 负责自己分到的参数，server group 共同维持所有参数的更新。</p>
<p>server manager node 负责维护一些元数据的一致性，比如各个节点的状态，参数的分配情况等；</p>
<p>worker 节点之间没有通信，只跟自己对应的 server 进行通信。每个 worker group 有一个 task scheduler，负责向 worker 分配任务，并且监控 worker 的运行情况。当有新的 worker 加入或者退出，task scheduler 负责重新分配任务。</p>
<h3 id="PS-架构有-5-个特点"><a href="#PS-架构有-5-个特点" class="headerlink" title="PS 架构有 5 个特点"></a>PS 架构有 5 个特点</h3><ol>
<li><p>高效的通信</p>
<p> 异步通信，使得计算不会被拖累</p>
</li>
<li><p>弹性一致性</p>
<p> 允许用户自定义一致性: 比如 Sequential（序列式的，即完全同步）、Eventual（完全不同步的）和 Bounded Delay（有条件的限制，可以允许用户在限制的次数内异步，比如限制为 3 次，如果某个节点已经超前了其他节点四次迭代了，那么要停下等待同步。在整个训练的过程中，Delay 可能是动态的，即 delay 的参数在训练过程中可以变大或变小）</p>
</li>
<li><p>扩展性强</p>
<p> 使用了一个分布式 hash 表使得新的 server 节点可以随时动态的插入到集合中；因此，新增一个节点不需要重新运行系统。</p>
</li>
<li><p>错误容忍</p>
<p> 在大规模商用服务器集群中。从非灾难性机器故障中恢复，只需要 1 秒，而且不需要中断计算。Vector Clocks 保证了经历故障之后还是能运行良好；</p>
</li>
<li><p>易用性</p>
<p> 全局共享的参数可以被表示成各种形式：vector，matrices 或者相应的 sparse 类型，这大大方便了机器学习算法的开发。并且提供的线性代数的数据类型都具有高性能的多线程库。</p>
</li>
</ol>
<h3 id="PS-的一些关键概念"><a href="#PS-的一些关键概念" class="headerlink" title="PS 的一些关键概念"></a>PS 的一些关键概念</h3><h4 id="1-key-value-，Range-Push-and-Pull"><a href="#1-key-value-，Range-Push-and-Pull" class="headerlink" title="1. (key, value)，Range Push and Pull"></a>1. (key, value)，Range Push and Pull</h4><p><img src="/img/parameter-server4.png" alt=""></p>
<p>parameter server 中，参数都是可以被表示成(key, value)的集合，比如一个最小化损失函数的问题，key 就是 feature ID，而 value 就是它的权值。对于稀疏参数，不存在的key，就可以认为是0。</p>
<p>把参数表示成 k-v， 形式更自然，易于理解，更易于编程解；</p>
<p>workers 跟 servers 之间通过 push 跟 pull 来通信。worker 通过 push 将计算好的梯度发送到 server，然后通过 pull 从 server 更新参数。为了提高计算性能和带宽效率，parameter server 允许用户使用 Range Push 跟 Range Pull 操作（使用区间更新的方式）。</p>
<h4 id="2-Key-value-vectors"><a href="#2-Key-value-vectors" class="headerlink" title="2. Key-value vectors"></a>2. Key-value vectors</h4><p>赋予每个 key 所对应的 value 一个向量概念或矩阵概念。</p>
<h4 id="3-User-Defined-Functions-on-the-Server"><a href="#3-User-Defined-Functions-on-the-Server" class="headerlink" title="3. User-Defined Functions on the Server"></a>3. User-Defined Functions on the Server</h4><p>服务器端更新参数的时候还有计算正则项，这样的操作可以由用户自定义。</p>
<h4 id="4-Asychronous-Tasks-and-Dependency"><a href="#4-Asychronous-Tasks-and-Dependency" class="headerlink" title="4. Asychronous Tasks and Dependency"></a>4. Asychronous Tasks and Dependency</h4><p><img src="/img/parameter-server3.png" alt=""></p>
<p>如图，如果 iter1 需要在 iter0 computation，push 跟 pull 都完成后才能开始，那么就是 Synchronous，反之就是Asynchronous.</p>
<p>参数服务器和工作节点之间的通信都属于远程调用。远程调用相对而言要比较耗时，因而 PS 框架让远程调用成为异步调用，比如参数的 push 和 pull 发出之后，立即使用当前值开始进行下一步的梯度计算。（失去了模型的一致性，但提升了速度）。</p>
<p>Asychronous 的优点是能够提高系统的效率（因为节省了很多等待的过程），但是，它的缺点就是容易降低算法的收敛速率；</p>
<p>所以，系统性能跟算法收敛速率之间是存在一个 trade-off 的，你需要同时考虑：</p>
<ol>
<li><p>算法对于参数非一致性的敏感度；</p>
</li>
<li><p>训练数据特征之间的关联度；</p>
</li>
<li><p>硬盘的存储容量</p>
</li>
</ol>
<h4 id="6-User-Defined-Filters（用户自定义过滤）"><a href="#6-User-Defined-Filters（用户自定义过滤）" class="headerlink" title="6. User-Defined Filters（用户自定义过滤）"></a>6. User-Defined Filters（用户自定义过滤）</h4><p>在工作节点这一端对梯度进行过滤，如果梯度并不是影响那么大，就不占用网络去更新，等积累一段时间之后再去做更新。</p>
<p>对于机器学习优化问题比如梯度下降来说，并不是每次计算的梯度对于最终优化都是有价值的，用户可以通过自定义的规则过滤一些不必要的传送，再进一步压缩带宽 cost：</p>
<ol>
<li>发送很小的梯度值是低效的：</li>
</ol>
<p>因此可以自定义设置，只在梯度值较大的时候发送；</p>
<ol>
<li>更新接近最优情况的值是低效的：</li>
</ol>
<p>因此，只在非最优的情况下发送，可通过KKT来判断；</p>
<h3 id="PS-的实现"><a href="#PS-的实现" class="headerlink" title="PS 的实现"></a>PS 的实现</h3><p><strong>Vector Clock</strong></p>
<p>为参数服务器中的每个参数加一个时间戳来跟踪参数的更新，防止重复发送数据。如果每个参数都有一个时间戳，那么参数众多，时间戳也众多。但借助于Vector概念，很多的参数可以作为向量存在k-v中，因而，时间戳的数量大大减少。</p>
<p>在刚开始的时候，所有的参数都是一个大向量，时间戳为0，每次来一个范围的更新，如果能找到对应的key，那么直接更新那个key的时间戳就可以了。否则，就可能会对某些向量进行切分，一次更新请求，最多能把一个区间切分为三个区间。</p>
<p><strong>一致性哈希</strong></p>
<p>参数服务器集群中每个节点都负责不同区域的参数，那么，类似于hash table，使用hash ring进行实现，key和server id都插入到hash ring上去。</p>
<p><strong>备份和一致性</strong></p>
<p>使用类似 hadoop 的 chain 备份方式，对于一个 master 节点，如果有更新，先更新它，然后再去更新备份的服务器。</p>
<p><img src="/img/parameter-server2.png" alt=""></p>
<p>在更新的时候，由于机器学习算法的特点，可以将多次梯度聚合之后再去更新备份服务器，从而减少带宽。</p>
<p><strong>Messages</strong></p>
<p>一条 message 包括：时间戳，len(range) 对 k-v。</p>
<p>这是 parameter server 中最基本的通信格式，不仅仅是共享的参数才有，task 的message 也是这样的格式，只要把这里的 (key, value) 改成 (task ID, 参数/返回值)。</p>
<p>由于机器学习问题通常都需要很高的网络带宽，因此信息的压缩是必须的。</p>
<p>key 的压缩：因为训练数据通常在分配之后都不会发生改变，因此worker没有必要每次都发送相同的key，只需要接收方在第一次接收的时候缓存起来就行了。第二次，worker不再需要同时发送key和value，只需要发送value 和 key list的hash就行。这样瞬间减少了一般的通信量。</p>
<p>value的压缩： 假设参数时稀疏的，那么就会有大量的0存在。因此，为了进一步压缩，我们只需要发送非0值。parameter server使用 Snappy 快速压缩库来压缩数据、高效去除 0 值。</p>
<pre><code>key 的压缩和 value 的压缩可以同时进行。
</code></pre><p><strong>Server Management</strong></p>
<p>由于 key 的 range 特性，当参数服务器集群中增加一个节点时，步骤如下：</p>
<ul>
<li>server manager 节点给新节点分配一个 key range，这可能会导致其他节点上的 key range 切分。</li>
<li>新节点从其他节点上将属于它的 key range 数据取过来，然后也将 slave 信息取过来。</li>
<li>server manager广播节点变动，其他节点得知消息后将不属于自己 key range 的数据删掉</li>
</ul>
<p>在第二步，从其他节点上取数据的时候，其他节点上的操作也分为两步，第一是拷贝数据，这可能也会导致 key range 的切分。第二是不再接受和这些数据有关的消息，而是进行转发，转发到新节点。</p>
<p>在第三步，收到广播信息后，节点会删除对应区间的数据，然后，扫描所有的和R有关发送出去的还没收到回复的消息，当这些消息回复时，转发到新节点。</p>
<p>节点的离开与节点的加入类似。</p>
<p><strong>Worker Management</strong></p>
<p>添加工作节点比添加服务器节点要简单一些，步骤如下：</p>
<ul>
<li>task scheduler 给新节点分配一些数据</li>
<li>节点从网络文件系统中载入数据，然后从服务器端拉取参数</li>
<li>task scheduler 广播变化，其他节点 free 掉一些训练数据</li>
</ul>
<p>当一个节点离开的时候，task scheduler 可能会寻找一个替代，但恢复节点是十分耗时的工作，同时，损失一些数据对最后的结果可能影响并不是很大。所以，系统会让用户进行选择，是恢复节点还是不做处理。这种机制甚至可以允许用户删掉跑的最慢的节点来提升速度。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/15/XGBoost-介绍/" itemprop="url">
                  XGBoost 介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-15T19:28:38+08:00" content="2017-03-15">
              2017-03-15
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/15/XGBoost-介绍/" class="leancloud_visitors" data-flag-title="XGBoost 介绍">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>我之前的博客提到过，Boosting 是一种将弱分类器转化为强分类器的方法，它的函数模型具有叠加性。</p>
<p>XGBoost 的目标函数由损失函数和复杂度组成。复杂度又由叶子数量和 L2 正则组成。</p>
<p>$$L(\phi) = \sum_i l(y^{*}_i, y_i) + \sum_k \Omega(f_k) $$</p>
<p>$$where \,\, \Omega(f) = \gamma T + \frac{1}{2}\lambda||w||^2$$</p>
<p>其中 i 是样本 id，k 是树 id（轮数），由于 loss 函数和复杂度项都是凸函数，所以有最小值。w 是与真实值的残差，将 w 的 L2 正则项加在目标函数中，可以有效的防止过拟合。叶子节点的数目也作为正则项加在了目标函数中，一定程度上限制了叶子数量，防止过拟合。而传统 GBDT 防止过拟合的手段是预剪枝或者后剪枝。</p>
<h3 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h3><p>对于每次迭代过程，可以将一颗树的训练目标函数写成形式如下：</p>
<p>$$L^{(t)} = \sum_{i=1}^n l(y_i, y_i^{*(t-1)} + f_t(x_i)) + \Omega(f_t)$$</p>
<p>输入是 \(t-1\) 轮后预测的值和真实值，用来拟合残差 \(f(x)\)。对于这个式子，因为无法对 \(f(x)\) 进行有效的最优估计，所以要进行如下推导：</p>
<p>首先将目标函数泰勒二阶展开：</p>
<p>$$L^{(t)} = \sum_{i=1}^n [ l(y_i, y_i^{*(t-1)}) + g_i f_i(x_i) + \frac{1}{2} h_if^2_t(x_i)] + \Omega(f_t)$$</p>
<p>去掉常数项，</p>
<p>$$L^{(t)} = \sum_{i=1}^n [g_i f_i(x_i) + \frac{1}{2} h_if^2_t(x_i)] + \Omega(f_t)$$</p>
<p>正则项展开，</p>
<p>$$L^{(t)} = \sum_{i=1}^n [g_i f_i(x_i) + \frac{1}{2} h_if^2_t(x_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2$$</p>
<p>首先做一下转换，</p>
<p>$$w_j = f(x_i) \, \, i \in _jI$$</p>
<p>所以，公式可以转化为：</p>
<p>$$L^{(t)} = \sum_{j=1}^T [(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda) w_j^2] + \gamma T$$</p>
<p>这是一个二次项形式，所以最后使得目标函数最小的 w 为，</p>
<p>$$w^*_j = - \frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$$</p>
<p>带入原方程最小值为：</p>
<p>$$L^{t}(q) = - \frac{1}{2} \sum_{j=1}^T \frac{(\sum_{i \in I_j} g_i)^2}{\sum_{i \in I_j} h_i + \lambda} + \gamma T$$</p>
<p>最后求得的w就是目标函数在一个样本集合条件下的最优解。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>为什么要用泰勒二阶近似展开。</strong></p>
<p>由于 gbdt 只用到了一阶信息，相当于 loss 函数只进行了一阶泰勒展开。在没有复杂度项的情况下，无法确定步长，所以只能用常数步长根据一阶梯度方向去逼近。这就是牛顿下降法和梯度下降法的区别。由于二阶展开用二次函数去逼近函数，所以可以利用二阶信息确定更新步长，比只利用一阶信息的 gdbt 用更少的迭代获得更好的效果。</p>
<p><strong>为何要对目标函数进行推导。</strong></p>
<ol>
<li>为了适应各种损失函数。</li>
<li>正则项的加入对参数估计产生了影响。</li>
<li>如果只考虑平方损失的条件下，在没有正则项的情况下参数的最优估计为样本均值。在没有指定损失函数情况下，我们也很容易想到均值是给定样本条件下的误差最小的最优估计。但是损失函数换成绝对值损失，那么最优估计就为中位数。可见不同损失函数下，结果并不想我们想的那么简单。</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/13/线性回归家族一览/" itemprop="url">
                  线性回归家族一览
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-13T20:59:23+08:00" content="2017-03-13">
              2017-03-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/03/13/线性回归家族一览/" class="leancloud_visitors" data-flag-title="线性回归家族一览">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>线性回归的目的是要得到输出向量 Y 和输入特征 X 之间的线性关系，求出线性回归系数 \(\theta\)，也就是 \(Y = X\theta\)。其中 Y 的维度是 m <em> 1，X 的维度是 m </em> n，而 \(\theta\) 的维度为 n * 1。m 代表样本个数，n 代表样本特征的维度。</p>
<p>为了得到线性回归系数 \(\theta\)，我们需要定义一个损失函数，一个极小化损失函数的优化方法，以及一个验证算法的方法。损失函数的不同，损失函数的优化方法的不同，验证方法的不同，就形成了不同的线性回归算法。</p>
<h3 id="LinearRegression"><a href="#LinearRegression" class="headerlink" title="LinearRegression"></a>LinearRegression</h3><p>损失函数：</p>
<p>LinearRegression类就是我们平时说的最常见普通的线性回归，它的损失函数也是最简单的，如下：</p>
<p>$$J(\theta) =  \frac{1}{2}(X\theta - Y)^{T}(X\theta - Y)$$</p>
<p>损失函数的优化方法：</p>
<p>对于这个函数，一般有梯度下降法和最小二乘法两种极小化损失函数的优化方法。通过最小二乘法，可以解出线性回归系数 \(\theta\) 为：</p>
<p>$$\theta = (X^TX)^{-1}X^{T}Y$$</p>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p>一般来说，只要我们觉得数据有线性关系，LinearRegression 是我们的首先应该采用的。如果发现拟合或者预测的不好，再考虑用其他的线性回归库。如果是学习线性回归，推荐先从这个类开始第一步的研究。</p>
<h3 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h3><p>由于单纯的线性回归没有考虑过拟合的问题，有可能泛化能力较差，这时损失函数可以加入正则化项，如果加入的是 L2 范数的正则化项，就是 Ridge 回归。此时，损失函数如下：</p>
<p>$$J(\theta) =  \frac{1}{2}(X\theta - Y)^{T}(X\theta - Y) + \frac{1}{2}\alpha ||\theta||^2_2$$</p>
<p>其中 \(\alpha\) 为常数系数，需要进行调优。</p>
<p>Ridge 回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，不至于过拟合。</p>
<h4 id="损失函数的优化方法"><a href="#损失函数的优化方法" class="headerlink" title="损失函数的优化方法"></a>损失函数的优化方法</h4><p>一般也用最小二乘法或者梯度下降法来优化。通过最小二乘法，可以解得系数：</p>
<p>$$\theta = (X^TX + \alpha E)^{-1}X^TY$$</p>
<h4 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h4><p>一般来说，只要我们觉得数据有线性关系，用 LinearRegression 拟合的不是特别好，需要正则化，可以考虑用 Ridge Regression。但是这个类最大的缺点是每次我们要自己指定一个超参数 \(\alpha\)，然后自己评估 \(\alpha\) 的好坏，比较麻烦。</p>
<h3 id="RidgeCV"><a href="#RidgeCV" class="headerlink" title="RidgeCV"></a>RidgeCV</h3><p>RidgeCV 的损失函数和损失函数的优化方法完全与 Ridge 类相同，区别在于验证方法。</p>
<p>RidgeCV 对超参数 \(\alpha\) 使用了交叉验证，来帮忙我们选择一个合适的 \(\alpha\)。在初始化 RidgeCV 时候，我们可以传一组备选的 \(\alpha\) 值，10个，100个都可以。RidgeCV类会帮我们选择一个合适的αα。免去了我们自己去一轮轮筛选αα的苦恼。　　</p>
<h4 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h4><p>一般来说，只要我们觉得数据有线性关系，用 LinearRegression 拟合的不是特别好，需要正则化，可以考虑用 RidgeCV。如果输入特征的维度很高，而且是稀疏线性关系的话，RidgeCV类就不合适了。这时应该主要考虑下面要讲到的 Lasso 回归类家族。</p>
<h3 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h3><p>线性回归的 L1 正则化通常称为 Lasso 回归，它和 Ridge 回归的区别是在损失函数上增加了的是 L1 正则化的项，而不是 L2 正则化项。L1 正则化的项也有一个常数系数 \(\alpha\) 来调节损失函数的均方差项和正则化项的权重，具体 Lasso 回归的损失函数表达式如下：</p>
<p>$$J(\theta) = \frac{1}{2m}(X\theta - Y)^{T}(X\theta - Y) + \alpha||\theta||_1$$</p>
<p>Lasso 回归可以使得一些特征的系数变小，甚至还是一些绝对值比较小的系数直接变为 0。增强模型的泛化能力。</p>
<h3 id="ElasticNet"><a href="#ElasticNet" class="headerlink" title="ElasticNet"></a>ElasticNet</h3><p>ElasticNet 可以看做 Lasso 和 Ridge 的结合。它也是对普通的线性回归做了正则化，但是它的损失函数记不全是 L1 的正则化，也不全是 L2 的正则化，而是一个权重 \(\rho\) 来平衡 L1 和 L2 正则化的比重，形成了一个全新的损失函数如下：</p>
<p>$$J(\theta) = \frac{1}{2m}(X\theta - Y)^{T}(X\theta - Y) + \alpha \rho||\theta||_1 + \frac{\alpha(1 - \rho)}{2}||\theta||^2_2$$</p>
<h4 id="使用场景-2"><a href="#使用场景-2" class="headerlink" title="使用场景"></a>使用场景</h4><p>ElasticNet 用在我们发现用 Lasso 回归太过（太多特征被稀疏为0），而用Ridge回归又正则化的不够（回归系数衰减的太慢）的时候。一般不推荐拿到数据就直接就 ElasticNet。</p>
<h3 id="BayesianRidge"><a href="#BayesianRidge" class="headerlink" title="BayesianRidge"></a>BayesianRidge</h3><p>贝叶斯回归模型假设先验概率，似然函数和后验概率都是正态分布。先验概率是假设模型输出 Y 是符合均值为 \(X\theta\) 的正态分布，正则化参数 \(X\alpha\) 被看作是一个需要从数据中估计得到的随机变量。回归系数 \(\theta\) 的先验分布规律为球形正态分布，超参数为 λ。我们需要通过最大化边际似然函数来估计超参数 α 和 λ，以及回归系数 \(\theta\)。</p>
<h4 id="使用场景：-1"><a href="#使用场景：-1" class="headerlink" title="使用场景："></a>使用场景：</h4><p>如果我们的数据有很多缺失或者矛盾的病态数据，可以考虑 BayesianRidge，它对病态数据鲁棒性很高，也不用交叉验证选择超参数。但是极大化似然函数的推断过程比较耗时，一般情况不推荐使用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="PengShuang" />
          <p class="site-author-name" itemprop="name">PengShuang</p>
          <p class="site-description motion-element" itemprop="description">在路上，慢慢走！</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">78</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/pengshuang" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2176899852/profile?rightmod=1&wvr=6&mod=personnumber&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://bbs.byr.cn/" title="北邮人" target="_blank">北邮人</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://coolshell.cn/" title="酷壳" target="_blank">酷壳</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.dongwm.com" title="小明明的博客" target="_blank">小明明的博客</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PengShuang</span>
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
  <p>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p>
</div>

<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("DKbLgBme7UkAx9JX6sM3D4Hj-gzGzoHsz", "GXjJ9Ox3pUGI9PJhm6CNfJGN");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
